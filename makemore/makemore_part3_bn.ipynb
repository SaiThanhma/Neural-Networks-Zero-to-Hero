{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "27b63b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "95fbf334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "742dd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read_words('names.txt')\n",
    "stoi, itos = get_mapping(words)\n",
    "nchars = len(stoi.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "6c866e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, val_split = 0.8, 0.1\n",
    "X, Y = build_dataset(words, stoi, block_size=3)\n",
    "X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "\n",
    "n = len(X)\n",
    "n1 = round(n * train_split)\n",
    "n2 = round(n * val_split)\n",
    "\n",
    "\n",
    "X_train, Y_train = X[:n1], Y[:n1]\n",
    "X_val, Y_val = X[n1:n1+n2], Y[n1:n1+n2]\n",
    "X_test, Y_test = X[n1+n2:], Y[n1+n2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "3a60a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, b = None):\n",
    "        self.W = torch.ones(fan_in, fan_out)\n",
    "        self.b = None\n",
    "        if b is not None:\n",
    "            self.b = torch.zeros(fan_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = x @ self.W\n",
    "        if self.b is not None:\n",
    "            out = out + self.b\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.W] + ([] if self.b is None else [self.b])\n",
    "    \n",
    "\n",
    "class LinearBatchNorm1d:\n",
    "    def __init__(self, fan_in, fan_out):\n",
    "        self.Training = True\n",
    "        self.Folding = True\n",
    "        self.linear = Linear(fan_in, fan_out, False)\n",
    "        self.bn = BatchNorm1d(fan_out)\n",
    "        self.W_folded = None\n",
    "        self.b_folded = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.Training or not self.Folding:\n",
    "            x = self.linear(x)\n",
    "            out = self.bn(x)\n",
    "        else:\n",
    "            if self.W_folded is None:\n",
    "                self.W_folded = self.bn.gamma * self.linear.W / (torch.sqrt(self.bn.running_var + self.bn.epsilon))\n",
    "                self.b_folded = self.bn.beta\n",
    "                print(\"test\")\n",
    "            out = x @ self.W_folded + self.b_folded\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def setTraining(self, train):\n",
    "        self.Training = train\n",
    "        self.bn.Training = train\n",
    "        if train:\n",
    "            self.W_folded = None\n",
    "            self.b_folded = None\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.linear.parameters() + self.bn.parameters()\n",
    "    \n",
    "    \n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        out = torch.tanh(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class BatchNorm1d:\n",
    "    def __init__(self, hidden_dim, epsilon = 1e-7, momentum = 0.99):\n",
    "        self.Training = True\n",
    "        self.gamma = torch.ones(hidden_dim)\n",
    "        self.beta = torch.zeros(hidden_dim)\n",
    "        self.running_mean = torch.zeros((1, hidden_dim))\n",
    "        self.running_var = torch.ones((1, hidden_dim))\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.Training:\n",
    "            mean = x.mean(0, keepdim = True)\n",
    "            var = x.var(0, keepdim = True)\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        x_gaus = (x - mean) / torch.sqrt(var + self.epsilon)\n",
    "        out = (self.gamma * x_gaus) + self.beta\n",
    "\n",
    "        if self.Training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.running_mean * self.momentum + (1 - self.momentum) * mean\n",
    "                self.running_var = self.running_var * self.momentum + (1 - self.momentum) * var\n",
    "\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "class Embedding:\n",
    "    def __init__(self, num_class, emb_dim):\n",
    "        self.C = torch.randn(num_class, emb_dim)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.C[x]\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.C]\n",
    "    \n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        N, T, C = x.shape\n",
    "        out = x.view(N, T*C)\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        p = []\n",
    "        for layer in self.layers:\n",
    "            p = p + layer.parameters()\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "043c0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 10\n",
    "block_size = 3\n",
    "hidden_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "caecbba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = Sequential([\n",
    "    Embedding(nchars, emb_dim),\n",
    "    Flatten(),\n",
    "    Linear(block_size * emb_dim, hidden_dim, b = False), Tanh(),\n",
    "    Linear(hidden_dim, hidden_dim, b = False), Tanh(),\n",
    "    Linear(hidden_dim, hidden_dim, b = False), Tanh(),\n",
    "    Linear(hidden_dim, hidden_dim, b = False), Tanh(),\n",
    "    Linear(hidden_dim, nchars, b = False)\n",
    "])\n",
    "\n",
    "for p in layers.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "14adbde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = layers(X_train)\n",
    "\n",
    "loss = F.cross_entropy(logits, Y_train)\n",
    "\n",
    "logits.retain_grad()\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "fd28bc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "174861c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "layer 3 (      Tanh): mean +0.06, std 0.92, saturated: 71.45%\n",
      "torch.Size([100])\n",
      "layer 5 (      Tanh): mean +0.05, std 1.00, saturated: 99.89%\n",
      "torch.Size([100])\n",
      "layer 7 (      Tanh): mean +0.05, std 1.00, saturated: 100.00%\n",
      "torch.Size([100])\n",
      "layer 9 (      Tanh): mean +0.05, std 1.00, saturated: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'activation distribution')"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers.layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out\n",
    "    print(t[0].shape)\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends)\n",
    "plt.title('activation distribution')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "f442272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/_xpmwfn50nb97kqnlvcpty9w0000gn/T/ipykernel_37815/1063832456.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  t = layer.out.grad\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[667], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Linear):\n\u001b[1;32m      5\u001b[0m     t \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mout\u001b[38;5;241m.\u001b[39mgrad\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%10s\u001b[39;00m\u001b[38;5;124m): mean \u001b[39m\u001b[38;5;132;01m%+f\u001b[39;00m\u001b[38;5;124m, std \u001b[39m\u001b[38;5;132;01m%e\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (i, layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m(), t\u001b[38;5;241m.\u001b[39mstd()))\n\u001b[1;32m      7\u001b[0m     hy, hx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhistogram(t, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(hx[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach(), hy\u001b[38;5;241m.\u001b[39mdetach())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'mean'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers.layers[:-1]):\n",
    "    if isinstance(layer, Linear):\n",
    "        t = layer.out.grad\n",
    "        print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "        if i == 10:\n",
    "            #print((logits.grad @ layer.W.T).std())\n",
    "            pass\n",
    "plt.legend(legends)\n",
    "plt.title('gradient distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4076170",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "61c9288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(nchars, emb_dim),\n",
    "    Flatten(),\n",
    "    LinearBatchNorm1d(block_size * emb_dim, hidden_dim), Tanh(),\n",
    "    LinearBatchNorm1d(hidden_dim, hidden_dim), Tanh(),\n",
    "    LinearBatchNorm1d(hidden_dim, hidden_dim), Tanh(),\n",
    "    LinearBatchNorm1d(hidden_dim, hidden_dim), Tanh(),\n",
    "    Linear(hidden_dim, nchars, b = False)\n",
    "])\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "a990a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 loss 3.295837163925171, acc 17.1875\n",
      "iteration 1 loss 3.289222478866577, acc 14.0625\n",
      "iteration 2 loss 3.2533953189849854, acc 18.75\n",
      "iteration 3 loss 3.207397699356079, acc 6.25\n",
      "iteration 4 loss 3.3675360679626465, acc 17.1875\n",
      "iteration 5 loss 3.2205169200897217, acc 14.0625\n",
      "iteration 6 loss 3.2628350257873535, acc 3.125\n",
      "iteration 7 loss 3.304940700531006, acc 4.6875\n",
      "iteration 8 loss 3.281834602355957, acc 10.9375\n",
      "iteration 9 loss 3.237934112548828, acc 14.0625\n",
      "iteration 10 loss 3.2235872745513916, acc 18.75\n",
      "iteration 11 loss 3.224116563796997, acc 20.3125\n",
      "iteration 12 loss 3.2382009029388428, acc 20.3125\n",
      "iteration 13 loss 3.3018176555633545, acc 14.0625\n",
      "iteration 14 loss 3.152125597000122, acc 25.0\n",
      "iteration 15 loss 3.2730894088745117, acc 17.1875\n",
      "iteration 16 loss 3.2990596294403076, acc 12.5\n",
      "iteration 17 loss 3.1330246925354004, acc 20.3125\n",
      "iteration 18 loss 3.2255256175994873, acc 6.25\n",
      "iteration 19 loss 3.0995614528656006, acc 25.0\n",
      "iteration 20 loss 3.181182861328125, acc 17.1875\n",
      "iteration 21 loss 3.0722174644470215, acc 29.6875\n",
      "iteration 22 loss 3.282519578933716, acc 12.5\n",
      "iteration 23 loss 3.0745394229888916, acc 23.4375\n",
      "iteration 24 loss 3.155512809753418, acc 21.875\n",
      "iteration 25 loss 3.2649478912353516, acc 12.5\n",
      "iteration 26 loss 3.1785614490509033, acc 12.5\n",
      "iteration 27 loss 3.1656181812286377, acc 15.625\n",
      "iteration 28 loss 3.14375901222229, acc 20.3125\n",
      "iteration 29 loss 3.1858367919921875, acc 15.625\n",
      "iteration 30 loss 3.1396756172180176, acc 15.625\n",
      "iteration 31 loss 3.224155902862549, acc 9.375\n",
      "iteration 32 loss 3.197793483734131, acc 7.8125\n",
      "iteration 33 loss 3.091332197189331, acc 23.4375\n",
      "iteration 34 loss 3.156796932220459, acc 17.1875\n",
      "iteration 35 loss 3.13834810256958, acc 17.1875\n",
      "iteration 36 loss 3.1719970703125, acc 10.9375\n",
      "iteration 37 loss 3.1889424324035645, acc 10.9375\n",
      "iteration 38 loss 3.1493911743164062, acc 6.25\n",
      "iteration 39 loss 3.1968257427215576, acc 7.8125\n",
      "iteration 40 loss 3.024003505706787, acc 25.0\n",
      "iteration 41 loss 3.2060229778289795, acc 14.0625\n",
      "iteration 42 loss 3.0543107986450195, acc 21.875\n",
      "iteration 43 loss 3.089510917663574, acc 18.75\n",
      "iteration 44 loss 2.9862418174743652, acc 28.125\n",
      "iteration 45 loss 3.104578733444214, acc 12.5\n",
      "iteration 46 loss 3.0309805870056152, acc 21.875\n",
      "iteration 47 loss 2.939805746078491, acc 28.125\n",
      "iteration 48 loss 3.1299102306365967, acc 17.1875\n",
      "iteration 49 loss 3.187117099761963, acc 17.1875\n",
      "iteration 50 loss 3.0733463764190674, acc 18.75\n",
      "iteration 51 loss 3.009080410003662, acc 25.0\n",
      "iteration 52 loss 3.0134313106536865, acc 18.75\n",
      "iteration 53 loss 3.1114115715026855, acc 18.75\n",
      "iteration 54 loss 3.0161736011505127, acc 21.875\n",
      "iteration 55 loss 3.0507655143737793, acc 18.75\n",
      "iteration 56 loss 2.980543613433838, acc 21.875\n",
      "iteration 57 loss 3.191025972366333, acc 6.25\n",
      "iteration 58 loss 2.9784021377563477, acc 26.5625\n",
      "iteration 59 loss 3.080019474029541, acc 12.5\n",
      "iteration 60 loss 3.0207316875457764, acc 17.1875\n",
      "iteration 61 loss 2.9977529048919678, acc 25.0\n",
      "iteration 62 loss 3.053532123565674, acc 17.1875\n",
      "iteration 63 loss 2.9777274131774902, acc 20.3125\n",
      "iteration 64 loss 3.0454838275909424, acc 14.0625\n",
      "iteration 65 loss 2.951698064804077, acc 12.5\n",
      "iteration 66 loss 2.9236459732055664, acc 18.75\n",
      "iteration 67 loss 3.082582950592041, acc 12.5\n",
      "iteration 68 loss 3.0647170543670654, acc 17.1875\n",
      "iteration 69 loss 3.1186091899871826, acc 18.75\n",
      "iteration 70 loss 3.114701986312866, acc 15.625\n",
      "iteration 71 loss 2.832123041152954, acc 35.9375\n",
      "iteration 72 loss 3.1044423580169678, acc 20.3125\n",
      "iteration 73 loss 3.191422939300537, acc 9.375\n",
      "iteration 74 loss 3.078876495361328, acc 14.0625\n",
      "iteration 75 loss 3.043513774871826, acc 20.3125\n",
      "iteration 76 loss 2.982525587081909, acc 17.1875\n",
      "iteration 77 loss 3.2180821895599365, acc 9.375\n",
      "iteration 78 loss 2.951547861099243, acc 23.4375\n",
      "iteration 79 loss 2.9995925426483154, acc 15.625\n",
      "iteration 80 loss 3.0725789070129395, acc 10.9375\n",
      "iteration 81 loss 2.9163081645965576, acc 18.75\n",
      "iteration 82 loss 3.007133960723877, acc 14.0625\n",
      "iteration 83 loss 3.058220863342285, acc 20.3125\n",
      "iteration 84 loss 2.9368503093719482, acc 20.3125\n",
      "iteration 85 loss 3.020717144012451, acc 18.75\n",
      "iteration 86 loss 3.0486767292022705, acc 15.625\n",
      "iteration 87 loss 2.971707820892334, acc 25.0\n",
      "iteration 88 loss 2.9531989097595215, acc 21.875\n",
      "iteration 89 loss 2.8970470428466797, acc 26.5625\n",
      "iteration 90 loss 2.895052433013916, acc 25.0\n",
      "iteration 91 loss 3.055663585662842, acc 18.75\n",
      "iteration 92 loss 2.9848997592926025, acc 18.75\n",
      "iteration 93 loss 3.0385332107543945, acc 12.5\n",
      "iteration 94 loss 3.0813639163970947, acc 10.9375\n",
      "iteration 95 loss 2.865297555923462, acc 21.875\n",
      "iteration 96 loss 2.7950785160064697, acc 20.3125\n",
      "iteration 97 loss 3.0491247177124023, acc 15.625\n",
      "iteration 98 loss 3.0435619354248047, acc 15.625\n",
      "iteration 99 loss 2.9336957931518555, acc 21.875\n",
      "iteration 100 loss 3.1431519985198975, acc 10.9375\n",
      "iteration 101 loss 3.0430145263671875, acc 17.1875\n",
      "iteration 102 loss 3.066119909286499, acc 9.375\n",
      "iteration 103 loss 3.0060994625091553, acc 17.1875\n",
      "iteration 104 loss 2.8581860065460205, acc 25.0\n",
      "iteration 105 loss 2.9368412494659424, acc 17.1875\n",
      "iteration 106 loss 2.981064558029175, acc 21.875\n",
      "iteration 107 loss 2.797577381134033, acc 26.5625\n",
      "iteration 108 loss 3.0404210090637207, acc 17.1875\n",
      "iteration 109 loss 2.8461780548095703, acc 26.5625\n",
      "iteration 110 loss 2.970078229904175, acc 18.75\n",
      "iteration 111 loss 2.808450222015381, acc 26.5625\n",
      "iteration 112 loss 3.0313563346862793, acc 18.75\n",
      "iteration 113 loss 2.9504647254943848, acc 17.1875\n",
      "iteration 114 loss 2.9644508361816406, acc 21.875\n",
      "iteration 115 loss 2.9987432956695557, acc 18.75\n",
      "iteration 116 loss 2.898961067199707, acc 25.0\n",
      "iteration 117 loss 2.8105757236480713, acc 26.5625\n",
      "iteration 118 loss 2.8593318462371826, acc 26.5625\n",
      "iteration 119 loss 2.903578996658325, acc 21.875\n",
      "iteration 120 loss 2.8206794261932373, acc 28.125\n",
      "iteration 121 loss 2.989440679550171, acc 20.3125\n",
      "iteration 122 loss 3.0433435440063477, acc 15.625\n",
      "iteration 123 loss 2.9100394248962402, acc 21.875\n",
      "iteration 124 loss 2.8615942001342773, acc 18.75\n",
      "iteration 125 loss 2.944153308868408, acc 17.1875\n",
      "iteration 126 loss 2.9135994911193848, acc 14.0625\n",
      "iteration 127 loss 2.9786229133605957, acc 15.625\n",
      "iteration 128 loss 2.8846616744995117, acc 25.0\n",
      "iteration 129 loss 3.020940065383911, acc 15.625\n",
      "iteration 130 loss 2.9509143829345703, acc 15.625\n",
      "iteration 131 loss 2.9150197505950928, acc 28.125\n",
      "iteration 132 loss 2.927976608276367, acc 14.0625\n",
      "iteration 133 loss 2.9768407344818115, acc 10.9375\n",
      "iteration 134 loss 2.835573673248291, acc 20.3125\n",
      "iteration 135 loss 2.933706521987915, acc 17.1875\n",
      "iteration 136 loss 2.907214403152466, acc 17.1875\n",
      "iteration 137 loss 2.9733636379241943, acc 20.3125\n",
      "iteration 138 loss 2.977652072906494, acc 17.1875\n",
      "iteration 139 loss 2.835918426513672, acc 18.75\n",
      "iteration 140 loss 2.9598209857940674, acc 15.625\n",
      "iteration 141 loss 2.8033292293548584, acc 21.875\n",
      "iteration 142 loss 3.0249321460723877, acc 20.3125\n",
      "iteration 143 loss 2.7963666915893555, acc 28.125\n",
      "iteration 144 loss 2.9555435180664062, acc 18.75\n",
      "iteration 145 loss 2.851526975631714, acc 23.4375\n",
      "iteration 146 loss 2.8528707027435303, acc 17.1875\n",
      "iteration 147 loss 2.9798524379730225, acc 10.9375\n",
      "iteration 148 loss 2.893454074859619, acc 23.4375\n",
      "iteration 149 loss 2.863582134246826, acc 12.5\n",
      "iteration 150 loss 3.0048165321350098, acc 12.5\n",
      "iteration 151 loss 2.9053430557250977, acc 21.875\n",
      "iteration 152 loss 2.8805463314056396, acc 20.3125\n",
      "iteration 153 loss 2.7975895404815674, acc 20.3125\n",
      "iteration 154 loss 2.9070112705230713, acc 18.75\n",
      "iteration 155 loss 2.931166172027588, acc 9.375\n",
      "iteration 156 loss 2.9670779705047607, acc 10.9375\n",
      "iteration 157 loss 2.974029064178467, acc 12.5\n",
      "iteration 158 loss 2.8976244926452637, acc 20.3125\n",
      "iteration 159 loss 2.8281149864196777, acc 23.4375\n",
      "iteration 160 loss 2.887033224105835, acc 20.3125\n",
      "iteration 161 loss 2.6838274002075195, acc 34.375\n",
      "iteration 162 loss 2.841820001602173, acc 21.875\n",
      "iteration 163 loss 2.8174993991851807, acc 26.5625\n",
      "iteration 164 loss 2.8999481201171875, acc 15.625\n",
      "iteration 165 loss 2.9628918170928955, acc 17.1875\n",
      "iteration 166 loss 2.776745080947876, acc 23.4375\n",
      "iteration 167 loss 2.9179720878601074, acc 12.5\n",
      "iteration 168 loss 2.8232364654541016, acc 18.75\n",
      "iteration 169 loss 2.840912103652954, acc 18.75\n",
      "iteration 170 loss 2.978245496749878, acc 18.75\n",
      "iteration 171 loss 2.7631537914276123, acc 20.3125\n",
      "iteration 172 loss 2.8671109676361084, acc 18.75\n",
      "iteration 173 loss 2.836069107055664, acc 17.1875\n",
      "iteration 174 loss 2.9278016090393066, acc 9.375\n",
      "iteration 175 loss 2.9372684955596924, acc 18.75\n",
      "iteration 176 loss 3.011284351348877, acc 14.0625\n",
      "iteration 177 loss 2.8320627212524414, acc 23.4375\n",
      "iteration 178 loss 2.930922031402588, acc 15.625\n",
      "iteration 179 loss 2.7142276763916016, acc 25.0\n",
      "iteration 180 loss 2.741002321243286, acc 21.875\n",
      "iteration 181 loss 3.0472495555877686, acc 6.25\n",
      "iteration 182 loss 2.8494980335235596, acc 20.3125\n",
      "iteration 183 loss 2.8349311351776123, acc 21.875\n",
      "iteration 184 loss 2.903923988342285, acc 17.1875\n",
      "iteration 185 loss 2.883960247039795, acc 23.4375\n",
      "iteration 186 loss 3.0074691772460938, acc 14.0625\n",
      "iteration 187 loss 2.9037256240844727, acc 18.75\n",
      "iteration 188 loss 2.7372405529022217, acc 25.0\n",
      "iteration 189 loss 2.971558094024658, acc 18.75\n",
      "iteration 190 loss 2.781865358352661, acc 20.3125\n",
      "iteration 191 loss 3.0783884525299072, acc 10.9375\n",
      "iteration 192 loss 2.9144647121429443, acc 23.4375\n",
      "iteration 193 loss 2.8458473682403564, acc 20.3125\n",
      "iteration 194 loss 2.9497413635253906, acc 17.1875\n",
      "iteration 195 loss 2.9316630363464355, acc 12.5\n",
      "iteration 196 loss 2.733474016189575, acc 26.5625\n",
      "iteration 197 loss 2.974855899810791, acc 14.0625\n",
      "iteration 198 loss 2.883035898208618, acc 14.0625\n",
      "iteration 199 loss 3.0006306171417236, acc 9.375\n",
      "iteration 200 loss 2.939176082611084, acc 23.4375\n",
      "iteration 201 loss 2.8004605770111084, acc 26.5625\n",
      "iteration 202 loss 2.615926742553711, acc 32.8125\n",
      "iteration 203 loss 2.757415771484375, acc 26.5625\n",
      "iteration 204 loss 2.9327285289764404, acc 15.625\n",
      "iteration 205 loss 2.8436505794525146, acc 21.875\n",
      "iteration 206 loss 2.8002991676330566, acc 21.875\n",
      "iteration 207 loss 2.755251407623291, acc 26.5625\n",
      "iteration 208 loss 2.804523229598999, acc 25.0\n",
      "iteration 209 loss 3.0394837856292725, acc 10.9375\n",
      "iteration 210 loss 2.9557301998138428, acc 12.5\n",
      "iteration 211 loss 2.870445966720581, acc 14.0625\n",
      "iteration 212 loss 2.7105252742767334, acc 25.0\n",
      "iteration 213 loss 2.764296054840088, acc 21.875\n",
      "iteration 214 loss 2.85292911529541, acc 21.875\n",
      "iteration 215 loss 2.5818302631378174, acc 26.5625\n",
      "iteration 216 loss 2.889977216720581, acc 21.875\n",
      "iteration 217 loss 2.882887125015259, acc 17.1875\n",
      "iteration 218 loss 3.009446620941162, acc 17.1875\n",
      "iteration 219 loss 2.838207244873047, acc 15.625\n",
      "iteration 220 loss 2.8326025009155273, acc 28.125\n",
      "iteration 221 loss 2.713249444961548, acc 25.0\n",
      "iteration 222 loss 2.912424087524414, acc 14.0625\n",
      "iteration 223 loss 3.0486960411071777, acc 9.375\n",
      "iteration 224 loss 2.6953518390655518, acc 21.875\n",
      "iteration 225 loss 2.9590296745300293, acc 18.75\n",
      "iteration 226 loss 2.9399826526641846, acc 15.625\n",
      "iteration 227 loss 2.872760772705078, acc 18.75\n",
      "iteration 228 loss 3.011137008666992, acc 14.0625\n",
      "iteration 229 loss 2.778632164001465, acc 25.0\n",
      "iteration 230 loss 2.826657295227051, acc 26.5625\n",
      "iteration 231 loss 2.706899881362915, acc 20.3125\n",
      "iteration 232 loss 2.868854284286499, acc 15.625\n",
      "iteration 233 loss 2.623725652694702, acc 21.875\n",
      "iteration 234 loss 2.7713770866394043, acc 21.875\n",
      "iteration 235 loss 2.9493956565856934, acc 23.4375\n",
      "iteration 236 loss 2.849673271179199, acc 20.3125\n",
      "iteration 237 loss 2.8285038471221924, acc 18.75\n",
      "iteration 238 loss 2.8076958656311035, acc 17.1875\n",
      "iteration 239 loss 2.984245538711548, acc 14.0625\n",
      "iteration 240 loss 2.88779354095459, acc 20.3125\n",
      "iteration 241 loss 2.6104209423065186, acc 29.6875\n",
      "iteration 242 loss 2.8537206649780273, acc 18.75\n",
      "iteration 243 loss 2.986929178237915, acc 17.1875\n",
      "iteration 244 loss 2.916813373565674, acc 12.5\n",
      "iteration 245 loss 2.7958126068115234, acc 20.3125\n",
      "iteration 246 loss 2.7974300384521484, acc 25.0\n",
      "iteration 247 loss 2.8690743446350098, acc 17.1875\n",
      "iteration 248 loss 2.8840718269348145, acc 17.1875\n",
      "iteration 249 loss 2.674286127090454, acc 23.4375\n",
      "iteration 250 loss 2.6758763790130615, acc 20.3125\n",
      "iteration 251 loss 2.665755271911621, acc 18.75\n",
      "iteration 252 loss 2.9451744556427, acc 12.5\n",
      "iteration 253 loss 2.8241968154907227, acc 23.4375\n",
      "iteration 254 loss 2.5721821784973145, acc 28.125\n",
      "iteration 255 loss 2.7601430416107178, acc 23.4375\n",
      "iteration 256 loss 2.7723090648651123, acc 17.1875\n",
      "iteration 257 loss 2.7270913124084473, acc 29.6875\n",
      "iteration 258 loss 2.9335989952087402, acc 17.1875\n",
      "iteration 259 loss 2.7886016368865967, acc 23.4375\n",
      "iteration 260 loss 2.732879877090454, acc 28.125\n",
      "iteration 261 loss 2.723426580429077, acc 29.6875\n",
      "iteration 262 loss 2.9759044647216797, acc 14.0625\n",
      "iteration 263 loss 2.7666406631469727, acc 26.5625\n",
      "iteration 264 loss 2.8113296031951904, acc 23.4375\n",
      "iteration 265 loss 2.88088059425354, acc 25.0\n",
      "iteration 266 loss 2.7645866870880127, acc 18.75\n",
      "iteration 267 loss 2.7918243408203125, acc 25.0\n",
      "iteration 268 loss 2.8931543827056885, acc 15.625\n",
      "iteration 269 loss 2.8103771209716797, acc 21.875\n",
      "iteration 270 loss 2.769333839416504, acc 21.875\n",
      "iteration 271 loss 2.8177196979522705, acc 21.875\n",
      "iteration 272 loss 2.922060966491699, acc 18.75\n",
      "iteration 273 loss 2.9797027111053467, acc 17.1875\n",
      "iteration 274 loss 2.9641356468200684, acc 15.625\n",
      "iteration 275 loss 2.891814947128296, acc 18.75\n",
      "iteration 276 loss 2.7777624130249023, acc 15.625\n",
      "iteration 277 loss 2.9250686168670654, acc 15.625\n",
      "iteration 278 loss 2.794396162033081, acc 20.3125\n",
      "iteration 279 loss 2.6661722660064697, acc 26.5625\n",
      "iteration 280 loss 2.873105049133301, acc 15.625\n",
      "iteration 281 loss 2.622859239578247, acc 31.25\n",
      "iteration 282 loss 2.624760150909424, acc 26.5625\n",
      "iteration 283 loss 2.9013423919677734, acc 17.1875\n",
      "iteration 284 loss 2.679387092590332, acc 20.3125\n",
      "iteration 285 loss 2.767324447631836, acc 17.1875\n",
      "iteration 286 loss 2.714370012283325, acc 23.4375\n",
      "iteration 287 loss 2.7299938201904297, acc 20.3125\n",
      "iteration 288 loss 2.7999958992004395, acc 20.3125\n",
      "iteration 289 loss 2.8833940029144287, acc 18.75\n",
      "iteration 290 loss 2.9075205326080322, acc 15.625\n",
      "iteration 291 loss 2.5554399490356445, acc 31.25\n",
      "iteration 292 loss 2.898329496383667, acc 12.5\n",
      "iteration 293 loss 2.877946376800537, acc 21.875\n",
      "iteration 294 loss 2.8447835445404053, acc 14.0625\n",
      "iteration 295 loss 2.6459507942199707, acc 26.5625\n",
      "iteration 296 loss 2.950563907623291, acc 10.9375\n",
      "iteration 297 loss 2.8369626998901367, acc 20.3125\n",
      "iteration 298 loss 2.667799711227417, acc 23.4375\n",
      "iteration 299 loss 2.8949241638183594, acc 14.0625\n",
      "iteration 300 loss 2.860713005065918, acc 12.5\n",
      "iteration 301 loss 2.787147045135498, acc 18.75\n",
      "iteration 302 loss 2.7812983989715576, acc 18.75\n",
      "iteration 303 loss 2.7037336826324463, acc 18.75\n",
      "iteration 304 loss 2.7490384578704834, acc 18.75\n",
      "iteration 305 loss 2.7064096927642822, acc 26.5625\n",
      "iteration 306 loss 3.1133322715759277, acc 12.5\n",
      "iteration 307 loss 2.7164111137390137, acc 21.875\n",
      "iteration 308 loss 2.801527261734009, acc 14.0625\n",
      "iteration 309 loss 2.7705605030059814, acc 21.875\n",
      "iteration 310 loss 2.789181709289551, acc 23.4375\n",
      "iteration 311 loss 2.7541420459747314, acc 17.1875\n",
      "iteration 312 loss 2.8647968769073486, acc 12.5\n",
      "iteration 313 loss 2.614875078201294, acc 26.5625\n",
      "iteration 314 loss 2.9242589473724365, acc 18.75\n",
      "iteration 315 loss 2.7954440116882324, acc 21.875\n",
      "iteration 316 loss 2.8183932304382324, acc 20.3125\n",
      "iteration 317 loss 2.7188947200775146, acc 21.875\n",
      "iteration 318 loss 2.849600315093994, acc 14.0625\n",
      "iteration 319 loss 2.685988426208496, acc 21.875\n",
      "iteration 320 loss 2.775956630706787, acc 23.4375\n",
      "iteration 321 loss 3.0181682109832764, acc 10.9375\n",
      "iteration 322 loss 2.892570734024048, acc 12.5\n",
      "iteration 323 loss 2.80100679397583, acc 12.5\n",
      "iteration 324 loss 2.820996046066284, acc 17.1875\n",
      "iteration 325 loss 2.8474619388580322, acc 23.4375\n",
      "iteration 326 loss 2.9135897159576416, acc 23.4375\n",
      "iteration 327 loss 2.8513760566711426, acc 18.75\n",
      "iteration 328 loss 2.628235340118408, acc 29.6875\n",
      "iteration 329 loss 2.6167056560516357, acc 29.6875\n",
      "iteration 330 loss 2.9844565391540527, acc 12.5\n",
      "iteration 331 loss 2.8159728050231934, acc 15.625\n",
      "iteration 332 loss 2.8322196006774902, acc 17.1875\n",
      "iteration 333 loss 2.7349071502685547, acc 25.0\n",
      "iteration 334 loss 2.9213247299194336, acc 12.5\n",
      "iteration 335 loss 2.8747549057006836, acc 17.1875\n",
      "iteration 336 loss 2.8349387645721436, acc 15.625\n",
      "iteration 337 loss 2.7057905197143555, acc 18.75\n",
      "iteration 338 loss 2.798905372619629, acc 18.75\n",
      "iteration 339 loss 2.790462017059326, acc 29.6875\n",
      "iteration 340 loss 2.7292025089263916, acc 18.75\n",
      "iteration 341 loss 2.739384889602661, acc 20.3125\n",
      "iteration 342 loss 2.759883165359497, acc 23.4375\n",
      "iteration 343 loss 2.6909713745117188, acc 23.4375\n",
      "iteration 344 loss 2.9888217449188232, acc 15.625\n",
      "iteration 345 loss 2.737593173980713, acc 26.5625\n",
      "iteration 346 loss 2.5815460681915283, acc 21.875\n",
      "iteration 347 loss 2.8896398544311523, acc 21.875\n",
      "iteration 348 loss 2.799293041229248, acc 14.0625\n",
      "iteration 349 loss 2.671553611755371, acc 25.0\n",
      "iteration 350 loss 3.1354806423187256, acc 9.375\n",
      "iteration 351 loss 2.6999974250793457, acc 26.5625\n",
      "iteration 352 loss 2.697380542755127, acc 32.8125\n",
      "iteration 353 loss 2.66097354888916, acc 29.6875\n",
      "iteration 354 loss 2.878777503967285, acc 18.75\n",
      "iteration 355 loss 2.6628174781799316, acc 23.4375\n",
      "iteration 356 loss 2.8309166431427, acc 20.3125\n",
      "iteration 357 loss 2.7214181423187256, acc 26.5625\n",
      "iteration 358 loss 2.7578282356262207, acc 21.875\n",
      "iteration 359 loss 2.5602526664733887, acc 31.25\n",
      "iteration 360 loss 2.8702971935272217, acc 17.1875\n",
      "iteration 361 loss 2.666038990020752, acc 21.875\n",
      "iteration 362 loss 2.882911205291748, acc 18.75\n",
      "iteration 363 loss 2.8643906116485596, acc 18.75\n",
      "iteration 364 loss 2.8947646617889404, acc 14.0625\n",
      "iteration 365 loss 2.7732644081115723, acc 18.75\n",
      "iteration 366 loss 2.805373191833496, acc 25.0\n",
      "iteration 367 loss 2.7487740516662598, acc 15.625\n",
      "iteration 368 loss 2.8300185203552246, acc 12.5\n",
      "iteration 369 loss 2.7521586418151855, acc 14.0625\n",
      "iteration 370 loss 2.6777255535125732, acc 25.0\n",
      "iteration 371 loss 2.873678684234619, acc 20.3125\n",
      "iteration 372 loss 2.750028133392334, acc 18.75\n",
      "iteration 373 loss 2.875753402709961, acc 14.0625\n",
      "iteration 374 loss 2.6220946311950684, acc 23.4375\n",
      "iteration 375 loss 2.6644797325134277, acc 25.0\n",
      "iteration 376 loss 2.830136299133301, acc 23.4375\n",
      "iteration 377 loss 3.0262274742126465, acc 9.375\n",
      "iteration 378 loss 2.773989677429199, acc 23.4375\n",
      "iteration 379 loss 2.542322874069214, acc 28.125\n",
      "iteration 380 loss 2.5726475715637207, acc 32.8125\n",
      "iteration 381 loss 2.9449069499969482, acc 20.3125\n",
      "iteration 382 loss 2.9063150882720947, acc 17.1875\n",
      "iteration 383 loss 2.721065044403076, acc 17.1875\n",
      "iteration 384 loss 2.8480730056762695, acc 15.625\n",
      "iteration 385 loss 2.6917543411254883, acc 23.4375\n",
      "iteration 386 loss 2.859008550643921, acc 17.1875\n",
      "iteration 387 loss 2.961010217666626, acc 14.0625\n",
      "iteration 388 loss 2.8690953254699707, acc 15.625\n",
      "iteration 389 loss 2.694202423095703, acc 20.3125\n",
      "iteration 390 loss 2.7820401191711426, acc 17.1875\n",
      "iteration 391 loss 2.5504164695739746, acc 29.6875\n",
      "iteration 392 loss 2.6174771785736084, acc 29.6875\n",
      "iteration 393 loss 2.906628370285034, acc 14.0625\n",
      "iteration 394 loss 2.9286229610443115, acc 15.625\n",
      "iteration 395 loss 2.7145204544067383, acc 26.5625\n",
      "iteration 396 loss 2.781188488006592, acc 20.3125\n",
      "iteration 397 loss 2.805205821990967, acc 18.75\n",
      "iteration 398 loss 2.860078811645508, acc 20.3125\n",
      "iteration 399 loss 2.812394142150879, acc 21.875\n",
      "iteration 400 loss 2.7789194583892822, acc 21.875\n",
      "iteration 401 loss 2.69728422164917, acc 23.4375\n",
      "iteration 402 loss 2.8059868812561035, acc 18.75\n",
      "iteration 403 loss 2.6975057125091553, acc 26.5625\n",
      "iteration 404 loss 2.685706615447998, acc 17.1875\n",
      "iteration 405 loss 2.661935567855835, acc 17.1875\n",
      "iteration 406 loss 2.681523323059082, acc 20.3125\n",
      "iteration 407 loss 2.9125723838806152, acc 18.75\n",
      "iteration 408 loss 2.7207276821136475, acc 21.875\n",
      "iteration 409 loss 2.7867431640625, acc 26.5625\n",
      "iteration 410 loss 2.718173027038574, acc 25.0\n",
      "iteration 411 loss 2.8031651973724365, acc 21.875\n",
      "iteration 412 loss 2.7371020317077637, acc 25.0\n",
      "iteration 413 loss 2.6549692153930664, acc 28.125\n",
      "iteration 414 loss 2.796093463897705, acc 17.1875\n",
      "iteration 415 loss 2.693819046020508, acc 20.3125\n",
      "iteration 416 loss 2.772061347961426, acc 18.75\n",
      "iteration 417 loss 2.8550376892089844, acc 21.875\n",
      "iteration 418 loss 2.6651549339294434, acc 28.125\n",
      "iteration 419 loss 3.116612195968628, acc 14.0625\n",
      "iteration 420 loss 2.634141445159912, acc 21.875\n",
      "iteration 421 loss 2.6875412464141846, acc 31.25\n",
      "iteration 422 loss 2.827545642852783, acc 18.75\n",
      "iteration 423 loss 2.7629168033599854, acc 18.75\n",
      "iteration 424 loss 2.788877487182617, acc 14.0625\n",
      "iteration 425 loss 2.7528719902038574, acc 29.6875\n",
      "iteration 426 loss 2.809969425201416, acc 20.3125\n",
      "iteration 427 loss 2.827181339263916, acc 18.75\n",
      "iteration 428 loss 2.6851422786712646, acc 23.4375\n",
      "iteration 429 loss 2.628289222717285, acc 31.25\n",
      "iteration 430 loss 2.759075164794922, acc 21.875\n",
      "iteration 431 loss 2.878199815750122, acc 18.75\n",
      "iteration 432 loss 2.8104560375213623, acc 20.3125\n",
      "iteration 433 loss 2.77856183052063, acc 18.75\n",
      "iteration 434 loss 2.79227876663208, acc 20.3125\n",
      "iteration 435 loss 2.8228700160980225, acc 15.625\n",
      "iteration 436 loss 2.7975552082061768, acc 20.3125\n",
      "iteration 437 loss 2.687770128250122, acc 28.125\n",
      "iteration 438 loss 2.5929152965545654, acc 25.0\n",
      "iteration 439 loss 2.869668483734131, acc 17.1875\n",
      "iteration 440 loss 2.7911577224731445, acc 17.1875\n",
      "iteration 441 loss 2.80018949508667, acc 21.875\n",
      "iteration 442 loss 2.873849391937256, acc 20.3125\n",
      "iteration 443 loss 2.79488468170166, acc 17.1875\n",
      "iteration 444 loss 2.764608860015869, acc 17.1875\n",
      "iteration 445 loss 2.7258224487304688, acc 21.875\n",
      "iteration 446 loss 2.8106489181518555, acc 20.3125\n",
      "iteration 447 loss 2.6817009449005127, acc 21.875\n",
      "iteration 448 loss 2.7707207202911377, acc 17.1875\n",
      "iteration 449 loss 2.704183340072632, acc 23.4375\n",
      "iteration 450 loss 2.8412833213806152, acc 18.75\n",
      "iteration 451 loss 2.617614984512329, acc 26.5625\n",
      "iteration 452 loss 2.5067055225372314, acc 32.8125\n",
      "iteration 453 loss 2.8220832347869873, acc 18.75\n",
      "iteration 454 loss 2.6894116401672363, acc 26.5625\n",
      "iteration 455 loss 2.921997547149658, acc 10.9375\n",
      "iteration 456 loss 2.8870046138763428, acc 15.625\n",
      "iteration 457 loss 2.6578800678253174, acc 20.3125\n",
      "iteration 458 loss 2.67187762260437, acc 23.4375\n",
      "iteration 459 loss 2.7564070224761963, acc 28.125\n",
      "iteration 460 loss 2.714381217956543, acc 21.875\n",
      "iteration 461 loss 2.8154947757720947, acc 18.75\n",
      "iteration 462 loss 2.830303430557251, acc 18.75\n",
      "iteration 463 loss 2.6655969619750977, acc 32.8125\n",
      "iteration 464 loss 2.809194803237915, acc 18.75\n",
      "iteration 465 loss 2.870936632156372, acc 10.9375\n",
      "iteration 466 loss 2.8069074153900146, acc 12.5\n",
      "iteration 467 loss 2.9101061820983887, acc 12.5\n",
      "iteration 468 loss 2.7949624061584473, acc 17.1875\n",
      "iteration 469 loss 2.8899693489074707, acc 18.75\n",
      "iteration 470 loss 2.7545223236083984, acc 25.0\n",
      "iteration 471 loss 2.8138973712921143, acc 21.875\n",
      "iteration 472 loss 2.7057712078094482, acc 15.625\n",
      "iteration 473 loss 2.8525829315185547, acc 14.0625\n",
      "iteration 474 loss 2.7526941299438477, acc 18.75\n",
      "iteration 475 loss 2.703272581100464, acc 25.0\n",
      "iteration 476 loss 2.7298479080200195, acc 21.875\n",
      "iteration 477 loss 2.6591105461120605, acc 23.4375\n",
      "iteration 478 loss 2.6393582820892334, acc 21.875\n",
      "iteration 479 loss 2.7482287883758545, acc 25.0\n",
      "iteration 480 loss 2.919332981109619, acc 14.0625\n",
      "iteration 481 loss 2.64959454536438, acc 20.3125\n",
      "iteration 482 loss 2.795628786087036, acc 12.5\n",
      "iteration 483 loss 2.6925442218780518, acc 15.625\n",
      "iteration 484 loss 2.6989552974700928, acc 14.0625\n",
      "iteration 485 loss 2.8806676864624023, acc 12.5\n",
      "iteration 486 loss 2.775585889816284, acc 12.5\n",
      "iteration 487 loss 2.7940847873687744, acc 23.4375\n",
      "iteration 488 loss 2.6717140674591064, acc 18.75\n",
      "iteration 489 loss 2.74570369720459, acc 17.1875\n",
      "iteration 490 loss 2.821887254714966, acc 20.3125\n",
      "iteration 491 loss 2.79408860206604, acc 12.5\n",
      "iteration 492 loss 2.9313671588897705, acc 10.9375\n",
      "iteration 493 loss 2.9624557495117188, acc 21.875\n",
      "iteration 494 loss 2.9058444499969482, acc 15.625\n",
      "iteration 495 loss 2.8679862022399902, acc 20.3125\n",
      "iteration 496 loss 2.7820985317230225, acc 23.4375\n",
      "iteration 497 loss 2.7484443187713623, acc 20.3125\n",
      "iteration 498 loss 2.756779432296753, acc 18.75\n",
      "iteration 499 loss 2.800813674926758, acc 21.875\n",
      "iteration 500 loss 2.5589613914489746, acc 29.6875\n",
      "iteration 501 loss 2.778615951538086, acc 18.75\n",
      "iteration 502 loss 2.7758917808532715, acc 23.4375\n",
      "iteration 503 loss 2.8141496181488037, acc 20.3125\n",
      "iteration 504 loss 2.7525863647460938, acc 26.5625\n",
      "iteration 505 loss 2.7895495891571045, acc 20.3125\n",
      "iteration 506 loss 2.852086305618286, acc 17.1875\n",
      "iteration 507 loss 2.6767678260803223, acc 18.75\n",
      "iteration 508 loss 2.750081777572632, acc 20.3125\n",
      "iteration 509 loss 2.6843812465667725, acc 20.3125\n",
      "iteration 510 loss 2.7342135906219482, acc 20.3125\n",
      "iteration 511 loss 2.953895330429077, acc 10.9375\n",
      "iteration 512 loss 2.6578943729400635, acc 31.25\n",
      "iteration 513 loss 2.739867687225342, acc 23.4375\n",
      "iteration 514 loss 2.7925305366516113, acc 20.3125\n",
      "iteration 515 loss 2.7098233699798584, acc 23.4375\n",
      "iteration 516 loss 2.876081705093384, acc 20.3125\n",
      "iteration 517 loss 2.882222890853882, acc 10.9375\n",
      "iteration 518 loss 2.7575552463531494, acc 18.75\n",
      "iteration 519 loss 2.815305709838867, acc 17.1875\n",
      "iteration 520 loss 2.6700286865234375, acc 28.125\n",
      "iteration 521 loss 2.677180290222168, acc 20.3125\n",
      "iteration 522 loss 2.635118246078491, acc 20.3125\n",
      "iteration 523 loss 2.6997547149658203, acc 21.875\n",
      "iteration 524 loss 2.8579511642456055, acc 20.3125\n",
      "iteration 525 loss 2.7186665534973145, acc 20.3125\n",
      "iteration 526 loss 2.7485361099243164, acc 17.1875\n",
      "iteration 527 loss 2.86029314994812, acc 15.625\n",
      "iteration 528 loss 2.882800817489624, acc 15.625\n",
      "iteration 529 loss 2.738905429840088, acc 20.3125\n",
      "iteration 530 loss 2.7547974586486816, acc 25.0\n",
      "iteration 531 loss 2.7137391567230225, acc 23.4375\n",
      "iteration 532 loss 2.797858238220215, acc 23.4375\n",
      "iteration 533 loss 2.811850070953369, acc 21.875\n",
      "iteration 534 loss 2.5573437213897705, acc 25.0\n",
      "iteration 535 loss 2.5625343322753906, acc 34.375\n",
      "iteration 536 loss 2.56156325340271, acc 23.4375\n",
      "iteration 537 loss 2.804680347442627, acc 21.875\n",
      "iteration 538 loss 2.944209337234497, acc 17.1875\n",
      "iteration 539 loss 2.924182176589966, acc 9.375\n",
      "iteration 540 loss 2.6827030181884766, acc 23.4375\n",
      "iteration 541 loss 2.6933138370513916, acc 28.125\n",
      "iteration 542 loss 2.802093982696533, acc 18.75\n",
      "iteration 543 loss 2.910788059234619, acc 18.75\n",
      "iteration 544 loss 2.7841150760650635, acc 17.1875\n",
      "iteration 545 loss 2.5276544094085693, acc 23.4375\n",
      "iteration 546 loss 2.8539345264434814, acc 9.375\n",
      "iteration 547 loss 2.8664042949676514, acc 15.625\n",
      "iteration 548 loss 2.656993865966797, acc 23.4375\n",
      "iteration 549 loss 2.865396022796631, acc 20.3125\n",
      "iteration 550 loss 2.899324417114258, acc 20.3125\n",
      "iteration 551 loss 2.719061851501465, acc 25.0\n",
      "iteration 552 loss 2.7815632820129395, acc 20.3125\n",
      "iteration 553 loss 2.7598965167999268, acc 25.0\n",
      "iteration 554 loss 2.846708059310913, acc 17.1875\n",
      "iteration 555 loss 2.935638904571533, acc 15.625\n",
      "iteration 556 loss 2.584444522857666, acc 21.875\n",
      "iteration 557 loss 2.743800163269043, acc 25.0\n",
      "iteration 558 loss 2.7042155265808105, acc 21.875\n",
      "iteration 559 loss 2.8496365547180176, acc 20.3125\n",
      "iteration 560 loss 2.7114217281341553, acc 25.0\n",
      "iteration 561 loss 2.8204941749572754, acc 20.3125\n",
      "iteration 562 loss 2.6605918407440186, acc 20.3125\n",
      "iteration 563 loss 2.610287666320801, acc 29.6875\n",
      "iteration 564 loss 2.8566510677337646, acc 14.0625\n",
      "iteration 565 loss 2.7599167823791504, acc 26.5625\n",
      "iteration 566 loss 2.910829782485962, acc 15.625\n",
      "iteration 567 loss 2.821845531463623, acc 15.625\n",
      "iteration 568 loss 2.8575925827026367, acc 17.1875\n",
      "iteration 569 loss 2.871274471282959, acc 10.9375\n",
      "iteration 570 loss 2.885105609893799, acc 14.0625\n",
      "iteration 571 loss 2.770711660385132, acc 18.75\n",
      "iteration 572 loss 2.7756521701812744, acc 21.875\n",
      "iteration 573 loss 2.8121304512023926, acc 17.1875\n",
      "iteration 574 loss 2.7377710342407227, acc 23.4375\n",
      "iteration 575 loss 3.0396628379821777, acc 15.625\n",
      "iteration 576 loss 2.882581949234009, acc 17.1875\n",
      "iteration 577 loss 2.829636573791504, acc 23.4375\n",
      "iteration 578 loss 2.6353206634521484, acc 26.5625\n",
      "iteration 579 loss 2.7726902961730957, acc 14.0625\n",
      "iteration 580 loss 2.747934579849243, acc 20.3125\n",
      "iteration 581 loss 2.927258014678955, acc 10.9375\n",
      "iteration 582 loss 2.777308464050293, acc 23.4375\n",
      "iteration 583 loss 2.89103364944458, acc 17.1875\n",
      "iteration 584 loss 2.7997937202453613, acc 17.1875\n",
      "iteration 585 loss 2.6113803386688232, acc 26.5625\n",
      "iteration 586 loss 2.878934383392334, acc 12.5\n",
      "iteration 587 loss 2.7897591590881348, acc 23.4375\n",
      "iteration 588 loss 2.6330389976501465, acc 23.4375\n",
      "iteration 589 loss 2.5450711250305176, acc 26.5625\n",
      "iteration 590 loss 2.794529914855957, acc 25.0\n",
      "iteration 591 loss 2.548595905303955, acc 23.4375\n",
      "iteration 592 loss 2.777209997177124, acc 17.1875\n",
      "iteration 593 loss 2.936664342880249, acc 14.0625\n",
      "iteration 594 loss 2.943150281906128, acc 18.75\n",
      "iteration 595 loss 2.6133153438568115, acc 25.0\n",
      "iteration 596 loss 2.724221706390381, acc 20.3125\n",
      "iteration 597 loss 3.011296510696411, acc 10.9375\n",
      "iteration 598 loss 2.7195630073547363, acc 23.4375\n",
      "iteration 599 loss 2.7567996978759766, acc 17.1875\n",
      "iteration 600 loss 2.726263999938965, acc 28.125\n",
      "iteration 601 loss 2.5685572624206543, acc 31.25\n",
      "iteration 602 loss 2.7528393268585205, acc 23.4375\n",
      "iteration 603 loss 2.626129627227783, acc 25.0\n",
      "iteration 604 loss 2.6102964878082275, acc 20.3125\n",
      "iteration 605 loss 2.738429546356201, acc 26.5625\n",
      "iteration 606 loss 2.6947779655456543, acc 17.1875\n",
      "iteration 607 loss 2.812379837036133, acc 23.4375\n",
      "iteration 608 loss 2.792315721511841, acc 15.625\n",
      "iteration 609 loss 2.755000114440918, acc 20.3125\n",
      "iteration 610 loss 2.665034294128418, acc 21.875\n",
      "iteration 611 loss 3.077342987060547, acc 14.0625\n",
      "iteration 612 loss 2.742424488067627, acc 23.4375\n",
      "iteration 613 loss 2.7608346939086914, acc 26.5625\n",
      "iteration 614 loss 2.7849464416503906, acc 20.3125\n",
      "iteration 615 loss 2.591095447540283, acc 29.6875\n",
      "iteration 616 loss 2.782346248626709, acc 25.0\n",
      "iteration 617 loss 2.5648326873779297, acc 29.6875\n",
      "iteration 618 loss 2.7980146408081055, acc 20.3125\n",
      "iteration 619 loss 2.8584208488464355, acc 10.9375\n",
      "iteration 620 loss 2.976466178894043, acc 14.0625\n",
      "iteration 621 loss 2.74609375, acc 15.625\n",
      "iteration 622 loss 2.6336140632629395, acc 17.1875\n",
      "iteration 623 loss 2.7832274436950684, acc 21.875\n",
      "iteration 624 loss 2.635943651199341, acc 29.6875\n",
      "iteration 625 loss 2.8290860652923584, acc 20.3125\n",
      "iteration 626 loss 2.9258270263671875, acc 21.875\n",
      "iteration 627 loss 2.81357741355896, acc 21.875\n",
      "iteration 628 loss 2.8870139122009277, acc 12.5\n",
      "iteration 629 loss 2.858219861984253, acc 23.4375\n",
      "iteration 630 loss 2.5446906089782715, acc 18.75\n",
      "iteration 631 loss 2.6391258239746094, acc 25.0\n",
      "iteration 632 loss 2.5840790271759033, acc 25.0\n",
      "iteration 633 loss 2.639925479888916, acc 20.3125\n",
      "iteration 634 loss 2.7830862998962402, acc 17.1875\n",
      "iteration 635 loss 2.658522605895996, acc 25.0\n",
      "iteration 636 loss 2.705605983734131, acc 20.3125\n",
      "iteration 637 loss 2.801459312438965, acc 17.1875\n",
      "iteration 638 loss 2.8341243267059326, acc 20.3125\n",
      "iteration 639 loss 2.8142426013946533, acc 12.5\n",
      "iteration 640 loss 2.713089942932129, acc 23.4375\n",
      "iteration 641 loss 2.7905385494232178, acc 15.625\n",
      "iteration 642 loss 2.6778483390808105, acc 20.3125\n",
      "iteration 643 loss 2.7112443447113037, acc 23.4375\n",
      "iteration 644 loss 2.5646133422851562, acc 26.5625\n",
      "iteration 645 loss 2.677053689956665, acc 21.875\n",
      "iteration 646 loss 2.573090076446533, acc 21.875\n",
      "iteration 647 loss 2.5819709300994873, acc 28.125\n",
      "iteration 648 loss 2.5172791481018066, acc 29.6875\n",
      "iteration 649 loss 2.642209768295288, acc 20.3125\n",
      "iteration 650 loss 2.719247579574585, acc 23.4375\n",
      "iteration 651 loss 2.978550672531128, acc 15.625\n",
      "iteration 652 loss 2.599982976913452, acc 20.3125\n",
      "iteration 653 loss 2.8674447536468506, acc 14.0625\n",
      "iteration 654 loss 2.634674310684204, acc 15.625\n",
      "iteration 655 loss 2.748263359069824, acc 15.625\n",
      "iteration 656 loss 2.767228364944458, acc 17.1875\n",
      "iteration 657 loss 2.8656914234161377, acc 25.0\n",
      "iteration 658 loss 2.53194522857666, acc 31.25\n",
      "iteration 659 loss 2.8161563873291016, acc 21.875\n",
      "iteration 660 loss 2.778153896331787, acc 18.75\n",
      "iteration 661 loss 2.654629707336426, acc 25.0\n",
      "iteration 662 loss 2.7236382961273193, acc 25.0\n",
      "iteration 663 loss 2.7310943603515625, acc 25.0\n",
      "iteration 664 loss 2.6204495429992676, acc 23.4375\n",
      "iteration 665 loss 2.878359317779541, acc 17.1875\n",
      "iteration 666 loss 2.5959112644195557, acc 18.75\n",
      "iteration 667 loss 2.7847487926483154, acc 12.5\n",
      "iteration 668 loss 2.7635269165039062, acc 14.0625\n",
      "iteration 669 loss 2.733414649963379, acc 29.6875\n",
      "iteration 670 loss 2.585378646850586, acc 25.0\n",
      "iteration 671 loss 2.7016756534576416, acc 25.0\n",
      "iteration 672 loss 2.6518309116363525, acc 26.5625\n",
      "iteration 673 loss 2.5825796127319336, acc 20.3125\n",
      "iteration 674 loss 2.766303539276123, acc 14.0625\n",
      "iteration 675 loss 2.892200469970703, acc 14.0625\n",
      "iteration 676 loss 2.7885897159576416, acc 21.875\n",
      "iteration 677 loss 2.813211441040039, acc 18.75\n",
      "iteration 678 loss 2.746490478515625, acc 21.875\n",
      "iteration 679 loss 2.653639316558838, acc 29.6875\n",
      "iteration 680 loss 2.768486499786377, acc 17.1875\n",
      "iteration 681 loss 2.599569797515869, acc 29.6875\n",
      "iteration 682 loss 2.863093614578247, acc 9.375\n",
      "iteration 683 loss 2.6882505416870117, acc 15.625\n",
      "iteration 684 loss 2.9393198490142822, acc 9.375\n",
      "iteration 685 loss 2.720810651779175, acc 17.1875\n",
      "iteration 686 loss 2.7632572650909424, acc 18.75\n",
      "iteration 687 loss 2.8905224800109863, acc 15.625\n",
      "iteration 688 loss 2.5751612186431885, acc 28.125\n",
      "iteration 689 loss 2.6419363021850586, acc 31.25\n",
      "iteration 690 loss 2.550121307373047, acc 29.6875\n",
      "iteration 691 loss 2.7877838611602783, acc 23.4375\n",
      "iteration 692 loss 2.6387054920196533, acc 28.125\n",
      "iteration 693 loss 2.8662097454071045, acc 14.0625\n",
      "iteration 694 loss 2.524279832839966, acc 28.125\n",
      "iteration 695 loss 2.560892105102539, acc 31.25\n",
      "iteration 696 loss 2.746873617172241, acc 21.875\n",
      "iteration 697 loss 2.662943124771118, acc 23.4375\n",
      "iteration 698 loss 2.6653287410736084, acc 20.3125\n",
      "iteration 699 loss 2.845618963241577, acc 15.625\n",
      "iteration 700 loss 2.682793378829956, acc 21.875\n",
      "iteration 701 loss 2.7274088859558105, acc 14.0625\n",
      "iteration 702 loss 2.688406229019165, acc 23.4375\n",
      "iteration 703 loss 2.8155062198638916, acc 18.75\n",
      "iteration 704 loss 2.774665355682373, acc 20.3125\n",
      "iteration 705 loss 3.0786728858947754, acc 9.375\n",
      "iteration 706 loss 2.7287721633911133, acc 20.3125\n",
      "iteration 707 loss 2.712144136428833, acc 28.125\n",
      "iteration 708 loss 2.7643070220947266, acc 23.4375\n",
      "iteration 709 loss 2.668574333190918, acc 23.4375\n",
      "iteration 710 loss 2.687771797180176, acc 15.625\n",
      "iteration 711 loss 2.7717790603637695, acc 21.875\n",
      "iteration 712 loss 2.747878313064575, acc 18.75\n",
      "iteration 713 loss 2.823714256286621, acc 12.5\n",
      "iteration 714 loss 2.6667115688323975, acc 18.75\n",
      "iteration 715 loss 2.750978469848633, acc 18.75\n",
      "iteration 716 loss 2.732844829559326, acc 29.6875\n",
      "iteration 717 loss 2.672822952270508, acc 23.4375\n",
      "iteration 718 loss 2.939746856689453, acc 12.5\n",
      "iteration 719 loss 2.860786199569702, acc 15.625\n",
      "iteration 720 loss 2.7697644233703613, acc 18.75\n",
      "iteration 721 loss 2.670642375946045, acc 21.875\n",
      "iteration 722 loss 2.743109703063965, acc 18.75\n",
      "iteration 723 loss 2.6726202964782715, acc 25.0\n",
      "iteration 724 loss 2.85148024559021, acc 20.3125\n",
      "iteration 725 loss 2.7115745544433594, acc 23.4375\n",
      "iteration 726 loss 2.997645616531372, acc 15.625\n",
      "iteration 727 loss 2.6759493350982666, acc 17.1875\n",
      "iteration 728 loss 2.8434157371520996, acc 15.625\n",
      "iteration 729 loss 2.7523412704467773, acc 12.5\n",
      "iteration 730 loss 2.812037467956543, acc 18.75\n",
      "iteration 731 loss 2.941039800643921, acc 17.1875\n",
      "iteration 732 loss 2.7269959449768066, acc 20.3125\n",
      "iteration 733 loss 2.619823455810547, acc 32.8125\n",
      "iteration 734 loss 2.7214653491973877, acc 25.0\n",
      "iteration 735 loss 2.662665605545044, acc 20.3125\n",
      "iteration 736 loss 2.9439845085144043, acc 17.1875\n",
      "iteration 737 loss 2.707677125930786, acc 23.4375\n",
      "iteration 738 loss 2.5429115295410156, acc 25.0\n",
      "iteration 739 loss 2.8637917041778564, acc 21.875\n",
      "iteration 740 loss 2.640180826187134, acc 15.625\n",
      "iteration 741 loss 2.727534770965576, acc 26.5625\n",
      "iteration 742 loss 2.533259868621826, acc 25.0\n",
      "iteration 743 loss 2.7055277824401855, acc 15.625\n",
      "iteration 744 loss 2.661768913269043, acc 14.0625\n",
      "iteration 745 loss 2.855107069015503, acc 17.1875\n",
      "iteration 746 loss 2.8656094074249268, acc 20.3125\n",
      "iteration 747 loss 2.868419647216797, acc 23.4375\n",
      "iteration 748 loss 2.639056921005249, acc 28.125\n",
      "iteration 749 loss 2.693660259246826, acc 25.0\n",
      "iteration 750 loss 2.8077902793884277, acc 17.1875\n",
      "iteration 751 loss 2.7749521732330322, acc 21.875\n",
      "iteration 752 loss 2.886688709259033, acc 18.75\n",
      "iteration 753 loss 2.7590742111206055, acc 14.0625\n",
      "iteration 754 loss 2.7901880741119385, acc 17.1875\n",
      "iteration 755 loss 2.6532132625579834, acc 20.3125\n",
      "iteration 756 loss 2.8212180137634277, acc 25.0\n",
      "iteration 757 loss 2.616349220275879, acc 25.0\n",
      "iteration 758 loss 2.6253457069396973, acc 21.875\n",
      "iteration 759 loss 2.734525442123413, acc 28.125\n",
      "iteration 760 loss 2.653672218322754, acc 23.4375\n",
      "iteration 761 loss 2.505722999572754, acc 31.25\n",
      "iteration 762 loss 2.5803062915802, acc 28.125\n",
      "iteration 763 loss 2.9565157890319824, acc 17.1875\n",
      "iteration 764 loss 2.6671218872070312, acc 23.4375\n",
      "iteration 765 loss 2.6473588943481445, acc 26.5625\n",
      "iteration 766 loss 2.9305074214935303, acc 18.75\n",
      "iteration 767 loss 2.5654618740081787, acc 25.0\n",
      "iteration 768 loss 2.527120590209961, acc 32.8125\n",
      "iteration 769 loss 2.6563849449157715, acc 25.0\n",
      "iteration 770 loss 2.99876070022583, acc 15.625\n",
      "iteration 771 loss 2.6940970420837402, acc 21.875\n",
      "iteration 772 loss 2.76222825050354, acc 18.75\n",
      "iteration 773 loss 2.6520493030548096, acc 25.0\n",
      "iteration 774 loss 2.759883403778076, acc 18.75\n",
      "iteration 775 loss 2.7906830310821533, acc 23.4375\n",
      "iteration 776 loss 2.759542942047119, acc 23.4375\n",
      "iteration 777 loss 2.7477152347564697, acc 15.625\n",
      "iteration 778 loss 2.6920506954193115, acc 26.5625\n",
      "iteration 779 loss 2.691166400909424, acc 26.5625\n",
      "iteration 780 loss 2.65913724899292, acc 26.5625\n",
      "iteration 781 loss 3.1051278114318848, acc 12.5\n",
      "iteration 782 loss 2.892096996307373, acc 15.625\n",
      "iteration 783 loss 2.784435987472534, acc 28.125\n",
      "iteration 784 loss 2.678825855255127, acc 20.3125\n",
      "iteration 785 loss 2.805068254470825, acc 18.75\n",
      "iteration 786 loss 2.7396488189697266, acc 21.875\n",
      "iteration 787 loss 2.6404058933258057, acc 18.75\n",
      "iteration 788 loss 2.7729179859161377, acc 18.75\n",
      "iteration 789 loss 2.5808298587799072, acc 17.1875\n",
      "iteration 790 loss 2.75211501121521, acc 14.0625\n",
      "iteration 791 loss 2.71319580078125, acc 31.25\n",
      "iteration 792 loss 2.7423694133758545, acc 18.75\n",
      "iteration 793 loss 2.6640169620513916, acc 25.0\n",
      "iteration 794 loss 2.8954179286956787, acc 12.5\n",
      "iteration 795 loss 2.6772072315216064, acc 21.875\n",
      "iteration 796 loss 2.8515100479125977, acc 17.1875\n",
      "iteration 797 loss 2.5434720516204834, acc 35.9375\n",
      "iteration 798 loss 2.8083479404449463, acc 21.875\n",
      "iteration 799 loss 2.8065345287323, acc 18.75\n",
      "iteration 800 loss 2.675283193588257, acc 21.875\n",
      "iteration 801 loss 2.576091766357422, acc 20.3125\n",
      "iteration 802 loss 2.6209347248077393, acc 20.3125\n",
      "iteration 803 loss 2.66021466255188, acc 18.75\n",
      "iteration 804 loss 2.5260097980499268, acc 26.5625\n",
      "iteration 805 loss 2.7987630367279053, acc 23.4375\n",
      "iteration 806 loss 2.7406256198883057, acc 18.75\n",
      "iteration 807 loss 2.67695951461792, acc 25.0\n",
      "iteration 808 loss 2.7758591175079346, acc 17.1875\n",
      "iteration 809 loss 2.4738540649414062, acc 26.5625\n",
      "iteration 810 loss 2.9700965881347656, acc 15.625\n",
      "iteration 811 loss 2.8099312782287598, acc 18.75\n",
      "iteration 812 loss 2.7604331970214844, acc 20.3125\n",
      "iteration 813 loss 2.6482927799224854, acc 18.75\n",
      "iteration 814 loss 2.619180202484131, acc 26.5625\n",
      "iteration 815 loss 2.76094126701355, acc 23.4375\n",
      "iteration 816 loss 2.777527093887329, acc 20.3125\n",
      "iteration 817 loss 2.817425489425659, acc 18.75\n",
      "iteration 818 loss 2.867107391357422, acc 14.0625\n",
      "iteration 819 loss 2.673220634460449, acc 25.0\n",
      "iteration 820 loss 2.710212230682373, acc 31.25\n",
      "iteration 821 loss 2.707214593887329, acc 20.3125\n",
      "iteration 822 loss 2.6875271797180176, acc 18.75\n",
      "iteration 823 loss 2.701650857925415, acc 21.875\n",
      "iteration 824 loss 2.7207934856414795, acc 17.1875\n",
      "iteration 825 loss 2.5651190280914307, acc 26.5625\n",
      "iteration 826 loss 2.6709790229797363, acc 18.75\n",
      "iteration 827 loss 2.6845710277557373, acc 25.0\n",
      "iteration 828 loss 2.627153158187866, acc 17.1875\n",
      "iteration 829 loss 2.7552874088287354, acc 21.875\n",
      "iteration 830 loss 2.664691686630249, acc 18.75\n",
      "iteration 831 loss 2.72448992729187, acc 17.1875\n",
      "iteration 832 loss 2.726572275161743, acc 20.3125\n",
      "iteration 833 loss 2.703892946243286, acc 20.3125\n",
      "iteration 834 loss 2.924128293991089, acc 14.0625\n",
      "iteration 835 loss 2.6541495323181152, acc 21.875\n",
      "iteration 836 loss 2.677560806274414, acc 18.75\n",
      "iteration 837 loss 2.6195807456970215, acc 28.125\n",
      "iteration 838 loss 2.8284263610839844, acc 15.625\n",
      "iteration 839 loss 3.024027109146118, acc 15.625\n",
      "iteration 840 loss 2.898026704788208, acc 10.9375\n",
      "iteration 841 loss 2.7204554080963135, acc 21.875\n",
      "iteration 842 loss 2.61742901802063, acc 34.375\n",
      "iteration 843 loss 2.7937049865722656, acc 17.1875\n",
      "iteration 844 loss 2.6720898151397705, acc 18.75\n",
      "iteration 845 loss 2.9439005851745605, acc 18.75\n",
      "iteration 846 loss 2.7201309204101562, acc 34.375\n",
      "iteration 847 loss 2.9761478900909424, acc 14.0625\n",
      "iteration 848 loss 2.788593292236328, acc 20.3125\n",
      "iteration 849 loss 2.8501949310302734, acc 9.375\n",
      "iteration 850 loss 2.7903788089752197, acc 14.0625\n",
      "iteration 851 loss 2.600386381149292, acc 25.0\n",
      "iteration 852 loss 2.815971612930298, acc 20.3125\n",
      "iteration 853 loss 2.8297014236450195, acc 17.1875\n",
      "iteration 854 loss 2.6948020458221436, acc 20.3125\n",
      "iteration 855 loss 2.7169740200042725, acc 26.5625\n",
      "iteration 856 loss 2.698024034500122, acc 25.0\n",
      "iteration 857 loss 2.6528115272521973, acc 25.0\n",
      "iteration 858 loss 2.857940196990967, acc 15.625\n",
      "iteration 859 loss 2.732609748840332, acc 15.625\n",
      "iteration 860 loss 3.0058114528656006, acc 10.9375\n",
      "iteration 861 loss 2.9535539150238037, acc 9.375\n",
      "iteration 862 loss 2.6886773109436035, acc 21.875\n",
      "iteration 863 loss 2.766448974609375, acc 23.4375\n",
      "iteration 864 loss 2.7634780406951904, acc 12.5\n",
      "iteration 865 loss 2.6975324153900146, acc 21.875\n",
      "iteration 866 loss 2.52716064453125, acc 26.5625\n",
      "iteration 867 loss 2.8227603435516357, acc 25.0\n",
      "iteration 868 loss 2.60591197013855, acc 26.5625\n",
      "iteration 869 loss 2.7425191402435303, acc 26.5625\n",
      "iteration 870 loss 2.687798500061035, acc 20.3125\n",
      "iteration 871 loss 2.728710651397705, acc 20.3125\n",
      "iteration 872 loss 2.5547425746917725, acc 21.875\n",
      "iteration 873 loss 2.921405076980591, acc 17.1875\n",
      "iteration 874 loss 2.926156520843506, acc 14.0625\n",
      "iteration 875 loss 2.813685655593872, acc 20.3125\n",
      "iteration 876 loss 3.0135059356689453, acc 15.625\n",
      "iteration 877 loss 2.7546439170837402, acc 15.625\n",
      "iteration 878 loss 2.731670618057251, acc 14.0625\n",
      "iteration 879 loss 2.7039012908935547, acc 25.0\n",
      "iteration 880 loss 2.7752835750579834, acc 20.3125\n",
      "iteration 881 loss 2.759199857711792, acc 21.875\n",
      "iteration 882 loss 2.886632204055786, acc 18.75\n",
      "iteration 883 loss 2.6770830154418945, acc 23.4375\n",
      "iteration 884 loss 2.7554805278778076, acc 21.875\n",
      "iteration 885 loss 2.7892677783966064, acc 21.875\n",
      "iteration 886 loss 2.6984715461730957, acc 20.3125\n",
      "iteration 887 loss 2.6506712436676025, acc 18.75\n",
      "iteration 888 loss 2.735642671585083, acc 23.4375\n",
      "iteration 889 loss 2.7696781158447266, acc 17.1875\n",
      "iteration 890 loss 2.7108707427978516, acc 23.4375\n",
      "iteration 891 loss 2.6341731548309326, acc 25.0\n",
      "iteration 892 loss 2.5796377658843994, acc 25.0\n",
      "iteration 893 loss 2.8376519680023193, acc 20.3125\n",
      "iteration 894 loss 2.7167727947235107, acc 23.4375\n",
      "iteration 895 loss 2.7864325046539307, acc 20.3125\n",
      "iteration 896 loss 2.5318069458007812, acc 29.6875\n",
      "iteration 897 loss 2.363770008087158, acc 34.375\n",
      "iteration 898 loss 2.8301875591278076, acc 18.75\n",
      "iteration 899 loss 2.9297075271606445, acc 14.0625\n",
      "iteration 900 loss 2.593806505203247, acc 28.125\n",
      "iteration 901 loss 2.6194889545440674, acc 23.4375\n",
      "iteration 902 loss 2.6906304359436035, acc 26.5625\n",
      "iteration 903 loss 2.689391613006592, acc 25.0\n",
      "iteration 904 loss 2.7785305976867676, acc 20.3125\n",
      "iteration 905 loss 2.7825427055358887, acc 12.5\n",
      "iteration 906 loss 2.808553695678711, acc 18.75\n",
      "iteration 907 loss 2.8274199962615967, acc 15.625\n",
      "iteration 908 loss 2.783935546875, acc 15.625\n",
      "iteration 909 loss 2.715155839920044, acc 26.5625\n",
      "iteration 910 loss 2.7246816158294678, acc 23.4375\n",
      "iteration 911 loss 2.9147555828094482, acc 18.75\n",
      "iteration 912 loss 2.6853291988372803, acc 26.5625\n",
      "iteration 913 loss 2.71018385887146, acc 23.4375\n",
      "iteration 914 loss 2.835118532180786, acc 17.1875\n",
      "iteration 915 loss 2.693992853164673, acc 25.0\n",
      "iteration 916 loss 2.894798994064331, acc 12.5\n",
      "iteration 917 loss 2.7455050945281982, acc 20.3125\n",
      "iteration 918 loss 2.684624671936035, acc 20.3125\n",
      "iteration 919 loss 2.658419609069824, acc 20.3125\n",
      "iteration 920 loss 2.6210622787475586, acc 25.0\n",
      "iteration 921 loss 2.6625661849975586, acc 18.75\n",
      "iteration 922 loss 2.9273879528045654, acc 18.75\n",
      "iteration 923 loss 2.6122028827667236, acc 23.4375\n",
      "iteration 924 loss 2.7159347534179688, acc 18.75\n",
      "iteration 925 loss 2.8104910850524902, acc 20.3125\n",
      "iteration 926 loss 2.704812526702881, acc 21.875\n",
      "iteration 927 loss 2.616673707962036, acc 25.0\n",
      "iteration 928 loss 2.7800087928771973, acc 20.3125\n",
      "iteration 929 loss 2.622293710708618, acc 26.5625\n",
      "iteration 930 loss 2.7071242332458496, acc 20.3125\n",
      "iteration 931 loss 2.4195311069488525, acc 32.8125\n",
      "iteration 932 loss 2.5970613956451416, acc 26.5625\n",
      "iteration 933 loss 2.7412209510803223, acc 18.75\n",
      "iteration 934 loss 2.638807535171509, acc 25.0\n",
      "iteration 935 loss 2.755707025527954, acc 15.625\n",
      "iteration 936 loss 2.514267683029175, acc 25.0\n",
      "iteration 937 loss 2.766975164413452, acc 18.75\n",
      "iteration 938 loss 2.795219898223877, acc 18.75\n",
      "iteration 939 loss 2.8533191680908203, acc 21.875\n",
      "iteration 940 loss 2.6760878562927246, acc 23.4375\n",
      "iteration 941 loss 2.8611338138580322, acc 14.0625\n",
      "iteration 942 loss 2.648994207382202, acc 18.75\n",
      "iteration 943 loss 2.7084550857543945, acc 18.75\n",
      "iteration 944 loss 2.7510342597961426, acc 18.75\n",
      "iteration 945 loss 2.8620011806488037, acc 15.625\n",
      "iteration 946 loss 2.881967544555664, acc 6.25\n",
      "iteration 947 loss 2.715562582015991, acc 14.0625\n",
      "iteration 948 loss 2.8683066368103027, acc 15.625\n",
      "iteration 949 loss 2.571645975112915, acc 23.4375\n",
      "iteration 950 loss 3.0528852939605713, acc 10.9375\n",
      "iteration 951 loss 2.890282392501831, acc 15.625\n",
      "iteration 952 loss 2.5829787254333496, acc 28.125\n",
      "iteration 953 loss 2.622699022293091, acc 23.4375\n",
      "iteration 954 loss 2.696089744567871, acc 26.5625\n",
      "iteration 955 loss 2.7972781658172607, acc 25.0\n",
      "iteration 956 loss 2.4912266731262207, acc 26.5625\n",
      "iteration 957 loss 2.8768372535705566, acc 18.75\n",
      "iteration 958 loss 2.9494247436523438, acc 18.75\n",
      "iteration 959 loss 2.970256805419922, acc 18.75\n",
      "iteration 960 loss 2.819307327270508, acc 25.0\n",
      "iteration 961 loss 2.631258487701416, acc 18.75\n",
      "iteration 962 loss 2.8424019813537598, acc 14.0625\n",
      "iteration 963 loss 2.6606624126434326, acc 17.1875\n",
      "iteration 964 loss 2.670389175415039, acc 26.5625\n",
      "iteration 965 loss 2.6984593868255615, acc 29.6875\n",
      "iteration 966 loss 2.8462026119232178, acc 17.1875\n",
      "iteration 967 loss 2.7784974575042725, acc 20.3125\n",
      "iteration 968 loss 2.6336066722869873, acc 25.0\n",
      "iteration 969 loss 2.6317834854125977, acc 21.875\n",
      "iteration 970 loss 2.807007312774658, acc 21.875\n",
      "iteration 971 loss 2.6013290882110596, acc 26.5625\n",
      "iteration 972 loss 2.6600284576416016, acc 28.125\n",
      "iteration 973 loss 2.753322124481201, acc 20.3125\n",
      "iteration 974 loss 2.631092071533203, acc 20.3125\n",
      "iteration 975 loss 2.8308990001678467, acc 20.3125\n",
      "iteration 976 loss 2.8103556632995605, acc 18.75\n",
      "iteration 977 loss 2.947460174560547, acc 15.625\n",
      "iteration 978 loss 2.7351624965667725, acc 21.875\n",
      "iteration 979 loss 2.6201624870300293, acc 26.5625\n",
      "iteration 980 loss 2.669111728668213, acc 17.1875\n",
      "iteration 981 loss 2.6299057006835938, acc 20.3125\n",
      "iteration 982 loss 2.7654716968536377, acc 20.3125\n",
      "iteration 983 loss 2.649573564529419, acc 20.3125\n",
      "iteration 984 loss 2.6114084720611572, acc 31.25\n",
      "iteration 985 loss 2.5532190799713135, acc 26.5625\n",
      "iteration 986 loss 2.871166706085205, acc 15.625\n",
      "iteration 987 loss 2.740201234817505, acc 18.75\n",
      "iteration 988 loss 2.7534334659576416, acc 23.4375\n",
      "iteration 989 loss 2.649550199508667, acc 17.1875\n",
      "iteration 990 loss 2.8071677684783936, acc 20.3125\n",
      "iteration 991 loss 2.5895771980285645, acc 18.75\n",
      "iteration 992 loss 2.782783031463623, acc 20.3125\n",
      "iteration 993 loss 2.660907030105591, acc 18.75\n",
      "iteration 994 loss 2.8335442543029785, acc 20.3125\n",
      "iteration 995 loss 2.835056781768799, acc 18.75\n",
      "iteration 996 loss 2.8760979175567627, acc 14.0625\n",
      "iteration 997 loss 2.7218151092529297, acc 20.3125\n",
      "iteration 998 loss 2.8476195335388184, acc 12.5\n",
      "iteration 999 loss 2.8050527572631836, acc 23.4375\n",
      "iteration 1000 loss 2.5542399883270264, acc 20.3125\n",
      "iteration 1001 loss 2.74462890625, acc 20.3125\n",
      "iteration 1002 loss 2.8690409660339355, acc 15.625\n",
      "iteration 1003 loss 2.7172977924346924, acc 23.4375\n",
      "iteration 1004 loss 2.684323787689209, acc 21.875\n",
      "iteration 1005 loss 2.659325122833252, acc 29.6875\n",
      "iteration 1006 loss 2.81658935546875, acc 17.1875\n",
      "iteration 1007 loss 2.5440969467163086, acc 29.6875\n",
      "iteration 1008 loss 2.857168197631836, acc 21.875\n",
      "iteration 1009 loss 2.818134307861328, acc 21.875\n",
      "iteration 1010 loss 2.687014102935791, acc 25.0\n",
      "iteration 1011 loss 2.6122679710388184, acc 25.0\n",
      "iteration 1012 loss 2.6843698024749756, acc 26.5625\n",
      "iteration 1013 loss 2.592327356338501, acc 23.4375\n",
      "iteration 1014 loss 2.7688772678375244, acc 20.3125\n",
      "iteration 1015 loss 2.684924602508545, acc 18.75\n",
      "iteration 1016 loss 2.7161223888397217, acc 25.0\n",
      "iteration 1017 loss 2.6438815593719482, acc 28.125\n",
      "iteration 1018 loss 2.6302614212036133, acc 20.3125\n",
      "iteration 1019 loss 2.653442621231079, acc 18.75\n",
      "iteration 1020 loss 2.659736394882202, acc 15.625\n",
      "iteration 1021 loss 2.608309745788574, acc 18.75\n",
      "iteration 1022 loss 2.7455360889434814, acc 20.3125\n",
      "iteration 1023 loss 2.9637815952301025, acc 6.25\n",
      "iteration 1024 loss 2.6309852600097656, acc 15.625\n",
      "iteration 1025 loss 2.6410722732543945, acc 15.625\n",
      "iteration 1026 loss 2.7290139198303223, acc 17.1875\n",
      "iteration 1027 loss 2.6020309925079346, acc 23.4375\n",
      "iteration 1028 loss 2.903480291366577, acc 18.75\n",
      "iteration 1029 loss 2.8758175373077393, acc 21.875\n",
      "iteration 1030 loss 2.6444108486175537, acc 25.0\n",
      "iteration 1031 loss 2.699000358581543, acc 18.75\n",
      "iteration 1032 loss 2.7135918140411377, acc 18.75\n",
      "iteration 1033 loss 2.7325432300567627, acc 20.3125\n",
      "iteration 1034 loss 2.8685028553009033, acc 18.75\n",
      "iteration 1035 loss 2.6471164226531982, acc 15.625\n",
      "iteration 1036 loss 2.4780197143554688, acc 25.0\n",
      "iteration 1037 loss 2.717155933380127, acc 23.4375\n",
      "iteration 1038 loss 2.7430546283721924, acc 14.0625\n",
      "iteration 1039 loss 2.6351375579833984, acc 20.3125\n",
      "iteration 1040 loss 2.744328022003174, acc 17.1875\n",
      "iteration 1041 loss 2.5837552547454834, acc 21.875\n",
      "iteration 1042 loss 2.6561622619628906, acc 28.125\n",
      "iteration 1043 loss 2.7241222858428955, acc 18.75\n",
      "iteration 1044 loss 2.8095500469207764, acc 15.625\n",
      "iteration 1045 loss 2.823791980743408, acc 15.625\n",
      "iteration 1046 loss 2.7302091121673584, acc 17.1875\n",
      "iteration 1047 loss 2.7001583576202393, acc 23.4375\n",
      "iteration 1048 loss 2.6787912845611572, acc 26.5625\n",
      "iteration 1049 loss 2.7571399211883545, acc 18.75\n",
      "iteration 1050 loss 2.6966371536254883, acc 20.3125\n",
      "iteration 1051 loss 2.8087656497955322, acc 15.625\n",
      "iteration 1052 loss 2.6727163791656494, acc 23.4375\n",
      "iteration 1053 loss 2.6154396533966064, acc 25.0\n",
      "iteration 1054 loss 2.9484190940856934, acc 23.4375\n",
      "iteration 1055 loss 2.938875913619995, acc 14.0625\n",
      "iteration 1056 loss 2.9296469688415527, acc 21.875\n",
      "iteration 1057 loss 2.833867073059082, acc 14.0625\n",
      "iteration 1058 loss 2.80236554145813, acc 17.1875\n",
      "iteration 1059 loss 2.767310619354248, acc 23.4375\n",
      "iteration 1060 loss 2.671445369720459, acc 20.3125\n",
      "iteration 1061 loss 2.7231295108795166, acc 20.3125\n",
      "iteration 1062 loss 2.631716012954712, acc 25.0\n",
      "iteration 1063 loss 2.7717301845550537, acc 28.125\n",
      "iteration 1064 loss 2.6582565307617188, acc 26.5625\n",
      "iteration 1065 loss 2.6610279083251953, acc 23.4375\n",
      "iteration 1066 loss 2.6304070949554443, acc 31.25\n",
      "iteration 1067 loss 2.8089280128479004, acc 18.75\n",
      "iteration 1068 loss 2.7181389331817627, acc 21.875\n",
      "iteration 1069 loss 2.651074171066284, acc 25.0\n",
      "iteration 1070 loss 2.7865400314331055, acc 17.1875\n",
      "iteration 1071 loss 2.798168420791626, acc 20.3125\n",
      "iteration 1072 loss 2.6119489669799805, acc 26.5625\n",
      "iteration 1073 loss 2.5399138927459717, acc 25.0\n",
      "iteration 1074 loss 2.7072970867156982, acc 20.3125\n",
      "iteration 1075 loss 2.8194198608398438, acc 17.1875\n",
      "iteration 1076 loss 2.6074070930480957, acc 25.0\n",
      "iteration 1077 loss 2.7729601860046387, acc 18.75\n",
      "iteration 1078 loss 2.4741873741149902, acc 25.0\n",
      "iteration 1079 loss 2.908724069595337, acc 12.5\n",
      "iteration 1080 loss 2.5158960819244385, acc 28.125\n",
      "iteration 1081 loss 2.81670880317688, acc 20.3125\n",
      "iteration 1082 loss 2.7369558811187744, acc 21.875\n",
      "iteration 1083 loss 2.7223637104034424, acc 18.75\n",
      "iteration 1084 loss 2.6797587871551514, acc 15.625\n",
      "iteration 1085 loss 2.495213031768799, acc 35.9375\n",
      "iteration 1086 loss 2.5953264236450195, acc 23.4375\n",
      "iteration 1087 loss 2.338118314743042, acc 34.375\n",
      "iteration 1088 loss 2.7254292964935303, acc 20.3125\n",
      "iteration 1089 loss 2.6780190467834473, acc 28.125\n",
      "iteration 1090 loss 2.744361400604248, acc 23.4375\n",
      "iteration 1091 loss 2.7760329246520996, acc 17.1875\n",
      "iteration 1092 loss 2.7319259643554688, acc 26.5625\n",
      "iteration 1093 loss 2.6564197540283203, acc 23.4375\n",
      "iteration 1094 loss 2.8493332862854004, acc 23.4375\n",
      "iteration 1095 loss 2.680806875228882, acc 23.4375\n",
      "iteration 1096 loss 2.762758493423462, acc 23.4375\n",
      "iteration 1097 loss 2.686858892440796, acc 23.4375\n",
      "iteration 1098 loss 2.6931724548339844, acc 20.3125\n",
      "iteration 1099 loss 2.610409736633301, acc 23.4375\n",
      "iteration 1100 loss 2.688614845275879, acc 26.5625\n",
      "iteration 1101 loss 2.5126688480377197, acc 25.0\n",
      "iteration 1102 loss 2.749798059463501, acc 20.3125\n",
      "iteration 1103 loss 2.6864054203033447, acc 20.3125\n",
      "iteration 1104 loss 2.6390013694763184, acc 26.5625\n",
      "iteration 1105 loss 2.7089757919311523, acc 23.4375\n",
      "iteration 1106 loss 2.727952718734741, acc 20.3125\n",
      "iteration 1107 loss 2.7942469120025635, acc 14.0625\n",
      "iteration 1108 loss 2.8587589263916016, acc 17.1875\n",
      "iteration 1109 loss 2.687624454498291, acc 21.875\n",
      "iteration 1110 loss 2.742011308670044, acc 21.875\n",
      "iteration 1111 loss 2.4975156784057617, acc 31.25\n",
      "iteration 1112 loss 2.611851453781128, acc 29.6875\n",
      "iteration 1113 loss 2.882205009460449, acc 18.75\n",
      "iteration 1114 loss 2.6768288612365723, acc 23.4375\n",
      "iteration 1115 loss 2.8765461444854736, acc 14.0625\n",
      "iteration 1116 loss 2.687743902206421, acc 21.875\n",
      "iteration 1117 loss 2.6620101928710938, acc 23.4375\n",
      "iteration 1118 loss 2.636509656906128, acc 18.75\n",
      "iteration 1119 loss 2.7172677516937256, acc 21.875\n",
      "iteration 1120 loss 2.6420977115631104, acc 20.3125\n",
      "iteration 1121 loss 2.7460012435913086, acc 17.1875\n",
      "iteration 1122 loss 2.7646477222442627, acc 23.4375\n",
      "iteration 1123 loss 2.873345136642456, acc 15.625\n",
      "iteration 1124 loss 2.633894443511963, acc 21.875\n",
      "iteration 1125 loss 2.5648558139801025, acc 23.4375\n",
      "iteration 1126 loss 2.738492965698242, acc 18.75\n",
      "iteration 1127 loss 2.90854549407959, acc 12.5\n",
      "iteration 1128 loss 2.646087646484375, acc 21.875\n",
      "iteration 1129 loss 2.8117825984954834, acc 9.375\n",
      "iteration 1130 loss 2.7738025188446045, acc 17.1875\n",
      "iteration 1131 loss 2.7383780479431152, acc 20.3125\n",
      "iteration 1132 loss 2.7563185691833496, acc 14.0625\n",
      "iteration 1133 loss 2.676215171813965, acc 18.75\n",
      "iteration 1134 loss 2.5062808990478516, acc 20.3125\n",
      "iteration 1135 loss 2.6925418376922607, acc 20.3125\n",
      "iteration 1136 loss 2.561211347579956, acc 18.75\n",
      "iteration 1137 loss 2.737334966659546, acc 20.3125\n",
      "iteration 1138 loss 2.8028690814971924, acc 18.75\n",
      "iteration 1139 loss 2.7434728145599365, acc 21.875\n",
      "iteration 1140 loss 2.6746630668640137, acc 29.6875\n",
      "iteration 1141 loss 2.750067710876465, acc 23.4375\n",
      "iteration 1142 loss 2.614243268966675, acc 23.4375\n",
      "iteration 1143 loss 2.8112783432006836, acc 21.875\n",
      "iteration 1144 loss 2.6662466526031494, acc 25.0\n",
      "iteration 1145 loss 2.8700308799743652, acc 14.0625\n",
      "iteration 1146 loss 2.655165672302246, acc 28.125\n",
      "iteration 1147 loss 2.8420732021331787, acc 14.0625\n",
      "iteration 1148 loss 2.575044870376587, acc 25.0\n",
      "iteration 1149 loss 2.886945962905884, acc 15.625\n",
      "iteration 1150 loss 2.692598581314087, acc 20.3125\n",
      "iteration 1151 loss 2.7522995471954346, acc 15.625\n",
      "iteration 1152 loss 2.6232833862304688, acc 26.5625\n",
      "iteration 1153 loss 2.77982759475708, acc 17.1875\n",
      "iteration 1154 loss 2.949697971343994, acc 10.9375\n",
      "iteration 1155 loss 2.6545679569244385, acc 26.5625\n",
      "iteration 1156 loss 2.639880657196045, acc 15.625\n",
      "iteration 1157 loss 2.731626510620117, acc 14.0625\n",
      "iteration 1158 loss 2.6818909645080566, acc 21.875\n",
      "iteration 1159 loss 2.6234281063079834, acc 28.125\n",
      "iteration 1160 loss 2.7229108810424805, acc 21.875\n",
      "iteration 1161 loss 2.5433783531188965, acc 21.875\n",
      "iteration 1162 loss 2.7011609077453613, acc 29.6875\n",
      "iteration 1163 loss 2.7590503692626953, acc 17.1875\n",
      "iteration 1164 loss 2.698986291885376, acc 25.0\n",
      "iteration 1165 loss 2.6856274604797363, acc 28.125\n",
      "iteration 1166 loss 2.752943515777588, acc 20.3125\n",
      "iteration 1167 loss 2.8092637062072754, acc 14.0625\n",
      "iteration 1168 loss 2.728013515472412, acc 23.4375\n",
      "iteration 1169 loss 2.8058972358703613, acc 14.0625\n",
      "iteration 1170 loss 2.8785552978515625, acc 10.9375\n",
      "iteration 1171 loss 2.8015079498291016, acc 25.0\n",
      "iteration 1172 loss 2.9860599040985107, acc 9.375\n",
      "iteration 1173 loss 2.58273983001709, acc 23.4375\n",
      "iteration 1174 loss 2.611069440841675, acc 26.5625\n",
      "iteration 1175 loss 2.8439111709594727, acc 12.5\n",
      "iteration 1176 loss 2.620823383331299, acc 29.6875\n",
      "iteration 1177 loss 2.4487686157226562, acc 28.125\n",
      "iteration 1178 loss 2.834665060043335, acc 20.3125\n",
      "iteration 1179 loss 2.7158331871032715, acc 20.3125\n",
      "iteration 1180 loss 2.8248238563537598, acc 18.75\n",
      "iteration 1181 loss 2.921701192855835, acc 20.3125\n",
      "iteration 1182 loss 2.6834845542907715, acc 18.75\n",
      "iteration 1183 loss 2.5616555213928223, acc 31.25\n",
      "iteration 1184 loss 2.810621976852417, acc 12.5\n",
      "iteration 1185 loss 2.788489580154419, acc 20.3125\n",
      "iteration 1186 loss 2.6452627182006836, acc 26.5625\n",
      "iteration 1187 loss 2.6975162029266357, acc 23.4375\n",
      "iteration 1188 loss 2.6581480503082275, acc 25.0\n",
      "iteration 1189 loss 3.1158740520477295, acc 15.625\n",
      "iteration 1190 loss 2.7784104347229004, acc 15.625\n",
      "iteration 1191 loss 2.666623115539551, acc 21.875\n",
      "iteration 1192 loss 2.5938074588775635, acc 21.875\n",
      "iteration 1193 loss 2.4730231761932373, acc 26.5625\n",
      "iteration 1194 loss 2.7436580657958984, acc 12.5\n",
      "iteration 1195 loss 2.678107976913452, acc 14.0625\n",
      "iteration 1196 loss 2.706089735031128, acc 15.625\n",
      "iteration 1197 loss 2.7681565284729004, acc 25.0\n",
      "iteration 1198 loss 2.6449432373046875, acc 29.6875\n",
      "iteration 1199 loss 2.6834843158721924, acc 23.4375\n",
      "iteration 1200 loss 2.523327589035034, acc 25.0\n",
      "iteration 1201 loss 2.7901947498321533, acc 26.5625\n",
      "iteration 1202 loss 2.688103675842285, acc 23.4375\n",
      "iteration 1203 loss 2.853854179382324, acc 23.4375\n",
      "iteration 1204 loss 2.6360039710998535, acc 20.3125\n",
      "iteration 1205 loss 2.766740322113037, acc 15.625\n",
      "iteration 1206 loss 2.803593635559082, acc 21.875\n",
      "iteration 1207 loss 2.576939344406128, acc 23.4375\n",
      "iteration 1208 loss 2.6463842391967773, acc 20.3125\n",
      "iteration 1209 loss 2.6886837482452393, acc 25.0\n",
      "iteration 1210 loss 2.8254199028015137, acc 17.1875\n",
      "iteration 1211 loss 2.9464597702026367, acc 21.875\n",
      "iteration 1212 loss 2.792947769165039, acc 26.5625\n",
      "iteration 1213 loss 2.8413476943969727, acc 15.625\n",
      "iteration 1214 loss 2.7029366493225098, acc 17.1875\n",
      "iteration 1215 loss 2.841144561767578, acc 14.0625\n",
      "iteration 1216 loss 2.533323049545288, acc 26.5625\n",
      "iteration 1217 loss 2.891204595565796, acc 17.1875\n",
      "iteration 1218 loss 2.6218161582946777, acc 17.1875\n",
      "iteration 1219 loss 2.7957229614257812, acc 26.5625\n",
      "iteration 1220 loss 2.8290624618530273, acc 17.1875\n",
      "iteration 1221 loss 2.6807987689971924, acc 23.4375\n",
      "iteration 1222 loss 2.9542236328125, acc 17.1875\n",
      "iteration 1223 loss 2.4553794860839844, acc 31.25\n",
      "iteration 1224 loss 2.6968750953674316, acc 25.0\n",
      "iteration 1225 loss 2.6805336475372314, acc 23.4375\n",
      "iteration 1226 loss 2.7077455520629883, acc 26.5625\n",
      "iteration 1227 loss 2.885469675064087, acc 18.75\n",
      "iteration 1228 loss 2.6962265968322754, acc 18.75\n",
      "iteration 1229 loss 2.6141810417175293, acc 29.6875\n",
      "iteration 1230 loss 2.6869027614593506, acc 18.75\n",
      "iteration 1231 loss 2.713121175765991, acc 21.875\n",
      "iteration 1232 loss 2.833139657974243, acc 17.1875\n",
      "iteration 1233 loss 2.6777350902557373, acc 20.3125\n",
      "iteration 1234 loss 2.6345577239990234, acc 21.875\n",
      "iteration 1235 loss 2.6102609634399414, acc 17.1875\n",
      "iteration 1236 loss 2.740720748901367, acc 25.0\n",
      "iteration 1237 loss 2.6211769580841064, acc 25.0\n",
      "iteration 1238 loss 2.4725356101989746, acc 37.5\n",
      "iteration 1239 loss 2.6441280841827393, acc 26.5625\n",
      "iteration 1240 loss 2.849581718444824, acc 20.3125\n",
      "iteration 1241 loss 2.631972312927246, acc 26.5625\n",
      "iteration 1242 loss 2.7176425457000732, acc 20.3125\n",
      "iteration 1243 loss 2.7151365280151367, acc 17.1875\n",
      "iteration 1244 loss 2.7367091178894043, acc 21.875\n",
      "iteration 1245 loss 2.9210033416748047, acc 18.75\n",
      "iteration 1246 loss 2.556520462036133, acc 23.4375\n",
      "iteration 1247 loss 2.672623872756958, acc 25.0\n",
      "iteration 1248 loss 2.6242377758026123, acc 20.3125\n",
      "iteration 1249 loss 2.49220609664917, acc 26.5625\n",
      "iteration 1250 loss 2.6920883655548096, acc 25.0\n",
      "iteration 1251 loss 2.803994655609131, acc 18.75\n",
      "iteration 1252 loss 2.7880706787109375, acc 20.3125\n",
      "iteration 1253 loss 2.6976354122161865, acc 28.125\n",
      "iteration 1254 loss 2.7639267444610596, acc 14.0625\n",
      "iteration 1255 loss 2.6837317943573, acc 31.25\n",
      "iteration 1256 loss 2.548746347427368, acc 29.6875\n",
      "iteration 1257 loss 2.4484806060791016, acc 34.375\n",
      "iteration 1258 loss 2.6420605182647705, acc 18.75\n",
      "iteration 1259 loss 2.469130039215088, acc 28.125\n",
      "iteration 1260 loss 2.692157506942749, acc 18.75\n",
      "iteration 1261 loss 2.8003978729248047, acc 15.625\n",
      "iteration 1262 loss 2.6617703437805176, acc 15.625\n",
      "iteration 1263 loss 2.836782932281494, acc 17.1875\n",
      "iteration 1264 loss 2.861396551132202, acc 18.75\n",
      "iteration 1265 loss 2.8513572216033936, acc 21.875\n",
      "iteration 1266 loss 2.8237435817718506, acc 15.625\n",
      "iteration 1267 loss 2.533135414123535, acc 28.125\n",
      "iteration 1268 loss 2.736278533935547, acc 25.0\n",
      "iteration 1269 loss 2.6328673362731934, acc 18.75\n",
      "iteration 1270 loss 2.731339931488037, acc 21.875\n",
      "iteration 1271 loss 2.6645402908325195, acc 26.5625\n",
      "iteration 1272 loss 2.5154495239257812, acc 28.125\n",
      "iteration 1273 loss 2.8232152462005615, acc 21.875\n",
      "iteration 1274 loss 2.502209186553955, acc 23.4375\n",
      "iteration 1275 loss 2.9019248485565186, acc 20.3125\n",
      "iteration 1276 loss 2.648806095123291, acc 26.5625\n",
      "iteration 1277 loss 2.6382176876068115, acc 20.3125\n",
      "iteration 1278 loss 2.7492496967315674, acc 18.75\n",
      "iteration 1279 loss 2.4937267303466797, acc 31.25\n",
      "iteration 1280 loss 2.791063070297241, acc 14.0625\n",
      "iteration 1281 loss 2.6096103191375732, acc 28.125\n",
      "iteration 1282 loss 2.6448113918304443, acc 26.5625\n",
      "iteration 1283 loss 2.7002336978912354, acc 15.625\n",
      "iteration 1284 loss 2.8206632137298584, acc 14.0625\n",
      "iteration 1285 loss 2.9140377044677734, acc 14.0625\n",
      "iteration 1286 loss 2.6534264087677, acc 26.5625\n",
      "iteration 1287 loss 2.646254777908325, acc 23.4375\n",
      "iteration 1288 loss 2.6663362979888916, acc 18.75\n",
      "iteration 1289 loss 2.7617292404174805, acc 25.0\n",
      "iteration 1290 loss 2.8509037494659424, acc 23.4375\n",
      "iteration 1291 loss 2.781902551651001, acc 15.625\n",
      "iteration 1292 loss 2.7481207847595215, acc 15.625\n",
      "iteration 1293 loss 2.800746202468872, acc 25.0\n",
      "iteration 1294 loss 2.756478786468506, acc 12.5\n",
      "iteration 1295 loss 2.614453077316284, acc 28.125\n",
      "iteration 1296 loss 2.675126075744629, acc 21.875\n",
      "iteration 1297 loss 2.767428398132324, acc 20.3125\n",
      "iteration 1298 loss 2.663478374481201, acc 21.875\n",
      "iteration 1299 loss 2.550917387008667, acc 28.125\n",
      "iteration 1300 loss 2.7893941402435303, acc 21.875\n",
      "iteration 1301 loss 2.545030117034912, acc 28.125\n",
      "iteration 1302 loss 2.5025782585144043, acc 26.5625\n",
      "iteration 1303 loss 2.7149219512939453, acc 25.0\n",
      "iteration 1304 loss 2.7000250816345215, acc 26.5625\n",
      "iteration 1305 loss 2.6359992027282715, acc 15.625\n",
      "iteration 1306 loss 2.6505191326141357, acc 15.625\n",
      "iteration 1307 loss 2.70593523979187, acc 23.4375\n",
      "iteration 1308 loss 2.728123426437378, acc 23.4375\n",
      "iteration 1309 loss 2.8161416053771973, acc 18.75\n",
      "iteration 1310 loss 2.6189136505126953, acc 25.0\n",
      "iteration 1311 loss 2.6779956817626953, acc 21.875\n",
      "iteration 1312 loss 2.708791494369507, acc 29.6875\n",
      "iteration 1313 loss 2.5991227626800537, acc 25.0\n",
      "iteration 1314 loss 2.8206799030303955, acc 26.5625\n",
      "iteration 1315 loss 2.8761725425720215, acc 21.875\n",
      "iteration 1316 loss 2.578657388687134, acc 28.125\n",
      "iteration 1317 loss 2.7169435024261475, acc 26.5625\n",
      "iteration 1318 loss 2.9574124813079834, acc 17.1875\n",
      "iteration 1319 loss 2.9124555587768555, acc 9.375\n",
      "iteration 1320 loss 2.821690559387207, acc 15.625\n",
      "iteration 1321 loss 2.705559730529785, acc 25.0\n",
      "iteration 1322 loss 2.6465930938720703, acc 26.5625\n",
      "iteration 1323 loss 2.717649221420288, acc 25.0\n",
      "iteration 1324 loss 2.7783541679382324, acc 18.75\n",
      "iteration 1325 loss 2.5564961433410645, acc 21.875\n",
      "iteration 1326 loss 2.84434175491333, acc 17.1875\n",
      "iteration 1327 loss 2.710442543029785, acc 23.4375\n",
      "iteration 1328 loss 2.632035732269287, acc 23.4375\n",
      "iteration 1329 loss 2.5183637142181396, acc 32.8125\n",
      "iteration 1330 loss 2.7283854484558105, acc 17.1875\n",
      "iteration 1331 loss 2.841111183166504, acc 15.625\n",
      "iteration 1332 loss 2.714337110519409, acc 18.75\n",
      "iteration 1333 loss 2.6898550987243652, acc 17.1875\n",
      "iteration 1334 loss 2.700679063796997, acc 20.3125\n",
      "iteration 1335 loss 2.743959665298462, acc 23.4375\n",
      "iteration 1336 loss 2.645683526992798, acc 21.875\n",
      "iteration 1337 loss 2.853161096572876, acc 23.4375\n",
      "iteration 1338 loss 2.6956372261047363, acc 20.3125\n",
      "iteration 1339 loss 2.7970685958862305, acc 21.875\n",
      "iteration 1340 loss 2.7161965370178223, acc 10.9375\n",
      "iteration 1341 loss 2.7312259674072266, acc 14.0625\n",
      "iteration 1342 loss 2.760925054550171, acc 20.3125\n",
      "iteration 1343 loss 2.8993074893951416, acc 17.1875\n",
      "iteration 1344 loss 2.7084615230560303, acc 20.3125\n",
      "iteration 1345 loss 2.825181007385254, acc 23.4375\n",
      "iteration 1346 loss 2.751157522201538, acc 18.75\n",
      "iteration 1347 loss 2.9734668731689453, acc 14.0625\n",
      "iteration 1348 loss 2.7612385749816895, acc 25.0\n",
      "iteration 1349 loss 2.800828218460083, acc 23.4375\n",
      "iteration 1350 loss 2.711413621902466, acc 18.75\n",
      "iteration 1351 loss 2.6190638542175293, acc 15.625\n",
      "iteration 1352 loss 2.6740031242370605, acc 29.6875\n",
      "iteration 1353 loss 2.575125217437744, acc 23.4375\n",
      "iteration 1354 loss 2.9442524909973145, acc 20.3125\n",
      "iteration 1355 loss 2.630260705947876, acc 17.1875\n",
      "iteration 1356 loss 2.6058788299560547, acc 25.0\n",
      "iteration 1357 loss 2.4265589714050293, acc 37.5\n",
      "iteration 1358 loss 2.869717836380005, acc 21.875\n",
      "iteration 1359 loss 2.8899223804473877, acc 18.75\n",
      "iteration 1360 loss 2.7897934913635254, acc 21.875\n",
      "iteration 1361 loss 2.7154624462127686, acc 25.0\n",
      "iteration 1362 loss 2.868816375732422, acc 14.0625\n",
      "iteration 1363 loss 2.549591302871704, acc 21.875\n",
      "iteration 1364 loss 2.7623443603515625, acc 23.4375\n",
      "iteration 1365 loss 2.7501540184020996, acc 23.4375\n",
      "iteration 1366 loss 2.757983684539795, acc 21.875\n",
      "iteration 1367 loss 2.6026158332824707, acc 23.4375\n",
      "iteration 1368 loss 2.8697657585144043, acc 21.875\n",
      "iteration 1369 loss 2.6686840057373047, acc 28.125\n",
      "iteration 1370 loss 2.480663537979126, acc 34.375\n",
      "iteration 1371 loss 2.690138816833496, acc 21.875\n",
      "iteration 1372 loss 2.5880446434020996, acc 28.125\n",
      "iteration 1373 loss 2.961456298828125, acc 17.1875\n",
      "iteration 1374 loss 2.940293312072754, acc 20.3125\n",
      "iteration 1375 loss 2.7669150829315186, acc 23.4375\n",
      "iteration 1376 loss 2.735166549682617, acc 25.0\n",
      "iteration 1377 loss 2.563898801803589, acc 21.875\n",
      "iteration 1378 loss 2.8135101795196533, acc 10.9375\n",
      "iteration 1379 loss 2.784503936767578, acc 7.8125\n",
      "iteration 1380 loss 2.801245927810669, acc 6.25\n",
      "iteration 1381 loss 2.8086884021759033, acc 15.625\n",
      "iteration 1382 loss 2.7148478031158447, acc 23.4375\n",
      "iteration 1383 loss 2.775035858154297, acc 17.1875\n",
      "iteration 1384 loss 2.7591805458068848, acc 21.875\n",
      "iteration 1385 loss 2.687185049057007, acc 28.125\n",
      "iteration 1386 loss 2.5288000106811523, acc 28.125\n",
      "iteration 1387 loss 2.712477207183838, acc 21.875\n",
      "iteration 1388 loss 2.6344380378723145, acc 21.875\n",
      "iteration 1389 loss 2.7673497200012207, acc 20.3125\n",
      "iteration 1390 loss 2.7690625190734863, acc 20.3125\n",
      "iteration 1391 loss 2.7163195610046387, acc 21.875\n",
      "iteration 1392 loss 2.525357723236084, acc 23.4375\n",
      "iteration 1393 loss 2.7744486331939697, acc 20.3125\n",
      "iteration 1394 loss 2.9594039916992188, acc 10.9375\n",
      "iteration 1395 loss 2.6407201290130615, acc 32.8125\n",
      "iteration 1396 loss 2.7026138305664062, acc 18.75\n",
      "iteration 1397 loss 2.9647762775421143, acc 9.375\n",
      "iteration 1398 loss 2.756990909576416, acc 20.3125\n",
      "iteration 1399 loss 2.68227481842041, acc 18.75\n",
      "iteration 1400 loss 2.686527967453003, acc 23.4375\n",
      "iteration 1401 loss 2.8466928005218506, acc 18.75\n",
      "iteration 1402 loss 2.7959489822387695, acc 15.625\n",
      "iteration 1403 loss 2.738314628601074, acc 21.875\n",
      "iteration 1404 loss 2.744753360748291, acc 17.1875\n",
      "iteration 1405 loss 2.8351945877075195, acc 23.4375\n",
      "iteration 1406 loss 2.664987564086914, acc 21.875\n",
      "iteration 1407 loss 2.878911018371582, acc 23.4375\n",
      "iteration 1408 loss 2.8958816528320312, acc 18.75\n",
      "iteration 1409 loss 2.5625977516174316, acc 25.0\n",
      "iteration 1410 loss 2.9390599727630615, acc 12.5\n",
      "iteration 1411 loss 2.5614399909973145, acc 25.0\n",
      "iteration 1412 loss 2.7048559188842773, acc 21.875\n",
      "iteration 1413 loss 2.578141927719116, acc 28.125\n",
      "iteration 1414 loss 2.7771129608154297, acc 18.75\n",
      "iteration 1415 loss 2.5340983867645264, acc 21.875\n",
      "iteration 1416 loss 2.694101095199585, acc 21.875\n",
      "iteration 1417 loss 2.678331136703491, acc 26.5625\n",
      "iteration 1418 loss 2.733768939971924, acc 17.1875\n",
      "iteration 1419 loss 2.5611815452575684, acc 23.4375\n",
      "iteration 1420 loss 2.581252098083496, acc 28.125\n",
      "iteration 1421 loss 2.6877574920654297, acc 17.1875\n",
      "iteration 1422 loss 2.7405009269714355, acc 18.75\n",
      "iteration 1423 loss 2.6974070072174072, acc 25.0\n",
      "iteration 1424 loss 2.7268869876861572, acc 21.875\n",
      "iteration 1425 loss 2.711679220199585, acc 25.0\n",
      "iteration 1426 loss 2.4305665493011475, acc 29.6875\n",
      "iteration 1427 loss 2.8775603771209717, acc 18.75\n",
      "iteration 1428 loss 2.8493309020996094, acc 18.75\n",
      "iteration 1429 loss 2.7458391189575195, acc 18.75\n",
      "iteration 1430 loss 2.7994816303253174, acc 15.625\n",
      "iteration 1431 loss 2.6523337364196777, acc 28.125\n",
      "iteration 1432 loss 2.664689064025879, acc 21.875\n",
      "iteration 1433 loss 2.596510887145996, acc 18.75\n",
      "iteration 1434 loss 2.7404422760009766, acc 23.4375\n",
      "iteration 1435 loss 2.708131790161133, acc 17.1875\n",
      "iteration 1436 loss 2.7378439903259277, acc 26.5625\n",
      "iteration 1437 loss 2.860513210296631, acc 18.75\n",
      "iteration 1438 loss 2.7711105346679688, acc 18.75\n",
      "iteration 1439 loss 2.589015483856201, acc 18.75\n",
      "iteration 1440 loss 2.4988369941711426, acc 28.125\n",
      "iteration 1441 loss 2.7699265480041504, acc 21.875\n",
      "iteration 1442 loss 2.6808719635009766, acc 15.625\n",
      "iteration 1443 loss 2.666086196899414, acc 12.5\n",
      "iteration 1444 loss 2.6647708415985107, acc 7.8125\n",
      "iteration 1445 loss 2.6672329902648926, acc 29.6875\n",
      "iteration 1446 loss 2.587294340133667, acc 26.5625\n",
      "iteration 1447 loss 2.5197994709014893, acc 29.6875\n",
      "iteration 1448 loss 2.76949143409729, acc 32.8125\n",
      "iteration 1449 loss 2.768362045288086, acc 21.875\n",
      "iteration 1450 loss 3.0004029273986816, acc 10.9375\n",
      "iteration 1451 loss 2.8054215908050537, acc 18.75\n",
      "iteration 1452 loss 2.4714701175689697, acc 32.8125\n",
      "iteration 1453 loss 2.5758445262908936, acc 31.25\n",
      "iteration 1454 loss 2.70342755317688, acc 20.3125\n",
      "iteration 1455 loss 2.690619707107544, acc 20.3125\n",
      "iteration 1456 loss 2.6170899868011475, acc 21.875\n",
      "iteration 1457 loss 2.8210296630859375, acc 14.0625\n",
      "iteration 1458 loss 2.611053943634033, acc 21.875\n",
      "iteration 1459 loss 2.6690011024475098, acc 20.3125\n",
      "iteration 1460 loss 2.639378547668457, acc 23.4375\n",
      "iteration 1461 loss 2.5492279529571533, acc 21.875\n",
      "iteration 1462 loss 2.6973907947540283, acc 26.5625\n",
      "iteration 1463 loss 2.7218821048736572, acc 21.875\n",
      "iteration 1464 loss 2.549213171005249, acc 25.0\n",
      "iteration 1465 loss 2.6839852333068848, acc 25.0\n",
      "iteration 1466 loss 2.6402649879455566, acc 21.875\n",
      "iteration 1467 loss 2.6325509548187256, acc 21.875\n",
      "iteration 1468 loss 2.7187864780426025, acc 23.4375\n",
      "iteration 1469 loss 2.7138774394989014, acc 18.75\n",
      "iteration 1470 loss 2.7312073707580566, acc 10.9375\n",
      "iteration 1471 loss 2.821324110031128, acc 14.0625\n",
      "iteration 1472 loss 2.7498466968536377, acc 23.4375\n",
      "iteration 1473 loss 2.5782110691070557, acc 17.1875\n",
      "iteration 1474 loss 2.7737410068511963, acc 23.4375\n",
      "iteration 1475 loss 2.840663433074951, acc 21.875\n",
      "iteration 1476 loss 2.770434856414795, acc 20.3125\n",
      "iteration 1477 loss 2.724874258041382, acc 18.75\n",
      "iteration 1478 loss 2.7615115642547607, acc 18.75\n",
      "iteration 1479 loss 2.729146957397461, acc 7.8125\n",
      "iteration 1480 loss 2.7039875984191895, acc 9.375\n",
      "iteration 1481 loss 2.9964587688446045, acc 15.625\n",
      "iteration 1482 loss 2.6751458644866943, acc 23.4375\n",
      "iteration 1483 loss 2.7202813625335693, acc 18.75\n",
      "iteration 1484 loss 2.7578251361846924, acc 17.1875\n",
      "iteration 1485 loss 2.734041213989258, acc 18.75\n",
      "iteration 1486 loss 2.4491169452667236, acc 28.125\n",
      "iteration 1487 loss 2.668653726577759, acc 25.0\n",
      "iteration 1488 loss 2.495800256729126, acc 35.9375\n",
      "iteration 1489 loss 2.8534188270568848, acc 26.5625\n",
      "iteration 1490 loss 2.7030301094055176, acc 14.0625\n",
      "iteration 1491 loss 2.6422839164733887, acc 21.875\n",
      "iteration 1492 loss 2.4983327388763428, acc 23.4375\n",
      "iteration 1493 loss 2.899158477783203, acc 17.1875\n",
      "iteration 1494 loss 2.5781636238098145, acc 26.5625\n",
      "iteration 1495 loss 2.953528642654419, acc 9.375\n",
      "iteration 1496 loss 2.8318331241607666, acc 17.1875\n",
      "iteration 1497 loss 2.6774914264678955, acc 31.25\n",
      "iteration 1498 loss 2.8922388553619385, acc 12.5\n",
      "iteration 1499 loss 2.6603527069091797, acc 17.1875\n",
      "iteration 1500 loss 2.688922643661499, acc 18.75\n",
      "iteration 1501 loss 2.8455421924591064, acc 18.75\n",
      "iteration 1502 loss 2.8528428077697754, acc 20.3125\n",
      "iteration 1503 loss 2.699186086654663, acc 17.1875\n",
      "iteration 1504 loss 2.81284499168396, acc 12.5\n",
      "iteration 1505 loss 2.579907178878784, acc 31.25\n",
      "iteration 1506 loss 2.622835636138916, acc 18.75\n",
      "iteration 1507 loss 2.9793331623077393, acc 12.5\n",
      "iteration 1508 loss 2.6707561016082764, acc 15.625\n",
      "iteration 1509 loss 2.75850772857666, acc 20.3125\n",
      "iteration 1510 loss 2.781611680984497, acc 14.0625\n",
      "iteration 1511 loss 2.6776630878448486, acc 17.1875\n",
      "iteration 1512 loss 2.5169758796691895, acc 29.6875\n",
      "iteration 1513 loss 2.8633763790130615, acc 10.9375\n",
      "iteration 1514 loss 2.7102012634277344, acc 26.5625\n",
      "iteration 1515 loss 2.8193721771240234, acc 29.6875\n",
      "iteration 1516 loss 2.771121025085449, acc 23.4375\n",
      "iteration 1517 loss 2.7260031700134277, acc 17.1875\n",
      "iteration 1518 loss 2.824800729751587, acc 18.75\n",
      "iteration 1519 loss 2.6981732845306396, acc 25.0\n",
      "iteration 1520 loss 2.9407429695129395, acc 14.0625\n",
      "iteration 1521 loss 2.6949665546417236, acc 18.75\n",
      "iteration 1522 loss 2.712430953979492, acc 17.1875\n",
      "iteration 1523 loss 2.6892876625061035, acc 17.1875\n",
      "iteration 1524 loss 2.656661033630371, acc 25.0\n",
      "iteration 1525 loss 2.656275987625122, acc 20.3125\n",
      "iteration 1526 loss 2.6998281478881836, acc 23.4375\n",
      "iteration 1527 loss 2.6799070835113525, acc 18.75\n",
      "iteration 1528 loss 2.6422784328460693, acc 25.0\n",
      "iteration 1529 loss 2.7935945987701416, acc 18.75\n",
      "iteration 1530 loss 2.7066943645477295, acc 20.3125\n",
      "iteration 1531 loss 2.597809314727783, acc 20.3125\n",
      "iteration 1532 loss 2.728891372680664, acc 15.625\n",
      "iteration 1533 loss 2.698493242263794, acc 23.4375\n",
      "iteration 1534 loss 2.713413953781128, acc 21.875\n",
      "iteration 1535 loss 2.6534817218780518, acc 28.125\n",
      "iteration 1536 loss 2.7885243892669678, acc 25.0\n",
      "iteration 1537 loss 2.5900328159332275, acc 28.125\n",
      "iteration 1538 loss 2.8415513038635254, acc 15.625\n",
      "iteration 1539 loss 2.668682336807251, acc 23.4375\n",
      "iteration 1540 loss 2.7050442695617676, acc 25.0\n",
      "iteration 1541 loss 2.5541486740112305, acc 26.5625\n",
      "iteration 1542 loss 2.720874309539795, acc 12.5\n",
      "iteration 1543 loss 2.7691521644592285, acc 15.625\n",
      "iteration 1544 loss 2.805328845977783, acc 18.75\n",
      "iteration 1545 loss 2.7923130989074707, acc 21.875\n",
      "iteration 1546 loss 2.765148162841797, acc 15.625\n",
      "iteration 1547 loss 2.769850492477417, acc 21.875\n",
      "iteration 1548 loss 2.672377586364746, acc 20.3125\n",
      "iteration 1549 loss 2.7420125007629395, acc 20.3125\n",
      "iteration 1550 loss 2.466498374938965, acc 28.125\n",
      "iteration 1551 loss 2.7760050296783447, acc 17.1875\n",
      "iteration 1552 loss 2.850553274154663, acc 18.75\n",
      "iteration 1553 loss 2.6179726123809814, acc 29.6875\n",
      "iteration 1554 loss 2.706584930419922, acc 17.1875\n",
      "iteration 1555 loss 2.6448826789855957, acc 23.4375\n",
      "iteration 1556 loss 2.6562376022338867, acc 21.875\n",
      "iteration 1557 loss 2.8663313388824463, acc 18.75\n",
      "iteration 1558 loss 2.6021010875701904, acc 25.0\n",
      "iteration 1559 loss 2.696695327758789, acc 21.875\n",
      "iteration 1560 loss 2.814342498779297, acc 18.75\n",
      "iteration 1561 loss 2.6586647033691406, acc 25.0\n",
      "iteration 1562 loss 2.843139410018921, acc 17.1875\n",
      "iteration 1563 loss 2.633474588394165, acc 20.3125\n",
      "iteration 1564 loss 2.6610751152038574, acc 28.125\n",
      "iteration 1565 loss 2.703608751296997, acc 15.625\n",
      "iteration 1566 loss 2.8346140384674072, acc 21.875\n",
      "iteration 1567 loss 2.5941572189331055, acc 29.6875\n",
      "iteration 1568 loss 2.745250701904297, acc 28.125\n",
      "iteration 1569 loss 2.755133867263794, acc 18.75\n",
      "iteration 1570 loss 2.8133649826049805, acc 18.75\n",
      "iteration 1571 loss 2.549208402633667, acc 35.9375\n",
      "iteration 1572 loss 2.7340238094329834, acc 20.3125\n",
      "iteration 1573 loss 2.7938425540924072, acc 18.75\n",
      "iteration 1574 loss 2.72168231010437, acc 18.75\n",
      "iteration 1575 loss 2.8333468437194824, acc 12.5\n",
      "iteration 1576 loss 2.720093250274658, acc 15.625\n",
      "iteration 1577 loss 2.6383419036865234, acc 29.6875\n",
      "iteration 1578 loss 2.572138547897339, acc 25.0\n",
      "iteration 1579 loss 2.7785959243774414, acc 17.1875\n",
      "iteration 1580 loss 2.5379061698913574, acc 23.4375\n",
      "iteration 1581 loss 2.740084409713745, acc 18.75\n",
      "iteration 1582 loss 2.6805357933044434, acc 15.625\n",
      "iteration 1583 loss 2.852036714553833, acc 15.625\n",
      "iteration 1584 loss 2.886361598968506, acc 17.1875\n",
      "iteration 1585 loss 2.802518367767334, acc 12.5\n",
      "iteration 1586 loss 2.5993475914001465, acc 20.3125\n",
      "iteration 1587 loss 2.8645081520080566, acc 15.625\n",
      "iteration 1588 loss 2.732051372528076, acc 14.0625\n",
      "iteration 1589 loss 2.7680842876434326, acc 18.75\n",
      "iteration 1590 loss 2.7104649543762207, acc 15.625\n",
      "iteration 1591 loss 2.714292526245117, acc 18.75\n",
      "iteration 1592 loss 2.7518136501312256, acc 12.5\n",
      "iteration 1593 loss 2.7490413188934326, acc 23.4375\n",
      "iteration 1594 loss 2.642238140106201, acc 25.0\n",
      "iteration 1595 loss 2.5306389331817627, acc 25.0\n",
      "iteration 1596 loss 2.7411043643951416, acc 20.3125\n",
      "iteration 1597 loss 2.7786471843719482, acc 17.1875\n",
      "iteration 1598 loss 2.742457866668701, acc 17.1875\n",
      "iteration 1599 loss 2.7861714363098145, acc 12.5\n",
      "iteration 1600 loss 2.712886095046997, acc 21.875\n",
      "iteration 1601 loss 2.8703525066375732, acc 15.625\n",
      "iteration 1602 loss 2.725898504257202, acc 18.75\n",
      "iteration 1603 loss 2.669785976409912, acc 25.0\n",
      "iteration 1604 loss 2.5884175300598145, acc 28.125\n",
      "iteration 1605 loss 2.856736660003662, acc 18.75\n",
      "iteration 1606 loss 2.406909227371216, acc 28.125\n",
      "iteration 1607 loss 2.799240827560425, acc 17.1875\n",
      "iteration 1608 loss 2.6236801147460938, acc 28.125\n",
      "iteration 1609 loss 3.014345169067383, acc 9.375\n",
      "iteration 1610 loss 2.904576539993286, acc 20.3125\n",
      "iteration 1611 loss 2.6200344562530518, acc 23.4375\n",
      "iteration 1612 loss 2.80730938911438, acc 21.875\n",
      "iteration 1613 loss 2.6784727573394775, acc 31.25\n",
      "iteration 1614 loss 2.725757360458374, acc 18.75\n",
      "iteration 1615 loss 2.5949041843414307, acc 32.8125\n",
      "iteration 1616 loss 2.621169328689575, acc 21.875\n",
      "iteration 1617 loss 2.857328176498413, acc 18.75\n",
      "iteration 1618 loss 2.6203649044036865, acc 21.875\n",
      "iteration 1619 loss 2.8042399883270264, acc 20.3125\n",
      "iteration 1620 loss 2.777188539505005, acc 21.875\n",
      "iteration 1621 loss 2.640263080596924, acc 18.75\n",
      "iteration 1622 loss 2.7158284187316895, acc 21.875\n",
      "iteration 1623 loss 2.8725900650024414, acc 17.1875\n",
      "iteration 1624 loss 2.86645770072937, acc 21.875\n",
      "iteration 1625 loss 2.682358980178833, acc 25.0\n",
      "iteration 1626 loss 3.010913848876953, acc 12.5\n",
      "iteration 1627 loss 2.7222647666931152, acc 12.5\n",
      "iteration 1628 loss 2.6540377140045166, acc 25.0\n",
      "iteration 1629 loss 2.567243814468384, acc 21.875\n",
      "iteration 1630 loss 2.8397364616394043, acc 20.3125\n",
      "iteration 1631 loss 2.564042091369629, acc 15.625\n",
      "iteration 1632 loss 2.736060380935669, acc 17.1875\n",
      "iteration 1633 loss 2.5327322483062744, acc 32.8125\n",
      "iteration 1634 loss 2.690134048461914, acc 18.75\n",
      "iteration 1635 loss 2.461489200592041, acc 37.5\n",
      "iteration 1636 loss 2.8347725868225098, acc 14.0625\n",
      "iteration 1637 loss 2.7265055179595947, acc 18.75\n",
      "iteration 1638 loss 2.727581262588501, acc 25.0\n",
      "iteration 1639 loss 2.7940351963043213, acc 20.3125\n",
      "iteration 1640 loss 2.6620285511016846, acc 26.5625\n",
      "iteration 1641 loss 2.8148202896118164, acc 21.875\n",
      "iteration 1642 loss 2.3881192207336426, acc 32.8125\n",
      "iteration 1643 loss 2.4958620071411133, acc 31.25\n",
      "iteration 1644 loss 2.7677066326141357, acc 25.0\n",
      "iteration 1645 loss 2.893568515777588, acc 20.3125\n",
      "iteration 1646 loss 2.683577060699463, acc 20.3125\n",
      "iteration 1647 loss 2.8090314865112305, acc 17.1875\n",
      "iteration 1648 loss 2.879859685897827, acc 12.5\n",
      "iteration 1649 loss 2.744978427886963, acc 12.5\n",
      "iteration 1650 loss 2.7167131900787354, acc 14.0625\n",
      "iteration 1651 loss 2.739149332046509, acc 18.75\n",
      "iteration 1652 loss 2.6659834384918213, acc 23.4375\n",
      "iteration 1653 loss 2.7071306705474854, acc 28.125\n",
      "iteration 1654 loss 2.5407161712646484, acc 29.6875\n",
      "iteration 1655 loss 2.7547824382781982, acc 17.1875\n",
      "iteration 1656 loss 2.6620612144470215, acc 25.0\n",
      "iteration 1657 loss 2.842026710510254, acc 20.3125\n",
      "iteration 1658 loss 2.653541088104248, acc 26.5625\n",
      "iteration 1659 loss 2.442924737930298, acc 29.6875\n",
      "iteration 1660 loss 2.5693981647491455, acc 20.3125\n",
      "iteration 1661 loss 2.566023588180542, acc 25.0\n",
      "iteration 1662 loss 2.5968666076660156, acc 32.8125\n",
      "iteration 1663 loss 3.091830015182495, acc 18.75\n",
      "iteration 1664 loss 2.580172538757324, acc 29.6875\n",
      "iteration 1665 loss 2.782200813293457, acc 21.875\n",
      "iteration 1666 loss 2.8063321113586426, acc 15.625\n",
      "iteration 1667 loss 2.721284866333008, acc 29.6875\n",
      "iteration 1668 loss 2.6480588912963867, acc 20.3125\n",
      "iteration 1669 loss 2.471916913986206, acc 26.5625\n",
      "iteration 1670 loss 2.6914258003234863, acc 18.75\n",
      "iteration 1671 loss 2.659618854522705, acc 21.875\n",
      "iteration 1672 loss 2.6052701473236084, acc 26.5625\n",
      "iteration 1673 loss 2.74318528175354, acc 20.3125\n",
      "iteration 1674 loss 2.738661766052246, acc 17.1875\n",
      "iteration 1675 loss 2.7273337841033936, acc 12.5\n",
      "iteration 1676 loss 3.025580883026123, acc 17.1875\n",
      "iteration 1677 loss 2.914245843887329, acc 17.1875\n",
      "iteration 1678 loss 2.4868671894073486, acc 23.4375\n",
      "iteration 1679 loss 2.931645154953003, acc 18.75\n",
      "iteration 1680 loss 2.6123602390289307, acc 26.5625\n",
      "iteration 1681 loss 2.73976469039917, acc 18.75\n",
      "iteration 1682 loss 2.658494710922241, acc 25.0\n",
      "iteration 1683 loss 2.5062308311462402, acc 29.6875\n",
      "iteration 1684 loss 2.890538215637207, acc 15.625\n",
      "iteration 1685 loss 2.7512872219085693, acc 18.75\n",
      "iteration 1686 loss 2.809720993041992, acc 14.0625\n",
      "iteration 1687 loss 2.7192282676696777, acc 21.875\n",
      "iteration 1688 loss 2.611074924468994, acc 21.875\n",
      "iteration 1689 loss 2.7477245330810547, acc 17.1875\n",
      "iteration 1690 loss 2.7203712463378906, acc 17.1875\n",
      "iteration 1691 loss 2.860438108444214, acc 20.3125\n",
      "iteration 1692 loss 2.94181227684021, acc 9.375\n",
      "iteration 1693 loss 2.673959255218506, acc 15.625\n",
      "iteration 1694 loss 2.9103918075561523, acc 18.75\n",
      "iteration 1695 loss 2.6447927951812744, acc 25.0\n",
      "iteration 1696 loss 2.6553831100463867, acc 29.6875\n",
      "iteration 1697 loss 2.701026201248169, acc 23.4375\n",
      "iteration 1698 loss 2.8231468200683594, acc 17.1875\n",
      "iteration 1699 loss 2.6745858192443848, acc 20.3125\n",
      "iteration 1700 loss 2.710178852081299, acc 20.3125\n",
      "iteration 1701 loss 2.814734935760498, acc 18.75\n",
      "iteration 1702 loss 2.6120543479919434, acc 28.125\n",
      "iteration 1703 loss 2.809790849685669, acc 18.75\n",
      "iteration 1704 loss 2.7640035152435303, acc 20.3125\n",
      "iteration 1705 loss 2.5931174755096436, acc 26.5625\n",
      "iteration 1706 loss 2.4543869495391846, acc 25.0\n",
      "iteration 1707 loss 2.561321258544922, acc 25.0\n",
      "iteration 1708 loss 2.598278522491455, acc 23.4375\n",
      "iteration 1709 loss 2.669999361038208, acc 21.875\n",
      "iteration 1710 loss 2.718367338180542, acc 20.3125\n",
      "iteration 1711 loss 2.7882487773895264, acc 15.625\n",
      "iteration 1712 loss 2.6514174938201904, acc 14.0625\n",
      "iteration 1713 loss 2.8456521034240723, acc 15.625\n",
      "iteration 1714 loss 2.7527263164520264, acc 17.1875\n",
      "iteration 1715 loss 2.7511818408966064, acc 23.4375\n",
      "iteration 1716 loss 2.694514751434326, acc 18.75\n",
      "iteration 1717 loss 2.8716506958007812, acc 20.3125\n",
      "iteration 1718 loss 2.770439386367798, acc 17.1875\n",
      "iteration 1719 loss 2.594947099685669, acc 25.0\n",
      "iteration 1720 loss 2.8339433670043945, acc 14.0625\n",
      "iteration 1721 loss 2.6281158924102783, acc 10.9375\n",
      "iteration 1722 loss 2.73736310005188, acc 20.3125\n",
      "iteration 1723 loss 2.6231257915496826, acc 29.6875\n",
      "iteration 1724 loss 2.817087411880493, acc 18.75\n",
      "iteration 1725 loss 2.804999828338623, acc 23.4375\n",
      "iteration 1726 loss 2.6874876022338867, acc 17.1875\n",
      "iteration 1727 loss 2.5090198516845703, acc 32.8125\n",
      "iteration 1728 loss 2.4922850131988525, acc 25.0\n",
      "iteration 1729 loss 2.5374443531036377, acc 26.5625\n",
      "iteration 1730 loss 2.5634171962738037, acc 23.4375\n",
      "iteration 1731 loss 2.8546788692474365, acc 18.75\n",
      "iteration 1732 loss 2.7487032413482666, acc 15.625\n",
      "iteration 1733 loss 2.559063673019409, acc 25.0\n",
      "iteration 1734 loss 2.9085421562194824, acc 15.625\n",
      "iteration 1735 loss 2.7970798015594482, acc 21.875\n",
      "iteration 1736 loss 2.751655101776123, acc 21.875\n",
      "iteration 1737 loss 2.6770029067993164, acc 29.6875\n",
      "iteration 1738 loss 2.62542986869812, acc 20.3125\n",
      "iteration 1739 loss 2.5996079444885254, acc 23.4375\n",
      "iteration 1740 loss 2.5973474979400635, acc 25.0\n",
      "iteration 1741 loss 2.7242648601531982, acc 17.1875\n",
      "iteration 1742 loss 2.78813099861145, acc 18.75\n",
      "iteration 1743 loss 2.8091375827789307, acc 15.625\n",
      "iteration 1744 loss 2.6685993671417236, acc 17.1875\n",
      "iteration 1745 loss 2.6038520336151123, acc 20.3125\n",
      "iteration 1746 loss 2.7502083778381348, acc 18.75\n",
      "iteration 1747 loss 2.7963743209838867, acc 21.875\n",
      "iteration 1748 loss 2.713555097579956, acc 21.875\n",
      "iteration 1749 loss 2.8718245029449463, acc 12.5\n",
      "iteration 1750 loss 2.793015956878662, acc 18.75\n",
      "iteration 1751 loss 2.7588984966278076, acc 23.4375\n",
      "iteration 1752 loss 2.5558114051818848, acc 32.8125\n",
      "iteration 1753 loss 2.8577096462249756, acc 17.1875\n",
      "iteration 1754 loss 2.6813347339630127, acc 28.125\n",
      "iteration 1755 loss 2.4919052124023438, acc 31.25\n",
      "iteration 1756 loss 2.641385078430176, acc 26.5625\n",
      "iteration 1757 loss 2.6858744621276855, acc 21.875\n",
      "iteration 1758 loss 2.846513032913208, acc 18.75\n",
      "iteration 1759 loss 2.5822556018829346, acc 28.125\n",
      "iteration 1760 loss 2.4635138511657715, acc 31.25\n",
      "iteration 1761 loss 2.4989078044891357, acc 29.6875\n",
      "iteration 1762 loss 2.656402826309204, acc 21.875\n",
      "iteration 1763 loss 2.919637680053711, acc 18.75\n",
      "iteration 1764 loss 2.741318464279175, acc 17.1875\n",
      "iteration 1765 loss 2.8579561710357666, acc 20.3125\n",
      "iteration 1766 loss 2.83251953125, acc 18.75\n",
      "iteration 1767 loss 2.67824125289917, acc 23.4375\n",
      "iteration 1768 loss 2.6956117153167725, acc 23.4375\n",
      "iteration 1769 loss 2.778900623321533, acc 12.5\n",
      "iteration 1770 loss 2.7431554794311523, acc 21.875\n",
      "iteration 1771 loss 2.6757864952087402, acc 18.75\n",
      "iteration 1772 loss 2.6535255908966064, acc 17.1875\n",
      "iteration 1773 loss 2.772141218185425, acc 12.5\n",
      "iteration 1774 loss 2.528717279434204, acc 20.3125\n",
      "iteration 1775 loss 2.678314685821533, acc 23.4375\n",
      "iteration 1776 loss 2.5587964057922363, acc 29.6875\n",
      "iteration 1777 loss 2.7233245372772217, acc 25.0\n",
      "iteration 1778 loss 2.681067705154419, acc 20.3125\n",
      "iteration 1779 loss 2.656986951828003, acc 21.875\n",
      "iteration 1780 loss 2.5999398231506348, acc 28.125\n",
      "iteration 1781 loss 2.83125901222229, acc 20.3125\n",
      "iteration 1782 loss 2.6860077381134033, acc 20.3125\n",
      "iteration 1783 loss 2.8367226123809814, acc 17.1875\n",
      "iteration 1784 loss 2.8375158309936523, acc 20.3125\n",
      "iteration 1785 loss 2.8543457984924316, acc 21.875\n",
      "iteration 1786 loss 2.848385810852051, acc 15.625\n",
      "iteration 1787 loss 2.6997201442718506, acc 20.3125\n",
      "iteration 1788 loss 2.878931760787964, acc 15.625\n",
      "iteration 1789 loss 2.7022788524627686, acc 17.1875\n",
      "iteration 1790 loss 2.808300733566284, acc 26.5625\n",
      "iteration 1791 loss 2.7477011680603027, acc 14.0625\n",
      "iteration 1792 loss 2.476360321044922, acc 29.6875\n",
      "iteration 1793 loss 2.6591711044311523, acc 20.3125\n",
      "iteration 1794 loss 2.8557348251342773, acc 17.1875\n",
      "iteration 1795 loss 2.8390371799468994, acc 20.3125\n",
      "iteration 1796 loss 2.748042583465576, acc 20.3125\n",
      "iteration 1797 loss 2.8656115531921387, acc 17.1875\n",
      "iteration 1798 loss 2.8235368728637695, acc 20.3125\n",
      "iteration 1799 loss 2.9733405113220215, acc 15.625\n",
      "iteration 1800 loss 2.820996046066284, acc 17.1875\n",
      "iteration 1801 loss 2.7596688270568848, acc 21.875\n",
      "iteration 1802 loss 2.7989025115966797, acc 15.625\n",
      "iteration 1803 loss 2.7722744941711426, acc 21.875\n",
      "iteration 1804 loss 2.7041070461273193, acc 20.3125\n",
      "iteration 1805 loss 2.902346134185791, acc 12.5\n",
      "iteration 1806 loss 2.648078441619873, acc 25.0\n",
      "iteration 1807 loss 2.7935051918029785, acc 25.0\n",
      "iteration 1808 loss 2.8597912788391113, acc 14.0625\n",
      "iteration 1809 loss 2.4589719772338867, acc 21.875\n",
      "iteration 1810 loss 2.725869655609131, acc 25.0\n",
      "iteration 1811 loss 2.6263692378997803, acc 18.75\n",
      "iteration 1812 loss 2.842184543609619, acc 20.3125\n",
      "iteration 1813 loss 2.6873607635498047, acc 17.1875\n",
      "iteration 1814 loss 2.744084358215332, acc 21.875\n",
      "iteration 1815 loss 2.7583723068237305, acc 21.875\n",
      "iteration 1816 loss 2.449302911758423, acc 26.5625\n",
      "iteration 1817 loss 2.7291059494018555, acc 20.3125\n",
      "iteration 1818 loss 2.6561882495880127, acc 20.3125\n",
      "iteration 1819 loss 2.789985418319702, acc 20.3125\n",
      "iteration 1820 loss 2.7011141777038574, acc 17.1875\n",
      "iteration 1821 loss 2.6551029682159424, acc 26.5625\n",
      "iteration 1822 loss 2.745835542678833, acc 20.3125\n",
      "iteration 1823 loss 2.661435604095459, acc 23.4375\n",
      "iteration 1824 loss 2.7268669605255127, acc 21.875\n",
      "iteration 1825 loss 2.559751272201538, acc 28.125\n",
      "iteration 1826 loss 2.4041237831115723, acc 23.4375\n",
      "iteration 1827 loss 2.579259157180786, acc 21.875\n",
      "iteration 1828 loss 2.777484893798828, acc 20.3125\n",
      "iteration 1829 loss 2.612286329269409, acc 32.8125\n",
      "iteration 1830 loss 3.061152458190918, acc 12.5\n",
      "iteration 1831 loss 2.6609058380126953, acc 23.4375\n",
      "iteration 1832 loss 2.8307788372039795, acc 25.0\n",
      "iteration 1833 loss 2.7202417850494385, acc 21.875\n",
      "iteration 1834 loss 2.8115901947021484, acc 15.625\n",
      "iteration 1835 loss 2.6475846767425537, acc 29.6875\n",
      "iteration 1836 loss 2.514953374862671, acc 26.5625\n",
      "iteration 1837 loss 2.7638888359069824, acc 14.0625\n",
      "iteration 1838 loss 2.4989800453186035, acc 18.75\n",
      "iteration 1839 loss 2.8419270515441895, acc 12.5\n",
      "iteration 1840 loss 2.61989164352417, acc 21.875\n",
      "iteration 1841 loss 2.736511707305908, acc 18.75\n",
      "iteration 1842 loss 2.5892493724823, acc 25.0\n",
      "iteration 1843 loss 2.7019598484039307, acc 20.3125\n",
      "iteration 1844 loss 2.643888235092163, acc 18.75\n",
      "iteration 1845 loss 2.806300640106201, acc 17.1875\n",
      "iteration 1846 loss 2.8313286304473877, acc 14.0625\n",
      "iteration 1847 loss 2.7318387031555176, acc 12.5\n",
      "iteration 1848 loss 2.751044988632202, acc 10.9375\n",
      "iteration 1849 loss 2.8279192447662354, acc 18.75\n",
      "iteration 1850 loss 2.486328363418579, acc 25.0\n",
      "iteration 1851 loss 2.832385301589966, acc 18.75\n",
      "iteration 1852 loss 2.7119367122650146, acc 12.5\n",
      "iteration 1853 loss 2.8557050228118896, acc 18.75\n",
      "iteration 1854 loss 2.6705610752105713, acc 28.125\n",
      "iteration 1855 loss 2.5756795406341553, acc 21.875\n",
      "iteration 1856 loss 2.642573595046997, acc 20.3125\n",
      "iteration 1857 loss 2.68973445892334, acc 25.0\n",
      "iteration 1858 loss 2.743804931640625, acc 15.625\n",
      "iteration 1859 loss 2.6159820556640625, acc 18.75\n",
      "iteration 1860 loss 2.898261547088623, acc 18.75\n",
      "iteration 1861 loss 2.8010261058807373, acc 12.5\n",
      "iteration 1862 loss 2.5938727855682373, acc 18.75\n",
      "iteration 1863 loss 2.59856915473938, acc 9.375\n",
      "iteration 1864 loss 2.4234421253204346, acc 20.3125\n",
      "iteration 1865 loss 2.5920112133026123, acc 29.6875\n",
      "iteration 1866 loss 2.7341814041137695, acc 14.0625\n",
      "iteration 1867 loss 2.64573073387146, acc 32.8125\n",
      "iteration 1868 loss 2.781775712966919, acc 20.3125\n",
      "iteration 1869 loss 2.9990761280059814, acc 12.5\n",
      "iteration 1870 loss 2.706531047821045, acc 28.125\n",
      "iteration 1871 loss 2.7175991535186768, acc 23.4375\n",
      "iteration 1872 loss 2.776223659515381, acc 21.875\n",
      "iteration 1873 loss 2.9437172412872314, acc 14.0625\n",
      "iteration 1874 loss 2.721292495727539, acc 25.0\n",
      "iteration 1875 loss 2.640730142593384, acc 28.125\n",
      "iteration 1876 loss 2.8074591159820557, acc 15.625\n",
      "iteration 1877 loss 2.7136757373809814, acc 25.0\n",
      "iteration 1878 loss 2.7146811485290527, acc 23.4375\n",
      "iteration 1879 loss 2.645718574523926, acc 28.125\n",
      "iteration 1880 loss 2.7276759147644043, acc 17.1875\n",
      "iteration 1881 loss 2.6819918155670166, acc 17.1875\n",
      "iteration 1882 loss 2.5377120971679688, acc 31.25\n",
      "iteration 1883 loss 2.7311365604400635, acc 20.3125\n",
      "iteration 1884 loss 2.754091501235962, acc 20.3125\n",
      "iteration 1885 loss 2.7223260402679443, acc 18.75\n",
      "iteration 1886 loss 2.6877026557922363, acc 20.3125\n",
      "iteration 1887 loss 2.7290353775024414, acc 17.1875\n",
      "iteration 1888 loss 2.6270270347595215, acc 25.0\n",
      "iteration 1889 loss 2.668548107147217, acc 28.125\n",
      "iteration 1890 loss 2.867955207824707, acc 18.75\n",
      "iteration 1891 loss 2.653519868850708, acc 20.3125\n",
      "iteration 1892 loss 2.835456371307373, acc 21.875\n",
      "iteration 1893 loss 2.653700113296509, acc 28.125\n",
      "iteration 1894 loss 2.9599571228027344, acc 15.625\n",
      "iteration 1895 loss 2.610821485519409, acc 17.1875\n",
      "iteration 1896 loss 2.73114013671875, acc 17.1875\n",
      "iteration 1897 loss 2.6872968673706055, acc 15.625\n",
      "iteration 1898 loss 2.6399621963500977, acc 25.0\n",
      "iteration 1899 loss 2.9985480308532715, acc 14.0625\n",
      "iteration 1900 loss 2.570528268814087, acc 20.3125\n",
      "iteration 1901 loss 2.6095526218414307, acc 26.5625\n",
      "iteration 1902 loss 2.676988363265991, acc 25.0\n",
      "iteration 1903 loss 2.763679027557373, acc 17.1875\n",
      "iteration 1904 loss 2.5856690406799316, acc 26.5625\n",
      "iteration 1905 loss 2.6032721996307373, acc 21.875\n",
      "iteration 1906 loss 2.61706280708313, acc 28.125\n",
      "iteration 1907 loss 2.788341760635376, acc 20.3125\n",
      "iteration 1908 loss 2.7155988216400146, acc 14.0625\n",
      "iteration 1909 loss 2.796635389328003, acc 15.625\n",
      "iteration 1910 loss 2.8097212314605713, acc 21.875\n",
      "iteration 1911 loss 2.6925055980682373, acc 23.4375\n",
      "iteration 1912 loss 2.6656253337860107, acc 26.5625\n",
      "iteration 1913 loss 2.544605255126953, acc 28.125\n",
      "iteration 1914 loss 2.7786638736724854, acc 18.75\n",
      "iteration 1915 loss 2.612755060195923, acc 18.75\n",
      "iteration 1916 loss 2.69092059135437, acc 17.1875\n",
      "iteration 1917 loss 2.7420291900634766, acc 10.9375\n",
      "iteration 1918 loss 2.793583393096924, acc 21.875\n",
      "iteration 1919 loss 2.5623395442962646, acc 26.5625\n",
      "iteration 1920 loss 2.5545170307159424, acc 23.4375\n",
      "iteration 1921 loss 2.67197322845459, acc 20.3125\n",
      "iteration 1922 loss 2.7630183696746826, acc 20.3125\n",
      "iteration 1923 loss 2.6576006412506104, acc 17.1875\n",
      "iteration 1924 loss 2.600187063217163, acc 18.75\n",
      "iteration 1925 loss 2.9125986099243164, acc 9.375\n",
      "iteration 1926 loss 2.714586019515991, acc 21.875\n",
      "iteration 1927 loss 2.5706653594970703, acc 21.875\n",
      "iteration 1928 loss 2.695460557937622, acc 17.1875\n",
      "iteration 1929 loss 2.5439789295196533, acc 18.75\n",
      "iteration 1930 loss 2.7401058673858643, acc 23.4375\n",
      "iteration 1931 loss 2.6685171127319336, acc 25.0\n",
      "iteration 1932 loss 2.8326001167297363, acc 14.0625\n",
      "iteration 1933 loss 2.700796127319336, acc 28.125\n",
      "iteration 1934 loss 2.665266990661621, acc 28.125\n",
      "iteration 1935 loss 2.7516725063323975, acc 21.875\n",
      "iteration 1936 loss 2.7746384143829346, acc 17.1875\n",
      "iteration 1937 loss 2.6618263721466064, acc 29.6875\n",
      "iteration 1938 loss 2.521493673324585, acc 23.4375\n",
      "iteration 1939 loss 2.862572193145752, acc 20.3125\n",
      "iteration 1940 loss 2.620270252227783, acc 23.4375\n",
      "iteration 1941 loss 2.711642265319824, acc 25.0\n",
      "iteration 1942 loss 2.742952823638916, acc 26.5625\n",
      "iteration 1943 loss 2.8994393348693848, acc 14.0625\n",
      "iteration 1944 loss 2.6516494750976562, acc 17.1875\n",
      "iteration 1945 loss 2.5702579021453857, acc 26.5625\n",
      "iteration 1946 loss 2.513089418411255, acc 20.3125\n",
      "iteration 1947 loss 2.6979734897613525, acc 14.0625\n",
      "iteration 1948 loss 2.5644137859344482, acc 34.375\n",
      "iteration 1949 loss 2.645582914352417, acc 23.4375\n",
      "iteration 1950 loss 2.6936755180358887, acc 17.1875\n",
      "iteration 1951 loss 2.6202738285064697, acc 21.875\n",
      "iteration 1952 loss 2.719792127609253, acc 20.3125\n",
      "iteration 1953 loss 2.633943796157837, acc 28.125\n",
      "iteration 1954 loss 2.55519700050354, acc 21.875\n",
      "iteration 1955 loss 2.6873316764831543, acc 23.4375\n",
      "iteration 1956 loss 2.8312571048736572, acc 20.3125\n",
      "iteration 1957 loss 2.8078675270080566, acc 12.5\n",
      "iteration 1958 loss 2.669943332672119, acc 25.0\n",
      "iteration 1959 loss 2.886284351348877, acc 17.1875\n",
      "iteration 1960 loss 2.7991580963134766, acc 17.1875\n",
      "iteration 1961 loss 2.821589946746826, acc 18.75\n",
      "iteration 1962 loss 2.5855817794799805, acc 26.5625\n",
      "iteration 1963 loss 2.8007569313049316, acc 17.1875\n",
      "iteration 1964 loss 2.609957218170166, acc 25.0\n",
      "iteration 1965 loss 2.6770434379577637, acc 21.875\n",
      "iteration 1966 loss 2.826519250869751, acc 20.3125\n",
      "iteration 1967 loss 2.550839900970459, acc 25.0\n",
      "iteration 1968 loss 2.739558458328247, acc 25.0\n",
      "iteration 1969 loss 2.750944137573242, acc 20.3125\n",
      "iteration 1970 loss 2.894260883331299, acc 15.625\n",
      "iteration 1971 loss 2.7305054664611816, acc 18.75\n",
      "iteration 1972 loss 2.744934558868408, acc 23.4375\n",
      "iteration 1973 loss 2.705031633377075, acc 21.875\n",
      "iteration 1974 loss 2.8106558322906494, acc 18.75\n",
      "iteration 1975 loss 2.6012611389160156, acc 21.875\n",
      "iteration 1976 loss 2.6843488216400146, acc 23.4375\n",
      "iteration 1977 loss 2.753757953643799, acc 25.0\n",
      "iteration 1978 loss 2.529479503631592, acc 25.0\n",
      "iteration 1979 loss 2.4405629634857178, acc 28.125\n",
      "iteration 1980 loss 3.0474233627319336, acc 14.0625\n",
      "iteration 1981 loss 2.656374216079712, acc 23.4375\n",
      "iteration 1982 loss 2.673635482788086, acc 23.4375\n",
      "iteration 1983 loss 2.6535744667053223, acc 26.5625\n",
      "iteration 1984 loss 2.7762346267700195, acc 17.1875\n",
      "iteration 1985 loss 2.635146141052246, acc 29.6875\n",
      "iteration 1986 loss 2.7274932861328125, acc 17.1875\n",
      "iteration 1987 loss 2.6775078773498535, acc 31.25\n",
      "iteration 1988 loss 2.3675107955932617, acc 28.125\n",
      "iteration 1989 loss 2.7534408569335938, acc 21.875\n",
      "iteration 1990 loss 2.911424398422241, acc 15.625\n",
      "iteration 1991 loss 2.6983578205108643, acc 17.1875\n",
      "iteration 1992 loss 2.508249521255493, acc 31.25\n",
      "iteration 1993 loss 2.8946425914764404, acc 15.625\n",
      "iteration 1994 loss 2.6985573768615723, acc 20.3125\n",
      "iteration 1995 loss 2.689707040786743, acc 18.75\n",
      "iteration 1996 loss 2.841355323791504, acc 10.9375\n",
      "iteration 1997 loss 2.557661294937134, acc 26.5625\n",
      "iteration 1998 loss 2.6436614990234375, acc 18.75\n",
      "iteration 1999 loss 2.7554454803466797, acc 15.625\n",
      "iteration 2000 loss 2.7691307067871094, acc 18.75\n",
      "iteration 2001 loss 2.7853286266326904, acc 20.3125\n",
      "iteration 2002 loss 2.586048126220703, acc 28.125\n",
      "iteration 2003 loss 2.687026262283325, acc 23.4375\n",
      "iteration 2004 loss 2.565652370452881, acc 29.6875\n",
      "iteration 2005 loss 2.695011615753174, acc 23.4375\n",
      "iteration 2006 loss 2.932173252105713, acc 17.1875\n",
      "iteration 2007 loss 2.644881248474121, acc 18.75\n",
      "iteration 2008 loss 2.644024610519409, acc 21.875\n",
      "iteration 2009 loss 2.7386653423309326, acc 17.1875\n",
      "iteration 2010 loss 2.810961961746216, acc 23.4375\n",
      "iteration 2011 loss 2.7343955039978027, acc 21.875\n",
      "iteration 2012 loss 2.6868672370910645, acc 17.1875\n",
      "iteration 2013 loss 2.7071726322174072, acc 21.875\n",
      "iteration 2014 loss 2.782147169113159, acc 18.75\n",
      "iteration 2015 loss 2.672832727432251, acc 18.75\n",
      "iteration 2016 loss 2.567077875137329, acc 21.875\n",
      "iteration 2017 loss 2.5986897945404053, acc 23.4375\n",
      "iteration 2018 loss 2.4851021766662598, acc 28.125\n",
      "iteration 2019 loss 2.6619749069213867, acc 26.5625\n",
      "iteration 2020 loss 2.6482114791870117, acc 20.3125\n",
      "iteration 2021 loss 2.668165445327759, acc 25.0\n",
      "iteration 2022 loss 2.5469932556152344, acc 23.4375\n",
      "iteration 2023 loss 2.528871774673462, acc 32.8125\n",
      "iteration 2024 loss 2.7415573596954346, acc 18.75\n",
      "iteration 2025 loss 2.653623580932617, acc 18.75\n",
      "iteration 2026 loss 2.7311127185821533, acc 26.5625\n",
      "iteration 2027 loss 2.751378297805786, acc 15.625\n",
      "iteration 2028 loss 2.5152478218078613, acc 21.875\n",
      "iteration 2029 loss 2.7175240516662598, acc 17.1875\n",
      "iteration 2030 loss 2.6035401821136475, acc 21.875\n",
      "iteration 2031 loss 2.6825101375579834, acc 23.4375\n",
      "iteration 2032 loss 2.678406238555908, acc 26.5625\n",
      "iteration 2033 loss 2.809513807296753, acc 18.75\n",
      "iteration 2034 loss 2.661350965499878, acc 21.875\n",
      "iteration 2035 loss 2.8807461261749268, acc 21.875\n",
      "iteration 2036 loss 2.5970206260681152, acc 25.0\n",
      "iteration 2037 loss 2.706721067428589, acc 28.125\n",
      "iteration 2038 loss 3.008140802383423, acc 20.3125\n",
      "iteration 2039 loss 2.5141634941101074, acc 25.0\n",
      "iteration 2040 loss 2.5852153301239014, acc 28.125\n",
      "iteration 2041 loss 2.802328586578369, acc 20.3125\n",
      "iteration 2042 loss 2.6588191986083984, acc 28.125\n",
      "iteration 2043 loss 2.7463443279266357, acc 14.0625\n",
      "iteration 2044 loss 2.5122671127319336, acc 25.0\n",
      "iteration 2045 loss 2.574260711669922, acc 25.0\n",
      "iteration 2046 loss 2.836573839187622, acc 15.625\n",
      "iteration 2047 loss 2.664188861846924, acc 17.1875\n",
      "iteration 2048 loss 2.8229751586914062, acc 25.0\n",
      "iteration 2049 loss 2.716799736022949, acc 25.0\n",
      "iteration 2050 loss 2.7879185676574707, acc 18.75\n",
      "iteration 2051 loss 2.6514692306518555, acc 18.75\n",
      "iteration 2052 loss 2.7696406841278076, acc 23.4375\n",
      "iteration 2053 loss 2.7957866191864014, acc 20.3125\n",
      "iteration 2054 loss 2.6440865993499756, acc 20.3125\n",
      "iteration 2055 loss 2.567856788635254, acc 35.9375\n",
      "iteration 2056 loss 2.7261691093444824, acc 20.3125\n",
      "iteration 2057 loss 2.7907662391662598, acc 17.1875\n",
      "iteration 2058 loss 2.531585931777954, acc 31.25\n",
      "iteration 2059 loss 2.8385796546936035, acc 20.3125\n",
      "iteration 2060 loss 2.8741416931152344, acc 15.625\n",
      "iteration 2061 loss 3.0402092933654785, acc 10.9375\n",
      "iteration 2062 loss 2.742558002471924, acc 25.0\n",
      "iteration 2063 loss 2.995929479598999, acc 15.625\n",
      "iteration 2064 loss 2.6615445613861084, acc 23.4375\n",
      "iteration 2065 loss 2.7647805213928223, acc 12.5\n",
      "iteration 2066 loss 2.8093302249908447, acc 15.625\n",
      "iteration 2067 loss 2.670273780822754, acc 20.3125\n",
      "iteration 2068 loss 2.9417624473571777, acc 20.3125\n",
      "iteration 2069 loss 2.880960702896118, acc 17.1875\n",
      "iteration 2070 loss 2.778719902038574, acc 15.625\n",
      "iteration 2071 loss 2.7189645767211914, acc 32.8125\n",
      "iteration 2072 loss 2.5696046352386475, acc 21.875\n",
      "iteration 2073 loss 2.6373140811920166, acc 21.875\n",
      "iteration 2074 loss 2.724142551422119, acc 20.3125\n",
      "iteration 2075 loss 2.8812685012817383, acc 18.75\n",
      "iteration 2076 loss 2.712376832962036, acc 23.4375\n",
      "iteration 2077 loss 2.657141923904419, acc 23.4375\n",
      "iteration 2078 loss 2.7099995613098145, acc 23.4375\n",
      "iteration 2079 loss 2.563328981399536, acc 26.5625\n",
      "iteration 2080 loss 2.8130273818969727, acc 21.875\n",
      "iteration 2081 loss 2.777906894683838, acc 25.0\n",
      "iteration 2082 loss 2.733020305633545, acc 14.0625\n",
      "iteration 2083 loss 2.67167329788208, acc 17.1875\n",
      "iteration 2084 loss 2.6685707569122314, acc 29.6875\n",
      "iteration 2085 loss 2.7329823970794678, acc 15.625\n",
      "iteration 2086 loss 2.644186019897461, acc 25.0\n",
      "iteration 2087 loss 2.721867084503174, acc 26.5625\n",
      "iteration 2088 loss 2.50146222114563, acc 31.25\n",
      "iteration 2089 loss 2.690812587738037, acc 26.5625\n",
      "iteration 2090 loss 2.4857265949249268, acc 29.6875\n",
      "iteration 2091 loss 2.816606044769287, acc 18.75\n",
      "iteration 2092 loss 2.8170509338378906, acc 18.75\n",
      "iteration 2093 loss 2.484771728515625, acc 23.4375\n",
      "iteration 2094 loss 3.023181915283203, acc 15.625\n",
      "iteration 2095 loss 2.718740701675415, acc 21.875\n",
      "iteration 2096 loss 2.7973382472991943, acc 15.625\n",
      "iteration 2097 loss 2.809551239013672, acc 15.625\n",
      "iteration 2098 loss 2.9183952808380127, acc 25.0\n",
      "iteration 2099 loss 2.668332099914551, acc 20.3125\n",
      "iteration 2100 loss 2.717970848083496, acc 23.4375\n",
      "iteration 2101 loss 2.853494167327881, acc 25.0\n",
      "iteration 2102 loss 2.653942823410034, acc 21.875\n",
      "iteration 2103 loss 2.5571370124816895, acc 23.4375\n",
      "iteration 2104 loss 2.6511168479919434, acc 18.75\n",
      "iteration 2105 loss 2.955239772796631, acc 14.0625\n",
      "iteration 2106 loss 2.7626023292541504, acc 18.75\n",
      "iteration 2107 loss 2.699979782104492, acc 23.4375\n",
      "iteration 2108 loss 2.647613763809204, acc 23.4375\n",
      "iteration 2109 loss 2.5983521938323975, acc 29.6875\n",
      "iteration 2110 loss 2.626368999481201, acc 31.25\n",
      "iteration 2111 loss 2.7114808559417725, acc 23.4375\n",
      "iteration 2112 loss 2.474634885787964, acc 18.75\n",
      "iteration 2113 loss 2.690955638885498, acc 17.1875\n",
      "iteration 2114 loss 2.5253329277038574, acc 31.25\n",
      "iteration 2115 loss 2.6531691551208496, acc 17.1875\n",
      "iteration 2116 loss 2.8262293338775635, acc 15.625\n",
      "iteration 2117 loss 2.693436622619629, acc 25.0\n",
      "iteration 2118 loss 2.7136595249176025, acc 28.125\n",
      "iteration 2119 loss 2.9021894931793213, acc 17.1875\n",
      "iteration 2120 loss 2.647939443588257, acc 23.4375\n",
      "iteration 2121 loss 2.596750497817993, acc 26.5625\n",
      "iteration 2122 loss 2.692399740219116, acc 20.3125\n",
      "iteration 2123 loss 2.6666839122772217, acc 18.75\n",
      "iteration 2124 loss 2.6170310974121094, acc 20.3125\n",
      "iteration 2125 loss 2.819289207458496, acc 17.1875\n",
      "iteration 2126 loss 2.653282642364502, acc 23.4375\n",
      "iteration 2127 loss 3.033599853515625, acc 15.625\n",
      "iteration 2128 loss 2.754683256149292, acc 20.3125\n",
      "iteration 2129 loss 2.6097252368927, acc 29.6875\n",
      "iteration 2130 loss 2.811833143234253, acc 18.75\n",
      "iteration 2131 loss 2.6433191299438477, acc 21.875\n",
      "iteration 2132 loss 2.519892692565918, acc 26.5625\n",
      "iteration 2133 loss 2.838648557662964, acc 23.4375\n",
      "iteration 2134 loss 2.656935930252075, acc 18.75\n",
      "iteration 2135 loss 2.572421073913574, acc 28.125\n",
      "iteration 2136 loss 2.640068531036377, acc 20.3125\n",
      "iteration 2137 loss 2.6923747062683105, acc 21.875\n",
      "iteration 2138 loss 2.861112356185913, acc 9.375\n",
      "iteration 2139 loss 2.7880349159240723, acc 9.375\n",
      "iteration 2140 loss 2.736382484436035, acc 25.0\n",
      "iteration 2141 loss 2.5329630374908447, acc 31.25\n",
      "iteration 2142 loss 2.766852617263794, acc 21.875\n",
      "iteration 2143 loss 2.5283024311065674, acc 31.25\n",
      "iteration 2144 loss 2.521744728088379, acc 18.75\n",
      "iteration 2145 loss 2.887108325958252, acc 20.3125\n",
      "iteration 2146 loss 2.567802667617798, acc 23.4375\n",
      "iteration 2147 loss 2.8676719665527344, acc 17.1875\n",
      "iteration 2148 loss 2.599304437637329, acc 21.875\n",
      "iteration 2149 loss 2.731776237487793, acc 26.5625\n",
      "iteration 2150 loss 2.43703293800354, acc 31.25\n",
      "iteration 2151 loss 2.655522346496582, acc 20.3125\n",
      "iteration 2152 loss 2.6159305572509766, acc 18.75\n",
      "iteration 2153 loss 2.879103660583496, acc 15.625\n",
      "iteration 2154 loss 2.8520278930664062, acc 12.5\n",
      "iteration 2155 loss 2.655445098876953, acc 18.75\n",
      "iteration 2156 loss 2.632357120513916, acc 20.3125\n",
      "iteration 2157 loss 2.7010934352874756, acc 21.875\n",
      "iteration 2158 loss 2.623868465423584, acc 20.3125\n",
      "iteration 2159 loss 2.7020487785339355, acc 21.875\n",
      "iteration 2160 loss 2.848087787628174, acc 12.5\n",
      "iteration 2161 loss 2.933732032775879, acc 17.1875\n",
      "iteration 2162 loss 2.767667293548584, acc 21.875\n",
      "iteration 2163 loss 2.5982749462127686, acc 25.0\n",
      "iteration 2164 loss 2.7345075607299805, acc 18.75\n",
      "iteration 2165 loss 3.0517404079437256, acc 9.375\n",
      "iteration 2166 loss 2.7282817363739014, acc 10.9375\n",
      "iteration 2167 loss 2.664431571960449, acc 23.4375\n",
      "iteration 2168 loss 2.6641485691070557, acc 28.125\n",
      "iteration 2169 loss 2.493786334991455, acc 25.0\n",
      "iteration 2170 loss 2.7391116619110107, acc 21.875\n",
      "iteration 2171 loss 2.8733904361724854, acc 17.1875\n",
      "iteration 2172 loss 2.4715383052825928, acc 34.375\n",
      "iteration 2173 loss 2.6198251247406006, acc 23.4375\n",
      "iteration 2174 loss 2.6883630752563477, acc 20.3125\n",
      "iteration 2175 loss 2.671168804168701, acc 21.875\n",
      "iteration 2176 loss 2.5952694416046143, acc 23.4375\n",
      "iteration 2177 loss 2.851282835006714, acc 21.875\n",
      "iteration 2178 loss 2.885695457458496, acc 14.0625\n",
      "iteration 2179 loss 2.9084393978118896, acc 20.3125\n",
      "iteration 2180 loss 2.717972755432129, acc 20.3125\n",
      "iteration 2181 loss 2.672224760055542, acc 29.6875\n",
      "iteration 2182 loss 2.5372602939605713, acc 32.8125\n",
      "iteration 2183 loss 2.8311100006103516, acc 14.0625\n",
      "iteration 2184 loss 2.657592535018921, acc 20.3125\n",
      "iteration 2185 loss 2.7702064514160156, acc 7.8125\n",
      "iteration 2186 loss 2.6329472064971924, acc 15.625\n",
      "iteration 2187 loss 2.7872915267944336, acc 3.125\n",
      "iteration 2188 loss 2.6207432746887207, acc 28.125\n",
      "iteration 2189 loss 2.7200517654418945, acc 23.4375\n",
      "iteration 2190 loss 2.7623789310455322, acc 18.75\n",
      "iteration 2191 loss 2.7007131576538086, acc 21.875\n",
      "iteration 2192 loss 2.8939552307128906, acc 18.75\n",
      "iteration 2193 loss 2.6326639652252197, acc 15.625\n",
      "iteration 2194 loss 2.63916277885437, acc 18.75\n",
      "iteration 2195 loss 2.660842180252075, acc 23.4375\n",
      "iteration 2196 loss 2.745945692062378, acc 18.75\n",
      "iteration 2197 loss 2.474111795425415, acc 34.375\n",
      "iteration 2198 loss 2.8147778511047363, acc 21.875\n",
      "iteration 2199 loss 2.701382637023926, acc 23.4375\n",
      "iteration 2200 loss 2.8544211387634277, acc 10.9375\n",
      "iteration 2201 loss 2.665545701980591, acc 20.3125\n",
      "iteration 2202 loss 2.593677043914795, acc 26.5625\n",
      "iteration 2203 loss 2.7554121017456055, acc 17.1875\n",
      "iteration 2204 loss 2.6272852420806885, acc 21.875\n",
      "iteration 2205 loss 2.83854603767395, acc 20.3125\n",
      "iteration 2206 loss 2.610156774520874, acc 26.5625\n",
      "iteration 2207 loss 2.6057283878326416, acc 28.125\n",
      "iteration 2208 loss 2.8669567108154297, acc 15.625\n",
      "iteration 2209 loss 2.5345802307128906, acc 23.4375\n",
      "iteration 2210 loss 2.8171591758728027, acc 10.9375\n",
      "iteration 2211 loss 2.765734910964966, acc 17.1875\n",
      "iteration 2212 loss 2.653512477874756, acc 23.4375\n",
      "iteration 2213 loss 2.7355544567108154, acc 23.4375\n",
      "iteration 2214 loss 2.8879201412200928, acc 23.4375\n",
      "iteration 2215 loss 2.538569927215576, acc 28.125\n",
      "iteration 2216 loss 2.6267480850219727, acc 18.75\n",
      "iteration 2217 loss 2.6029133796691895, acc 29.6875\n",
      "iteration 2218 loss 2.5720300674438477, acc 28.125\n",
      "iteration 2219 loss 2.930840015411377, acc 15.625\n",
      "iteration 2220 loss 3.0589399337768555, acc 10.9375\n",
      "iteration 2221 loss 2.8110969066619873, acc 18.75\n",
      "iteration 2222 loss 2.6690144538879395, acc 25.0\n",
      "iteration 2223 loss 2.524918556213379, acc 31.25\n",
      "iteration 2224 loss 2.53761625289917, acc 25.0\n",
      "iteration 2225 loss 2.6278371810913086, acc 28.125\n",
      "iteration 2226 loss 2.404331684112549, acc 28.125\n",
      "iteration 2227 loss 2.68975567817688, acc 20.3125\n",
      "iteration 2228 loss 2.86810302734375, acc 20.3125\n",
      "iteration 2229 loss 2.7539901733398438, acc 15.625\n",
      "iteration 2230 loss 2.616243362426758, acc 18.75\n",
      "iteration 2231 loss 2.5554773807525635, acc 18.75\n",
      "iteration 2232 loss 2.9249396324157715, acc 15.625\n",
      "iteration 2233 loss 2.825338840484619, acc 10.9375\n",
      "iteration 2234 loss 2.5915493965148926, acc 25.0\n",
      "iteration 2235 loss 2.7736003398895264, acc 18.75\n",
      "iteration 2236 loss 2.7352213859558105, acc 17.1875\n",
      "iteration 2237 loss 2.7557711601257324, acc 20.3125\n",
      "iteration 2238 loss 2.601914167404175, acc 25.0\n",
      "iteration 2239 loss 2.782465696334839, acc 20.3125\n",
      "iteration 2240 loss 2.964848518371582, acc 15.625\n",
      "iteration 2241 loss 2.8667099475860596, acc 21.875\n",
      "iteration 2242 loss 2.763664484024048, acc 17.1875\n",
      "iteration 2243 loss 2.9618873596191406, acc 10.9375\n",
      "iteration 2244 loss 2.5995447635650635, acc 18.75\n",
      "iteration 2245 loss 2.8104805946350098, acc 12.5\n",
      "iteration 2246 loss 2.5854220390319824, acc 21.875\n",
      "iteration 2247 loss 2.779865026473999, acc 14.0625\n",
      "iteration 2248 loss 2.672398567199707, acc 21.875\n",
      "iteration 2249 loss 2.7167673110961914, acc 14.0625\n",
      "iteration 2250 loss 2.734053611755371, acc 18.75\n",
      "iteration 2251 loss 2.664290428161621, acc 21.875\n",
      "iteration 2252 loss 2.469774007797241, acc 29.6875\n",
      "iteration 2253 loss 2.892279863357544, acc 21.875\n",
      "iteration 2254 loss 2.43405818939209, acc 31.25\n",
      "iteration 2255 loss 2.6479485034942627, acc 25.0\n",
      "iteration 2256 loss 2.4246714115142822, acc 23.4375\n",
      "iteration 2257 loss 2.708683490753174, acc 26.5625\n",
      "iteration 2258 loss 2.717648506164551, acc 20.3125\n",
      "iteration 2259 loss 2.5685818195343018, acc 20.3125\n",
      "iteration 2260 loss 2.742159605026245, acc 18.75\n",
      "iteration 2261 loss 2.5721044540405273, acc 26.5625\n",
      "iteration 2262 loss 2.7337937355041504, acc 21.875\n",
      "iteration 2263 loss 2.725492238998413, acc 18.75\n",
      "iteration 2264 loss 2.776914358139038, acc 21.875\n",
      "iteration 2265 loss 2.7089388370513916, acc 15.625\n",
      "iteration 2266 loss 2.4987852573394775, acc 26.5625\n",
      "iteration 2267 loss 2.4903182983398438, acc 31.25\n",
      "iteration 2268 loss 2.6095807552337646, acc 26.5625\n",
      "iteration 2269 loss 2.956993818283081, acc 9.375\n",
      "iteration 2270 loss 2.46665358543396, acc 20.3125\n",
      "iteration 2271 loss 2.6747329235076904, acc 14.0625\n",
      "iteration 2272 loss 2.6191489696502686, acc 21.875\n",
      "iteration 2273 loss 2.6201629638671875, acc 21.875\n",
      "iteration 2274 loss 2.4644877910614014, acc 32.8125\n",
      "iteration 2275 loss 2.718371629714966, acc 21.875\n",
      "iteration 2276 loss 2.8131449222564697, acc 23.4375\n",
      "iteration 2277 loss 2.7327821254730225, acc 28.125\n",
      "iteration 2278 loss 2.6666417121887207, acc 21.875\n",
      "iteration 2279 loss 2.785433769226074, acc 20.3125\n",
      "iteration 2280 loss 2.6434245109558105, acc 15.625\n",
      "iteration 2281 loss 2.7535595893859863, acc 29.6875\n",
      "iteration 2282 loss 2.4734740257263184, acc 28.125\n",
      "iteration 2283 loss 2.4736156463623047, acc 32.8125\n",
      "iteration 2284 loss 2.638822317123413, acc 29.6875\n",
      "iteration 2285 loss 2.592456340789795, acc 23.4375\n",
      "iteration 2286 loss 2.620636224746704, acc 23.4375\n",
      "iteration 2287 loss 2.6561524868011475, acc 28.125\n",
      "iteration 2288 loss 2.547635316848755, acc 26.5625\n",
      "iteration 2289 loss 2.608701467514038, acc 25.0\n",
      "iteration 2290 loss 2.6932663917541504, acc 25.0\n",
      "iteration 2291 loss 2.5472168922424316, acc 23.4375\n",
      "iteration 2292 loss 2.5280706882476807, acc 23.4375\n",
      "iteration 2293 loss 2.659067392349243, acc 18.75\n",
      "iteration 2294 loss 2.6973116397857666, acc 15.625\n",
      "iteration 2295 loss 2.8844244480133057, acc 15.625\n",
      "iteration 2296 loss 2.5768227577209473, acc 26.5625\n",
      "iteration 2297 loss 2.708421468734741, acc 20.3125\n",
      "iteration 2298 loss 2.5401816368103027, acc 28.125\n",
      "iteration 2299 loss 2.699497699737549, acc 17.1875\n",
      "iteration 2300 loss 2.631622076034546, acc 17.1875\n",
      "iteration 2301 loss 2.66469144821167, acc 25.0\n",
      "iteration 2302 loss 2.8213727474212646, acc 17.1875\n",
      "iteration 2303 loss 2.776092290878296, acc 15.625\n",
      "iteration 2304 loss 2.833017349243164, acc 10.9375\n",
      "iteration 2305 loss 2.5899176597595215, acc 26.5625\n",
      "iteration 2306 loss 2.699547529220581, acc 20.3125\n",
      "iteration 2307 loss 2.6750223636627197, acc 20.3125\n",
      "iteration 2308 loss 2.5753300189971924, acc 17.1875\n",
      "iteration 2309 loss 2.7851028442382812, acc 18.75\n",
      "iteration 2310 loss 2.56905198097229, acc 31.25\n",
      "iteration 2311 loss 2.577704668045044, acc 25.0\n",
      "iteration 2312 loss 2.5744097232818604, acc 26.5625\n",
      "iteration 2313 loss 2.6992928981781006, acc 21.875\n",
      "iteration 2314 loss 2.6005184650421143, acc 26.5625\n",
      "iteration 2315 loss 2.591865301132202, acc 20.3125\n",
      "iteration 2316 loss 2.6893954277038574, acc 23.4375\n",
      "iteration 2317 loss 2.6422502994537354, acc 26.5625\n",
      "iteration 2318 loss 2.6842899322509766, acc 26.5625\n",
      "iteration 2319 loss 2.8749821186065674, acc 12.5\n",
      "iteration 2320 loss 2.6417789459228516, acc 21.875\n",
      "iteration 2321 loss 2.8426876068115234, acc 18.75\n",
      "iteration 2322 loss 2.831098794937134, acc 18.75\n",
      "iteration 2323 loss 2.5241408348083496, acc 37.5\n",
      "iteration 2324 loss 2.467824697494507, acc 26.5625\n",
      "iteration 2325 loss 2.677027940750122, acc 26.5625\n",
      "iteration 2326 loss 2.6082444190979004, acc 23.4375\n",
      "iteration 2327 loss 2.8409218788146973, acc 17.1875\n",
      "iteration 2328 loss 2.882828950881958, acc 14.0625\n",
      "iteration 2329 loss 2.8558902740478516, acc 17.1875\n",
      "iteration 2330 loss 2.8479886054992676, acc 20.3125\n",
      "iteration 2331 loss 2.72099232673645, acc 21.875\n",
      "iteration 2332 loss 2.6641225814819336, acc 21.875\n",
      "iteration 2333 loss 2.5642879009246826, acc 37.5\n",
      "iteration 2334 loss 2.667497158050537, acc 25.0\n",
      "iteration 2335 loss 2.6499295234680176, acc 20.3125\n",
      "iteration 2336 loss 2.6983284950256348, acc 18.75\n",
      "iteration 2337 loss 2.906496047973633, acc 10.9375\n",
      "iteration 2338 loss 2.69773006439209, acc 32.8125\n",
      "iteration 2339 loss 2.697349786758423, acc 25.0\n",
      "iteration 2340 loss 2.731215476989746, acc 15.625\n",
      "iteration 2341 loss 2.6594767570495605, acc 20.3125\n",
      "iteration 2342 loss 2.5094847679138184, acc 26.5625\n",
      "iteration 2343 loss 2.7941672801971436, acc 20.3125\n",
      "iteration 2344 loss 2.7302446365356445, acc 21.875\n",
      "iteration 2345 loss 2.639359712600708, acc 31.25\n",
      "iteration 2346 loss 2.5907554626464844, acc 26.5625\n",
      "iteration 2347 loss 2.718050479888916, acc 21.875\n",
      "iteration 2348 loss 2.6621687412261963, acc 23.4375\n",
      "iteration 2349 loss 2.743455648422241, acc 18.75\n",
      "iteration 2350 loss 2.7156083583831787, acc 17.1875\n",
      "iteration 2351 loss 2.9181149005889893, acc 7.8125\n",
      "iteration 2352 loss 2.895491600036621, acc 7.8125\n",
      "iteration 2353 loss 2.824418067932129, acc 21.875\n",
      "iteration 2354 loss 2.661526679992676, acc 20.3125\n",
      "iteration 2355 loss 2.896695852279663, acc 18.75\n",
      "iteration 2356 loss 2.625486373901367, acc 26.5625\n",
      "iteration 2357 loss 2.909679651260376, acc 18.75\n",
      "iteration 2358 loss 2.580678701400757, acc 21.875\n",
      "iteration 2359 loss 2.690023899078369, acc 17.1875\n",
      "iteration 2360 loss 2.719097137451172, acc 20.3125\n",
      "iteration 2361 loss 2.806684732437134, acc 17.1875\n",
      "iteration 2362 loss 2.6663153171539307, acc 20.3125\n",
      "iteration 2363 loss 2.5844829082489014, acc 21.875\n",
      "iteration 2364 loss 2.70693302154541, acc 23.4375\n",
      "iteration 2365 loss 2.824172258377075, acc 17.1875\n",
      "iteration 2366 loss 2.683729410171509, acc 18.75\n",
      "iteration 2367 loss 2.941585063934326, acc 25.0\n",
      "iteration 2368 loss 2.659834146499634, acc 20.3125\n",
      "iteration 2369 loss 2.71099853515625, acc 20.3125\n",
      "iteration 2370 loss 2.5361673831939697, acc 25.0\n",
      "iteration 2371 loss 2.7993004322052, acc 21.875\n",
      "iteration 2372 loss 2.6714529991149902, acc 21.875\n",
      "iteration 2373 loss 2.838881254196167, acc 25.0\n",
      "iteration 2374 loss 2.7291245460510254, acc 18.75\n",
      "iteration 2375 loss 2.725928783416748, acc 26.5625\n",
      "iteration 2376 loss 2.6845529079437256, acc 21.875\n",
      "iteration 2377 loss 2.6420819759368896, acc 21.875\n",
      "iteration 2378 loss 2.9038541316986084, acc 15.625\n",
      "iteration 2379 loss 2.6119256019592285, acc 20.3125\n",
      "iteration 2380 loss 2.925999164581299, acc 12.5\n",
      "iteration 2381 loss 2.6727676391601562, acc 18.75\n",
      "iteration 2382 loss 2.766005516052246, acc 14.0625\n",
      "iteration 2383 loss 2.6758739948272705, acc 9.375\n",
      "iteration 2384 loss 2.744093894958496, acc 14.0625\n",
      "iteration 2385 loss 2.5912206172943115, acc 17.1875\n",
      "iteration 2386 loss 2.738675355911255, acc 23.4375\n",
      "iteration 2387 loss 2.9090707302093506, acc 20.3125\n",
      "iteration 2388 loss 2.473740816116333, acc 28.125\n",
      "iteration 2389 loss 2.932345390319824, acc 12.5\n",
      "iteration 2390 loss 2.5310866832733154, acc 25.0\n",
      "iteration 2391 loss 2.565864086151123, acc 21.875\n",
      "iteration 2392 loss 2.6540448665618896, acc 20.3125\n",
      "iteration 2393 loss 2.5491480827331543, acc 21.875\n",
      "iteration 2394 loss 2.664982318878174, acc 23.4375\n",
      "iteration 2395 loss 2.764293670654297, acc 18.75\n",
      "iteration 2396 loss 2.652674913406372, acc 23.4375\n",
      "iteration 2397 loss 2.6516835689544678, acc 21.875\n",
      "iteration 2398 loss 2.8013360500335693, acc 18.75\n",
      "iteration 2399 loss 2.578348159790039, acc 23.4375\n",
      "iteration 2400 loss 2.8669116497039795, acc 20.3125\n",
      "iteration 2401 loss 2.437953472137451, acc 26.5625\n",
      "iteration 2402 loss 2.6996164321899414, acc 23.4375\n",
      "iteration 2403 loss 2.6565537452697754, acc 15.625\n",
      "iteration 2404 loss 2.721484422683716, acc 21.875\n",
      "iteration 2405 loss 2.910404682159424, acc 14.0625\n",
      "iteration 2406 loss 2.607719659805298, acc 23.4375\n",
      "iteration 2407 loss 2.5754482746124268, acc 29.6875\n",
      "iteration 2408 loss 2.745534896850586, acc 14.0625\n",
      "iteration 2409 loss 2.6425023078918457, acc 20.3125\n",
      "iteration 2410 loss 2.45127272605896, acc 32.8125\n",
      "iteration 2411 loss 2.7192037105560303, acc 14.0625\n",
      "iteration 2412 loss 2.737722158432007, acc 25.0\n",
      "iteration 2413 loss 2.6060781478881836, acc 20.3125\n",
      "iteration 2414 loss 2.7242484092712402, acc 20.3125\n",
      "iteration 2415 loss 2.572751045227051, acc 15.625\n",
      "iteration 2416 loss 2.6897032260894775, acc 26.5625\n",
      "iteration 2417 loss 2.5695722103118896, acc 29.6875\n",
      "iteration 2418 loss 2.777482032775879, acc 21.875\n",
      "iteration 2419 loss 2.8008692264556885, acc 15.625\n",
      "iteration 2420 loss 2.8561794757843018, acc 17.1875\n",
      "iteration 2421 loss 2.6289851665496826, acc 23.4375\n",
      "iteration 2422 loss 2.6083343029022217, acc 25.0\n",
      "iteration 2423 loss 2.8600189685821533, acc 18.75\n",
      "iteration 2424 loss 2.7643585205078125, acc 14.0625\n",
      "iteration 2425 loss 2.701815128326416, acc 26.5625\n",
      "iteration 2426 loss 2.6100852489471436, acc 17.1875\n",
      "iteration 2427 loss 2.5852322578430176, acc 23.4375\n",
      "iteration 2428 loss 2.6490769386291504, acc 29.6875\n",
      "iteration 2429 loss 2.6987662315368652, acc 20.3125\n",
      "iteration 2430 loss 2.9553256034851074, acc 15.625\n",
      "iteration 2431 loss 2.5575618743896484, acc 20.3125\n",
      "iteration 2432 loss 2.517238140106201, acc 25.0\n",
      "iteration 2433 loss 2.794553756713867, acc 18.75\n",
      "iteration 2434 loss 2.8994264602661133, acc 14.0625\n",
      "iteration 2435 loss 2.740781307220459, acc 25.0\n",
      "iteration 2436 loss 2.7066290378570557, acc 17.1875\n",
      "iteration 2437 loss 2.713367223739624, acc 15.625\n",
      "iteration 2438 loss 2.8156940937042236, acc 25.0\n",
      "iteration 2439 loss 2.6242454051971436, acc 23.4375\n",
      "iteration 2440 loss 2.7953014373779297, acc 18.75\n",
      "iteration 2441 loss 2.4627838134765625, acc 29.6875\n",
      "iteration 2442 loss 2.5649657249450684, acc 29.6875\n",
      "iteration 2443 loss 2.780103921890259, acc 18.75\n",
      "iteration 2444 loss 2.6731512546539307, acc 23.4375\n",
      "iteration 2445 loss 2.6491870880126953, acc 20.3125\n",
      "iteration 2446 loss 2.6444084644317627, acc 21.875\n",
      "iteration 2447 loss 2.8066556453704834, acc 18.75\n",
      "iteration 2448 loss 2.779139518737793, acc 17.1875\n",
      "iteration 2449 loss 2.9363327026367188, acc 14.0625\n",
      "iteration 2450 loss 2.5499143600463867, acc 25.0\n",
      "iteration 2451 loss 2.6969242095947266, acc 23.4375\n",
      "iteration 2452 loss 2.679555654525757, acc 18.75\n",
      "iteration 2453 loss 2.8425581455230713, acc 25.0\n",
      "iteration 2454 loss 2.844393730163574, acc 25.0\n",
      "iteration 2455 loss 2.7315895557403564, acc 25.0\n",
      "iteration 2456 loss 2.6464555263519287, acc 31.25\n",
      "iteration 2457 loss 2.8266217708587646, acc 17.1875\n",
      "iteration 2458 loss 2.6364328861236572, acc 26.5625\n",
      "iteration 2459 loss 2.676325798034668, acc 26.5625\n",
      "iteration 2460 loss 2.6638951301574707, acc 23.4375\n",
      "iteration 2461 loss 3.003304958343506, acc 17.1875\n",
      "iteration 2462 loss 2.7201669216156006, acc 21.875\n",
      "iteration 2463 loss 2.69429874420166, acc 23.4375\n",
      "iteration 2464 loss 2.7390356063842773, acc 20.3125\n",
      "iteration 2465 loss 2.511979341506958, acc 28.125\n",
      "iteration 2466 loss 2.664501905441284, acc 25.0\n",
      "iteration 2467 loss 2.771641492843628, acc 20.3125\n",
      "iteration 2468 loss 2.7860710620880127, acc 20.3125\n",
      "iteration 2469 loss 2.7731401920318604, acc 17.1875\n",
      "iteration 2470 loss 2.6481597423553467, acc 29.6875\n",
      "iteration 2471 loss 2.660195827484131, acc 17.1875\n",
      "iteration 2472 loss 2.6751480102539062, acc 28.125\n",
      "iteration 2473 loss 2.6523585319519043, acc 15.625\n",
      "iteration 2474 loss 2.6716468334198, acc 26.5625\n",
      "iteration 2475 loss 2.459390878677368, acc 26.5625\n",
      "iteration 2476 loss 2.697584629058838, acc 15.625\n",
      "iteration 2477 loss 2.7009594440460205, acc 20.3125\n",
      "iteration 2478 loss 2.8313393592834473, acc 17.1875\n",
      "iteration 2479 loss 2.716003656387329, acc 20.3125\n",
      "iteration 2480 loss 2.7779431343078613, acc 20.3125\n",
      "iteration 2481 loss 2.7023134231567383, acc 17.1875\n",
      "iteration 2482 loss 2.6943013668060303, acc 21.875\n",
      "iteration 2483 loss 2.8792359828948975, acc 17.1875\n",
      "iteration 2484 loss 2.5280473232269287, acc 23.4375\n",
      "iteration 2485 loss 2.8372156620025635, acc 18.75\n",
      "iteration 2486 loss 2.8166122436523438, acc 20.3125\n",
      "iteration 2487 loss 2.750777244567871, acc 17.1875\n",
      "iteration 2488 loss 2.64444637298584, acc 25.0\n",
      "iteration 2489 loss 2.6742208003997803, acc 23.4375\n",
      "iteration 2490 loss 2.4903929233551025, acc 34.375\n",
      "iteration 2491 loss 2.744004964828491, acc 21.875\n",
      "iteration 2492 loss 2.865039587020874, acc 14.0625\n",
      "iteration 2493 loss 2.751988649368286, acc 15.625\n",
      "iteration 2494 loss 2.689992904663086, acc 15.625\n",
      "iteration 2495 loss 2.657454252243042, acc 18.75\n",
      "iteration 2496 loss 2.7214035987854004, acc 21.875\n",
      "iteration 2497 loss 2.5720880031585693, acc 21.875\n",
      "iteration 2498 loss 2.788804054260254, acc 20.3125\n",
      "iteration 2499 loss 2.69321608543396, acc 26.5625\n",
      "iteration 2500 loss 3.037003755569458, acc 14.0625\n",
      "iteration 2501 loss 2.9066343307495117, acc 18.75\n",
      "iteration 2502 loss 2.646127462387085, acc 23.4375\n",
      "iteration 2503 loss 2.701597213745117, acc 26.5625\n",
      "iteration 2504 loss 2.792900562286377, acc 14.0625\n",
      "iteration 2505 loss 2.611161947250366, acc 20.3125\n",
      "iteration 2506 loss 2.7492902278900146, acc 18.75\n",
      "iteration 2507 loss 2.8884284496307373, acc 14.0625\n",
      "iteration 2508 loss 2.7118606567382812, acc 17.1875\n",
      "iteration 2509 loss 2.694948673248291, acc 18.75\n",
      "iteration 2510 loss 2.820852041244507, acc 14.0625\n",
      "iteration 2511 loss 2.676520347595215, acc 20.3125\n",
      "iteration 2512 loss 2.6513030529022217, acc 14.0625\n",
      "iteration 2513 loss 2.772199869155884, acc 14.0625\n",
      "iteration 2514 loss 2.7804441452026367, acc 14.0625\n",
      "iteration 2515 loss 2.9707446098327637, acc 17.1875\n",
      "iteration 2516 loss 2.895543336868286, acc 17.1875\n",
      "iteration 2517 loss 2.6341652870178223, acc 23.4375\n",
      "iteration 2518 loss 2.745710849761963, acc 20.3125\n",
      "iteration 2519 loss 2.625293254852295, acc 28.125\n",
      "iteration 2520 loss 2.9020512104034424, acc 12.5\n",
      "iteration 2521 loss 2.8190624713897705, acc 25.0\n",
      "iteration 2522 loss 2.5404186248779297, acc 29.6875\n",
      "iteration 2523 loss 2.6931185722351074, acc 23.4375\n",
      "iteration 2524 loss 2.4489686489105225, acc 28.125\n",
      "iteration 2525 loss 2.973093032836914, acc 14.0625\n",
      "iteration 2526 loss 2.7233245372772217, acc 21.875\n",
      "iteration 2527 loss 2.6668479442596436, acc 25.0\n",
      "iteration 2528 loss 2.571453094482422, acc 18.75\n",
      "iteration 2529 loss 2.7644617557525635, acc 14.0625\n",
      "iteration 2530 loss 3.017122507095337, acc 7.8125\n",
      "iteration 2531 loss 2.479361057281494, acc 20.3125\n",
      "iteration 2532 loss 2.8258275985717773, acc 25.0\n",
      "iteration 2533 loss 2.4956157207489014, acc 23.4375\n",
      "iteration 2534 loss 2.564581871032715, acc 28.125\n",
      "iteration 2535 loss 2.6021547317504883, acc 28.125\n",
      "iteration 2536 loss 2.696791172027588, acc 23.4375\n",
      "iteration 2537 loss 2.657538414001465, acc 29.6875\n",
      "iteration 2538 loss 2.8314809799194336, acc 20.3125\n",
      "iteration 2539 loss 2.8785195350646973, acc 21.875\n",
      "iteration 2540 loss 2.6560840606689453, acc 15.625\n",
      "iteration 2541 loss 2.5677154064178467, acc 14.0625\n",
      "iteration 2542 loss 2.7670304775238037, acc 17.1875\n",
      "iteration 2543 loss 2.7651686668395996, acc 20.3125\n",
      "iteration 2544 loss 2.8570377826690674, acc 12.5\n",
      "iteration 2545 loss 2.7221693992614746, acc 18.75\n",
      "iteration 2546 loss 2.755056619644165, acc 20.3125\n",
      "iteration 2547 loss 2.5731852054595947, acc 21.875\n",
      "iteration 2548 loss 2.6819005012512207, acc 23.4375\n",
      "iteration 2549 loss 2.717552423477173, acc 18.75\n",
      "iteration 2550 loss 2.772474527359009, acc 20.3125\n",
      "iteration 2551 loss 2.572354316711426, acc 25.0\n",
      "iteration 2552 loss 2.792067527770996, acc 17.1875\n",
      "iteration 2553 loss 2.7260069847106934, acc 15.625\n",
      "iteration 2554 loss 2.7568788528442383, acc 21.875\n",
      "iteration 2555 loss 2.6439578533172607, acc 25.0\n",
      "iteration 2556 loss 2.5066311359405518, acc 29.6875\n",
      "iteration 2557 loss 2.752624273300171, acc 10.9375\n",
      "iteration 2558 loss 2.691154956817627, acc 17.1875\n",
      "iteration 2559 loss 2.559833526611328, acc 26.5625\n",
      "iteration 2560 loss 2.833000659942627, acc 15.625\n",
      "iteration 2561 loss 2.5346951484680176, acc 23.4375\n",
      "iteration 2562 loss 2.6923112869262695, acc 26.5625\n",
      "iteration 2563 loss 2.7609922885894775, acc 18.75\n",
      "iteration 2564 loss 2.5822219848632812, acc 18.75\n",
      "iteration 2565 loss 2.679640054702759, acc 23.4375\n",
      "iteration 2566 loss 2.8906638622283936, acc 23.4375\n",
      "iteration 2567 loss 2.736114025115967, acc 17.1875\n",
      "iteration 2568 loss 2.7466554641723633, acc 20.3125\n",
      "iteration 2569 loss 2.546724319458008, acc 20.3125\n",
      "iteration 2570 loss 2.6069767475128174, acc 21.875\n",
      "iteration 2571 loss 2.642174005508423, acc 15.625\n",
      "iteration 2572 loss 2.5998873710632324, acc 17.1875\n",
      "iteration 2573 loss 2.77667498588562, acc 12.5\n",
      "iteration 2574 loss 2.6507205963134766, acc 26.5625\n",
      "iteration 2575 loss 2.7872231006622314, acc 15.625\n",
      "iteration 2576 loss 2.621098279953003, acc 21.875\n",
      "iteration 2577 loss 2.5879619121551514, acc 20.3125\n",
      "iteration 2578 loss 2.6353600025177, acc 18.75\n",
      "iteration 2579 loss 2.8447372913360596, acc 15.625\n",
      "iteration 2580 loss 2.544269561767578, acc 17.1875\n",
      "iteration 2581 loss 2.8629233837127686, acc 21.875\n",
      "iteration 2582 loss 2.5854787826538086, acc 28.125\n",
      "iteration 2583 loss 2.759531259536743, acc 23.4375\n",
      "iteration 2584 loss 2.682762622833252, acc 20.3125\n",
      "iteration 2585 loss 2.6175642013549805, acc 29.6875\n",
      "iteration 2586 loss 2.49314546585083, acc 28.125\n",
      "iteration 2587 loss 2.860199213027954, acc 14.0625\n",
      "iteration 2588 loss 2.7297444343566895, acc 20.3125\n",
      "iteration 2589 loss 2.685990810394287, acc 21.875\n",
      "iteration 2590 loss 2.8992879390716553, acc 17.1875\n",
      "iteration 2591 loss 2.76434063911438, acc 15.625\n",
      "iteration 2592 loss 2.8965303897857666, acc 14.0625\n",
      "iteration 2593 loss 2.8187549114227295, acc 20.3125\n",
      "iteration 2594 loss 2.885573625564575, acc 14.0625\n",
      "iteration 2595 loss 2.6467137336730957, acc 29.6875\n",
      "iteration 2596 loss 2.5452933311462402, acc 25.0\n",
      "iteration 2597 loss 2.6847496032714844, acc 28.125\n",
      "iteration 2598 loss 2.8166046142578125, acc 20.3125\n",
      "iteration 2599 loss 2.7058374881744385, acc 26.5625\n",
      "iteration 2600 loss 2.4872870445251465, acc 29.6875\n",
      "iteration 2601 loss 2.6124112606048584, acc 20.3125\n",
      "iteration 2602 loss 2.6924612522125244, acc 26.5625\n",
      "iteration 2603 loss 2.6448922157287598, acc 21.875\n",
      "iteration 2604 loss 2.8204431533813477, acc 14.0625\n",
      "iteration 2605 loss 2.8275399208068848, acc 14.0625\n",
      "iteration 2606 loss 2.662945508956909, acc 17.1875\n",
      "iteration 2607 loss 2.7546651363372803, acc 17.1875\n",
      "iteration 2608 loss 2.750356674194336, acc 15.625\n",
      "iteration 2609 loss 2.7147462368011475, acc 20.3125\n",
      "iteration 2610 loss 3.1098060607910156, acc 14.0625\n",
      "iteration 2611 loss 2.385692596435547, acc 28.125\n",
      "iteration 2612 loss 2.7521257400512695, acc 17.1875\n",
      "iteration 2613 loss 2.61763596534729, acc 25.0\n",
      "iteration 2614 loss 2.8346681594848633, acc 18.75\n",
      "iteration 2615 loss 2.4826743602752686, acc 26.5625\n",
      "iteration 2616 loss 3.0097780227661133, acc 12.5\n",
      "iteration 2617 loss 2.55726957321167, acc 23.4375\n",
      "iteration 2618 loss 2.527985095977783, acc 31.25\n",
      "iteration 2619 loss 2.5526671409606934, acc 28.125\n",
      "iteration 2620 loss 2.5380444526672363, acc 29.6875\n",
      "iteration 2621 loss 2.681241035461426, acc 18.75\n",
      "iteration 2622 loss 2.651277542114258, acc 18.75\n",
      "iteration 2623 loss 2.800086736679077, acc 12.5\n",
      "iteration 2624 loss 2.5461373329162598, acc 18.75\n",
      "iteration 2625 loss 2.626044511795044, acc 18.75\n",
      "iteration 2626 loss 2.7388226985931396, acc 17.1875\n",
      "iteration 2627 loss 2.7889137268066406, acc 18.75\n",
      "iteration 2628 loss 2.6159162521362305, acc 26.5625\n",
      "iteration 2629 loss 2.6315715312957764, acc 14.0625\n",
      "iteration 2630 loss 2.878659248352051, acc 23.4375\n",
      "iteration 2631 loss 2.72556471824646, acc 21.875\n",
      "iteration 2632 loss 2.671832799911499, acc 26.5625\n",
      "iteration 2633 loss 2.7081007957458496, acc 23.4375\n",
      "iteration 2634 loss 2.5417656898498535, acc 25.0\n",
      "iteration 2635 loss 2.808043956756592, acc 17.1875\n",
      "iteration 2636 loss 2.749424695968628, acc 17.1875\n",
      "iteration 2637 loss 2.612727165222168, acc 21.875\n",
      "iteration 2638 loss 2.66279935836792, acc 25.0\n",
      "iteration 2639 loss 2.707970380783081, acc 23.4375\n",
      "iteration 2640 loss 2.653937578201294, acc 26.5625\n",
      "iteration 2641 loss 2.5907163619995117, acc 25.0\n",
      "iteration 2642 loss 2.6137301921844482, acc 25.0\n",
      "iteration 2643 loss 2.8932440280914307, acc 20.3125\n",
      "iteration 2644 loss 2.7054994106292725, acc 25.0\n",
      "iteration 2645 loss 2.801867961883545, acc 21.875\n",
      "iteration 2646 loss 2.931084156036377, acc 15.625\n",
      "iteration 2647 loss 2.5828380584716797, acc 23.4375\n",
      "iteration 2648 loss 2.7959158420562744, acc 21.875\n",
      "iteration 2649 loss 2.8396353721618652, acc 21.875\n",
      "iteration 2650 loss 2.773613691329956, acc 20.3125\n",
      "iteration 2651 loss 2.6834192276000977, acc 21.875\n",
      "iteration 2652 loss 2.591637372970581, acc 25.0\n",
      "iteration 2653 loss 2.5152056217193604, acc 21.875\n",
      "iteration 2654 loss 2.5181665420532227, acc 31.25\n",
      "iteration 2655 loss 2.4811620712280273, acc 31.25\n",
      "iteration 2656 loss 2.754340171813965, acc 18.75\n",
      "iteration 2657 loss 2.644164562225342, acc 15.625\n",
      "iteration 2658 loss 2.648658037185669, acc 25.0\n",
      "iteration 2659 loss 2.9162344932556152, acc 21.875\n",
      "iteration 2660 loss 2.6178667545318604, acc 26.5625\n",
      "iteration 2661 loss 2.576632499694824, acc 29.6875\n",
      "iteration 2662 loss 2.686830520629883, acc 18.75\n",
      "iteration 2663 loss 2.71602463722229, acc 25.0\n",
      "iteration 2664 loss 2.7110533714294434, acc 23.4375\n",
      "iteration 2665 loss 2.891892194747925, acc 12.5\n",
      "iteration 2666 loss 2.738593578338623, acc 18.75\n",
      "iteration 2667 loss 2.7733607292175293, acc 17.1875\n",
      "iteration 2668 loss 2.8341174125671387, acc 17.1875\n",
      "iteration 2669 loss 2.5461349487304688, acc 26.5625\n",
      "iteration 2670 loss 2.60555362701416, acc 23.4375\n",
      "iteration 2671 loss 2.7813360691070557, acc 26.5625\n",
      "iteration 2672 loss 2.534560441970825, acc 37.5\n",
      "iteration 2673 loss 2.7466890811920166, acc 23.4375\n",
      "iteration 2674 loss 2.845907688140869, acc 20.3125\n",
      "iteration 2675 loss 2.688825845718384, acc 23.4375\n",
      "iteration 2676 loss 2.797388792037964, acc 10.9375\n",
      "iteration 2677 loss 2.552471160888672, acc 26.5625\n",
      "iteration 2678 loss 2.6280524730682373, acc 21.875\n",
      "iteration 2679 loss 2.837553024291992, acc 17.1875\n",
      "iteration 2680 loss 2.830982208251953, acc 17.1875\n",
      "iteration 2681 loss 2.7202401161193848, acc 25.0\n",
      "iteration 2682 loss 2.594341278076172, acc 21.875\n",
      "iteration 2683 loss 2.7118661403656006, acc 26.5625\n",
      "iteration 2684 loss 2.7942497730255127, acc 17.1875\n",
      "iteration 2685 loss 2.948368549346924, acc 18.75\n",
      "iteration 2686 loss 2.7435364723205566, acc 18.75\n",
      "iteration 2687 loss 2.588299512863159, acc 28.125\n",
      "iteration 2688 loss 2.863609552383423, acc 17.1875\n",
      "iteration 2689 loss 2.5107264518737793, acc 25.0\n",
      "iteration 2690 loss 2.786290168762207, acc 18.75\n",
      "iteration 2691 loss 2.720949172973633, acc 20.3125\n",
      "iteration 2692 loss 2.5362069606781006, acc 26.5625\n",
      "iteration 2693 loss 2.787485122680664, acc 17.1875\n",
      "iteration 2694 loss 2.698110342025757, acc 26.5625\n",
      "iteration 2695 loss 2.8484175205230713, acc 20.3125\n",
      "iteration 2696 loss 2.761664628982544, acc 20.3125\n",
      "iteration 2697 loss 2.5065712928771973, acc 28.125\n",
      "iteration 2698 loss 2.4701220989227295, acc 25.0\n",
      "iteration 2699 loss 2.6297049522399902, acc 25.0\n",
      "iteration 2700 loss 2.7106785774230957, acc 26.5625\n",
      "iteration 2701 loss 2.833667755126953, acc 21.875\n",
      "iteration 2702 loss 2.657020330429077, acc 21.875\n",
      "iteration 2703 loss 2.8834335803985596, acc 18.75\n",
      "iteration 2704 loss 2.746298313140869, acc 21.875\n",
      "iteration 2705 loss 2.70955753326416, acc 21.875\n",
      "iteration 2706 loss 2.642624855041504, acc 26.5625\n",
      "iteration 2707 loss 2.651252031326294, acc 21.875\n",
      "iteration 2708 loss 2.7028563022613525, acc 25.0\n",
      "iteration 2709 loss 2.6594438552856445, acc 28.125\n",
      "iteration 2710 loss 2.648953676223755, acc 26.5625\n",
      "iteration 2711 loss 2.908843755722046, acc 17.1875\n",
      "iteration 2712 loss 2.4564058780670166, acc 31.25\n",
      "iteration 2713 loss 2.6864192485809326, acc 21.875\n",
      "iteration 2714 loss 2.6269164085388184, acc 20.3125\n",
      "iteration 2715 loss 2.626215696334839, acc 25.0\n",
      "iteration 2716 loss 2.760514736175537, acc 10.9375\n",
      "iteration 2717 loss 2.7147090435028076, acc 14.0625\n",
      "iteration 2718 loss 2.7944793701171875, acc 9.375\n",
      "iteration 2719 loss 2.434643507003784, acc 21.875\n",
      "iteration 2720 loss 2.72833514213562, acc 26.5625\n",
      "iteration 2721 loss 2.6311228275299072, acc 23.4375\n",
      "iteration 2722 loss 2.574741840362549, acc 26.5625\n",
      "iteration 2723 loss 2.5792500972747803, acc 29.6875\n",
      "iteration 2724 loss 2.8065829277038574, acc 21.875\n",
      "iteration 2725 loss 2.7448017597198486, acc 20.3125\n",
      "iteration 2726 loss 2.502683162689209, acc 26.5625\n",
      "iteration 2727 loss 2.6879055500030518, acc 23.4375\n",
      "iteration 2728 loss 2.8108224868774414, acc 20.3125\n",
      "iteration 2729 loss 2.6664912700653076, acc 23.4375\n",
      "iteration 2730 loss 2.7675507068634033, acc 18.75\n",
      "iteration 2731 loss 2.786696672439575, acc 12.5\n",
      "iteration 2732 loss 2.799494743347168, acc 3.125\n",
      "iteration 2733 loss 2.721480131149292, acc 18.75\n",
      "iteration 2734 loss 2.5651819705963135, acc 23.4375\n",
      "iteration 2735 loss 2.6571154594421387, acc 28.125\n",
      "iteration 2736 loss 2.9618160724639893, acc 17.1875\n",
      "iteration 2737 loss 2.7747397422790527, acc 20.3125\n",
      "iteration 2738 loss 2.871567726135254, acc 17.1875\n",
      "iteration 2739 loss 2.7526447772979736, acc 26.5625\n",
      "iteration 2740 loss 2.654449462890625, acc 21.875\n",
      "iteration 2741 loss 2.665613889694214, acc 18.75\n",
      "iteration 2742 loss 2.9904706478118896, acc 15.625\n",
      "iteration 2743 loss 2.6495580673217773, acc 20.3125\n",
      "iteration 2744 loss 2.721204996109009, acc 15.625\n",
      "iteration 2745 loss 2.7222654819488525, acc 26.5625\n",
      "iteration 2746 loss 2.5326502323150635, acc 28.125\n",
      "iteration 2747 loss 2.8092100620269775, acc 14.0625\n",
      "iteration 2748 loss 2.6935131549835205, acc 25.0\n",
      "iteration 2749 loss 2.6543006896972656, acc 20.3125\n",
      "iteration 2750 loss 2.541935920715332, acc 29.6875\n",
      "iteration 2751 loss 2.7713632583618164, acc 20.3125\n",
      "iteration 2752 loss 2.4887657165527344, acc 29.6875\n",
      "iteration 2753 loss 2.9766640663146973, acc 14.0625\n",
      "iteration 2754 loss 2.624375343322754, acc 29.6875\n",
      "iteration 2755 loss 2.776571750640869, acc 17.1875\n",
      "iteration 2756 loss 2.625195264816284, acc 31.25\n",
      "iteration 2757 loss 2.8437423706054688, acc 14.0625\n",
      "iteration 2758 loss 2.85764217376709, acc 14.0625\n",
      "iteration 2759 loss 2.7286722660064697, acc 20.3125\n",
      "iteration 2760 loss 2.8532869815826416, acc 15.625\n",
      "iteration 2761 loss 2.5934762954711914, acc 29.6875\n",
      "iteration 2762 loss 2.6977763175964355, acc 23.4375\n",
      "iteration 2763 loss 2.5401759147644043, acc 28.125\n",
      "iteration 2764 loss 2.7718844413757324, acc 21.875\n",
      "iteration 2765 loss 2.6666810512542725, acc 21.875\n",
      "iteration 2766 loss 2.691049575805664, acc 25.0\n",
      "iteration 2767 loss 2.841920852661133, acc 15.625\n",
      "iteration 2768 loss 2.4420907497406006, acc 28.125\n",
      "iteration 2769 loss 2.6343398094177246, acc 18.75\n",
      "iteration 2770 loss 2.516369104385376, acc 28.125\n",
      "iteration 2771 loss 2.8077290058135986, acc 20.3125\n",
      "iteration 2772 loss 2.5795319080352783, acc 26.5625\n",
      "iteration 2773 loss 2.662060499191284, acc 28.125\n",
      "iteration 2774 loss 2.6697864532470703, acc 26.5625\n",
      "iteration 2775 loss 2.604353904724121, acc 28.125\n",
      "iteration 2776 loss 2.7278356552124023, acc 28.125\n",
      "iteration 2777 loss 2.78446364402771, acc 12.5\n",
      "iteration 2778 loss 2.7547194957733154, acc 15.625\n",
      "iteration 2779 loss 2.7907259464263916, acc 23.4375\n",
      "iteration 2780 loss 2.5509376525878906, acc 29.6875\n",
      "iteration 2781 loss 2.802363634109497, acc 18.75\n",
      "iteration 2782 loss 2.766854763031006, acc 21.875\n",
      "iteration 2783 loss 2.6493895053863525, acc 21.875\n",
      "iteration 2784 loss 2.5112218856811523, acc 28.125\n",
      "iteration 2785 loss 2.8346941471099854, acc 21.875\n",
      "iteration 2786 loss 2.6681578159332275, acc 29.6875\n",
      "iteration 2787 loss 2.751629114151001, acc 18.75\n",
      "iteration 2788 loss 2.4556689262390137, acc 25.0\n",
      "iteration 2789 loss 2.7040555477142334, acc 21.875\n",
      "iteration 2790 loss 2.738945484161377, acc 18.75\n",
      "iteration 2791 loss 2.7152044773101807, acc 12.5\n",
      "iteration 2792 loss 2.8774001598358154, acc 17.1875\n",
      "iteration 2793 loss 2.8175456523895264, acc 17.1875\n",
      "iteration 2794 loss 2.4176828861236572, acc 34.375\n",
      "iteration 2795 loss 2.705004930496216, acc 26.5625\n",
      "iteration 2796 loss 2.452037811279297, acc 28.125\n",
      "iteration 2797 loss 2.6394784450531006, acc 29.6875\n",
      "iteration 2798 loss 2.6177327632904053, acc 21.875\n",
      "iteration 2799 loss 2.518249034881592, acc 28.125\n",
      "iteration 2800 loss 2.7180933952331543, acc 21.875\n",
      "iteration 2801 loss 2.8000030517578125, acc 23.4375\n",
      "iteration 2802 loss 2.7211410999298096, acc 21.875\n",
      "iteration 2803 loss 2.7153658866882324, acc 21.875\n",
      "iteration 2804 loss 2.5997133255004883, acc 34.375\n",
      "iteration 2805 loss 2.8734240531921387, acc 20.3125\n",
      "iteration 2806 loss 2.827918291091919, acc 17.1875\n",
      "iteration 2807 loss 2.7241101264953613, acc 21.875\n",
      "iteration 2808 loss 2.8876805305480957, acc 20.3125\n",
      "iteration 2809 loss 2.7408945560455322, acc 17.1875\n",
      "iteration 2810 loss 2.66357159614563, acc 23.4375\n",
      "iteration 2811 loss 2.5104422569274902, acc 23.4375\n",
      "iteration 2812 loss 2.5187225341796875, acc 21.875\n",
      "iteration 2813 loss 2.69490122795105, acc 23.4375\n",
      "iteration 2814 loss 2.8891568183898926, acc 9.375\n",
      "iteration 2815 loss 2.674287796020508, acc 20.3125\n",
      "iteration 2816 loss 2.5068235397338867, acc 18.75\n",
      "iteration 2817 loss 2.9699716567993164, acc 12.5\n",
      "iteration 2818 loss 2.633951187133789, acc 21.875\n",
      "iteration 2819 loss 2.441359758377075, acc 18.75\n",
      "iteration 2820 loss 2.509882688522339, acc 25.0\n",
      "iteration 2821 loss 2.653160333633423, acc 18.75\n",
      "iteration 2822 loss 2.870328187942505, acc 15.625\n",
      "iteration 2823 loss 2.693451166152954, acc 23.4375\n",
      "iteration 2824 loss 2.6379752159118652, acc 18.75\n",
      "iteration 2825 loss 2.596647262573242, acc 32.8125\n",
      "iteration 2826 loss 2.5775809288024902, acc 23.4375\n",
      "iteration 2827 loss 2.9224884510040283, acc 14.0625\n",
      "iteration 2828 loss 2.637554883956909, acc 25.0\n",
      "iteration 2829 loss 2.608187437057495, acc 17.1875\n",
      "iteration 2830 loss 2.415699005126953, acc 37.5\n",
      "iteration 2831 loss 2.632854700088501, acc 25.0\n",
      "iteration 2832 loss 2.6264243125915527, acc 26.5625\n",
      "iteration 2833 loss 2.841956377029419, acc 15.625\n",
      "iteration 2834 loss 2.9022276401519775, acc 15.625\n",
      "iteration 2835 loss 2.8099119663238525, acc 23.4375\n",
      "iteration 2836 loss 2.658731460571289, acc 20.3125\n",
      "iteration 2837 loss 2.6733694076538086, acc 20.3125\n",
      "iteration 2838 loss 2.5931198596954346, acc 26.5625\n",
      "iteration 2839 loss 2.8976337909698486, acc 20.3125\n",
      "iteration 2840 loss 2.576005220413208, acc 31.25\n",
      "iteration 2841 loss 2.935633659362793, acc 12.5\n",
      "iteration 2842 loss 2.684307813644409, acc 21.875\n",
      "iteration 2843 loss 2.707193374633789, acc 21.875\n",
      "iteration 2844 loss 2.7334139347076416, acc 26.5625\n",
      "iteration 2845 loss 2.7104079723358154, acc 20.3125\n",
      "iteration 2846 loss 2.6074061393737793, acc 23.4375\n",
      "iteration 2847 loss 2.6776256561279297, acc 20.3125\n",
      "iteration 2848 loss 2.603543519973755, acc 21.875\n",
      "iteration 2849 loss 2.848971366882324, acc 17.1875\n",
      "iteration 2850 loss 2.8218486309051514, acc 17.1875\n",
      "iteration 2851 loss 2.577427864074707, acc 25.0\n",
      "iteration 2852 loss 2.8152835369110107, acc 21.875\n",
      "iteration 2853 loss 2.8502657413482666, acc 14.0625\n",
      "iteration 2854 loss 2.442472457885742, acc 25.0\n",
      "iteration 2855 loss 2.7020182609558105, acc 14.0625\n",
      "iteration 2856 loss 2.6437954902648926, acc 18.75\n",
      "iteration 2857 loss 2.6734766960144043, acc 20.3125\n",
      "iteration 2858 loss 2.7425308227539062, acc 12.5\n",
      "iteration 2859 loss 2.585056781768799, acc 14.0625\n",
      "iteration 2860 loss 2.674598217010498, acc 21.875\n",
      "iteration 2861 loss 2.818739891052246, acc 17.1875\n",
      "iteration 2862 loss 2.7718400955200195, acc 15.625\n",
      "iteration 2863 loss 2.678361654281616, acc 29.6875\n",
      "iteration 2864 loss 2.6649749279022217, acc 12.5\n",
      "iteration 2865 loss 2.813100576400757, acc 10.9375\n",
      "iteration 2866 loss 2.78955340385437, acc 17.1875\n",
      "iteration 2867 loss 2.7489013671875, acc 25.0\n",
      "iteration 2868 loss 2.6376278400421143, acc 23.4375\n",
      "iteration 2869 loss 2.645385265350342, acc 20.3125\n",
      "iteration 2870 loss 2.8773701190948486, acc 14.0625\n",
      "iteration 2871 loss 2.5820953845977783, acc 20.3125\n",
      "iteration 2872 loss 2.744518995285034, acc 25.0\n",
      "iteration 2873 loss 2.7250139713287354, acc 28.125\n",
      "iteration 2874 loss 2.482908010482788, acc 29.6875\n",
      "iteration 2875 loss 2.5819919109344482, acc 28.125\n",
      "iteration 2876 loss 2.6921627521514893, acc 18.75\n",
      "iteration 2877 loss 2.8019371032714844, acc 25.0\n",
      "iteration 2878 loss 2.7553393840789795, acc 23.4375\n",
      "iteration 2879 loss 2.488905668258667, acc 29.6875\n",
      "iteration 2880 loss 2.7258009910583496, acc 17.1875\n",
      "iteration 2881 loss 2.7704010009765625, acc 23.4375\n",
      "iteration 2882 loss 2.691572666168213, acc 20.3125\n",
      "iteration 2883 loss 2.7185847759246826, acc 14.0625\n",
      "iteration 2884 loss 2.6186091899871826, acc 25.0\n",
      "iteration 2885 loss 2.624323606491089, acc 26.5625\n",
      "iteration 2886 loss 2.5970757007598877, acc 21.875\n",
      "iteration 2887 loss 2.6210107803344727, acc 25.0\n",
      "iteration 2888 loss 2.609670400619507, acc 28.125\n",
      "iteration 2889 loss 2.546064853668213, acc 26.5625\n",
      "iteration 2890 loss 2.6222288608551025, acc 26.5625\n",
      "iteration 2891 loss 2.8320648670196533, acc 18.75\n",
      "iteration 2892 loss 2.547316551208496, acc 25.0\n",
      "iteration 2893 loss 2.700408935546875, acc 17.1875\n",
      "iteration 2894 loss 2.53464412689209, acc 29.6875\n",
      "iteration 2895 loss 2.759998321533203, acc 21.875\n",
      "iteration 2896 loss 2.8583288192749023, acc 21.875\n",
      "iteration 2897 loss 2.7714943885803223, acc 18.75\n",
      "iteration 2898 loss 2.714595317840576, acc 18.75\n",
      "iteration 2899 loss 2.6883888244628906, acc 20.3125\n",
      "iteration 2900 loss 2.800701856613159, acc 29.6875\n",
      "iteration 2901 loss 2.8246819972991943, acc 18.75\n",
      "iteration 2902 loss 2.6604256629943848, acc 21.875\n",
      "iteration 2903 loss 2.675774097442627, acc 21.875\n",
      "iteration 2904 loss 2.7109382152557373, acc 21.875\n",
      "iteration 2905 loss 2.776540756225586, acc 18.75\n",
      "iteration 2906 loss 2.709414005279541, acc 21.875\n",
      "iteration 2907 loss 2.660360336303711, acc 21.875\n",
      "iteration 2908 loss 2.7056493759155273, acc 28.125\n",
      "iteration 2909 loss 2.5983502864837646, acc 25.0\n",
      "iteration 2910 loss 2.7644202709198, acc 17.1875\n",
      "iteration 2911 loss 2.772576332092285, acc 15.625\n",
      "iteration 2912 loss 2.5798592567443848, acc 25.0\n",
      "iteration 2913 loss 2.8863120079040527, acc 21.875\n",
      "iteration 2914 loss 2.6501803398132324, acc 21.875\n",
      "iteration 2915 loss 2.8453404903411865, acc 15.625\n",
      "iteration 2916 loss 2.778679370880127, acc 25.0\n",
      "iteration 2917 loss 2.543660879135132, acc 26.5625\n",
      "iteration 2918 loss 2.5730881690979004, acc 31.25\n",
      "iteration 2919 loss 2.7789297103881836, acc 20.3125\n",
      "iteration 2920 loss 2.7059690952301025, acc 25.0\n",
      "iteration 2921 loss 2.954167127609253, acc 9.375\n",
      "iteration 2922 loss 2.8676578998565674, acc 18.75\n",
      "iteration 2923 loss 2.673625946044922, acc 17.1875\n",
      "iteration 2924 loss 2.612182855606079, acc 26.5625\n",
      "iteration 2925 loss 2.456023693084717, acc 28.125\n",
      "iteration 2926 loss 2.737746000289917, acc 17.1875\n",
      "iteration 2927 loss 2.651224136352539, acc 20.3125\n",
      "iteration 2928 loss 2.826178550720215, acc 17.1875\n",
      "iteration 2929 loss 2.6205573081970215, acc 23.4375\n",
      "iteration 2930 loss 2.451812982559204, acc 31.25\n",
      "iteration 2931 loss 2.74851131439209, acc 18.75\n",
      "iteration 2932 loss 2.830561876296997, acc 21.875\n",
      "iteration 2933 loss 2.617783784866333, acc 25.0\n",
      "iteration 2934 loss 2.605153799057007, acc 25.0\n",
      "iteration 2935 loss 2.879289388656616, acc 17.1875\n",
      "iteration 2936 loss 2.5082201957702637, acc 26.5625\n",
      "iteration 2937 loss 2.5824954509735107, acc 21.875\n",
      "iteration 2938 loss 2.5561106204986572, acc 21.875\n",
      "iteration 2939 loss 2.849963903427124, acc 17.1875\n",
      "iteration 2940 loss 2.6848344802856445, acc 28.125\n",
      "iteration 2941 loss 2.7564237117767334, acc 23.4375\n",
      "iteration 2942 loss 2.603363513946533, acc 31.25\n",
      "iteration 2943 loss 2.7447359561920166, acc 21.875\n",
      "iteration 2944 loss 2.797377347946167, acc 20.3125\n",
      "iteration 2945 loss 2.8027405738830566, acc 26.5625\n",
      "iteration 2946 loss 2.5888640880584717, acc 17.1875\n",
      "iteration 2947 loss 2.6077513694763184, acc 21.875\n",
      "iteration 2948 loss 2.755662441253662, acc 20.3125\n",
      "iteration 2949 loss 2.582799196243286, acc 29.6875\n",
      "iteration 2950 loss 2.694779872894287, acc 23.4375\n",
      "iteration 2951 loss 2.7468440532684326, acc 18.75\n",
      "iteration 2952 loss 2.5331151485443115, acc 28.125\n",
      "iteration 2953 loss 2.713108777999878, acc 21.875\n",
      "iteration 2954 loss 2.8733417987823486, acc 21.875\n",
      "iteration 2955 loss 2.9258956909179688, acc 15.625\n",
      "iteration 2956 loss 2.6925857067108154, acc 20.3125\n",
      "iteration 2957 loss 2.6643102169036865, acc 17.1875\n",
      "iteration 2958 loss 2.5968973636627197, acc 15.625\n",
      "iteration 2959 loss 2.761418104171753, acc 15.625\n",
      "iteration 2960 loss 2.619661808013916, acc 20.3125\n",
      "iteration 2961 loss 2.5920000076293945, acc 20.3125\n",
      "iteration 2962 loss 2.5643277168273926, acc 20.3125\n",
      "iteration 2963 loss 2.7350573539733887, acc 29.6875\n",
      "iteration 2964 loss 2.5089426040649414, acc 34.375\n",
      "iteration 2965 loss 2.704054594039917, acc 21.875\n",
      "iteration 2966 loss 2.678880214691162, acc 17.1875\n",
      "iteration 2967 loss 2.8615918159484863, acc 14.0625\n",
      "iteration 2968 loss 2.6565449237823486, acc 20.3125\n",
      "iteration 2969 loss 2.4820854663848877, acc 20.3125\n",
      "iteration 2970 loss 2.6615796089172363, acc 17.1875\n",
      "iteration 2971 loss 2.5531492233276367, acc 31.25\n",
      "iteration 2972 loss 2.6961848735809326, acc 25.0\n",
      "iteration 2973 loss 2.684276819229126, acc 23.4375\n",
      "iteration 2974 loss 2.9428839683532715, acc 14.0625\n",
      "iteration 2975 loss 2.6729960441589355, acc 21.875\n",
      "iteration 2976 loss 2.7504920959472656, acc 21.875\n",
      "iteration 2977 loss 2.800896644592285, acc 17.1875\n",
      "iteration 2978 loss 2.6081581115722656, acc 21.875\n",
      "iteration 2979 loss 2.9129855632781982, acc 15.625\n",
      "iteration 2980 loss 2.7451107501983643, acc 25.0\n",
      "iteration 2981 loss 2.785270929336548, acc 23.4375\n",
      "iteration 2982 loss 2.5022406578063965, acc 18.75\n",
      "iteration 2983 loss 2.506014585494995, acc 35.9375\n",
      "iteration 2984 loss 2.6675941944122314, acc 21.875\n",
      "iteration 2985 loss 2.653858184814453, acc 26.5625\n",
      "iteration 2986 loss 2.5628998279571533, acc 28.125\n",
      "iteration 2987 loss 2.6969127655029297, acc 21.875\n",
      "iteration 2988 loss 2.650866985321045, acc 20.3125\n",
      "iteration 2989 loss 2.8588290214538574, acc 15.625\n",
      "iteration 2990 loss 2.815528392791748, acc 20.3125\n",
      "iteration 2991 loss 2.8261687755584717, acc 26.5625\n",
      "iteration 2992 loss 2.847437858581543, acc 12.5\n",
      "iteration 2993 loss 2.551683187484741, acc 26.5625\n",
      "iteration 2994 loss 2.70843768119812, acc 25.0\n",
      "iteration 2995 loss 2.472827911376953, acc 29.6875\n",
      "iteration 2996 loss 2.7073168754577637, acc 23.4375\n",
      "iteration 2997 loss 2.78216290473938, acc 17.1875\n",
      "iteration 2998 loss 2.6017682552337646, acc 15.625\n",
      "iteration 2999 loss 2.7753283977508545, acc 23.4375\n",
      "iteration 3000 loss 2.6409881114959717, acc 23.4375\n",
      "iteration 3001 loss 2.627408027648926, acc 20.3125\n",
      "iteration 3002 loss 2.53517484664917, acc 32.8125\n",
      "iteration 3003 loss 2.4264211654663086, acc 26.5625\n",
      "iteration 3004 loss 2.6796486377716064, acc 23.4375\n",
      "iteration 3005 loss 2.7330050468444824, acc 17.1875\n",
      "iteration 3006 loss 2.774076461791992, acc 20.3125\n",
      "iteration 3007 loss 2.7133755683898926, acc 20.3125\n",
      "iteration 3008 loss 2.583714723587036, acc 21.875\n",
      "iteration 3009 loss 2.5683376789093018, acc 17.1875\n",
      "iteration 3010 loss 2.664926052093506, acc 21.875\n",
      "iteration 3011 loss 2.690373420715332, acc 23.4375\n",
      "iteration 3012 loss 2.6310176849365234, acc 17.1875\n",
      "iteration 3013 loss 2.6024720668792725, acc 23.4375\n",
      "iteration 3014 loss 2.6366755962371826, acc 26.5625\n",
      "iteration 3015 loss 2.9492530822753906, acc 12.5\n",
      "iteration 3016 loss 2.587801218032837, acc 29.6875\n",
      "iteration 3017 loss 2.6777117252349854, acc 15.625\n",
      "iteration 3018 loss 3.025972366333008, acc 12.5\n",
      "iteration 3019 loss 2.6267077922821045, acc 28.125\n",
      "iteration 3020 loss 2.7796785831451416, acc 15.625\n",
      "iteration 3021 loss 2.783846616744995, acc 17.1875\n",
      "iteration 3022 loss 2.7501869201660156, acc 17.1875\n",
      "iteration 3023 loss 2.6308231353759766, acc 20.3125\n",
      "iteration 3024 loss 2.8346710205078125, acc 18.75\n",
      "iteration 3025 loss 2.6241674423217773, acc 20.3125\n",
      "iteration 3026 loss 2.655014753341675, acc 18.75\n",
      "iteration 3027 loss 2.979179859161377, acc 23.4375\n",
      "iteration 3028 loss 2.782132387161255, acc 15.625\n",
      "iteration 3029 loss 2.679037570953369, acc 18.75\n",
      "iteration 3030 loss 2.756521701812744, acc 18.75\n",
      "iteration 3031 loss 2.5420379638671875, acc 20.3125\n",
      "iteration 3032 loss 2.7336316108703613, acc 6.25\n",
      "iteration 3033 loss 2.737473726272583, acc 25.0\n",
      "iteration 3034 loss 2.4489152431488037, acc 28.125\n",
      "iteration 3035 loss 2.8653249740600586, acc 17.1875\n",
      "iteration 3036 loss 2.7428345680236816, acc 18.75\n",
      "iteration 3037 loss 2.5372602939605713, acc 28.125\n",
      "iteration 3038 loss 2.7409610748291016, acc 23.4375\n",
      "iteration 3039 loss 2.690732479095459, acc 21.875\n",
      "iteration 3040 loss 2.7287650108337402, acc 14.0625\n",
      "iteration 3041 loss 2.8004391193389893, acc 17.1875\n",
      "iteration 3042 loss 2.6931445598602295, acc 18.75\n",
      "iteration 3043 loss 2.885920286178589, acc 15.625\n",
      "iteration 3044 loss 2.6590895652770996, acc 20.3125\n",
      "iteration 3045 loss 2.813748598098755, acc 18.75\n",
      "iteration 3046 loss 2.6441471576690674, acc 14.0625\n",
      "iteration 3047 loss 2.5457489490509033, acc 21.875\n",
      "iteration 3048 loss 2.744534492492676, acc 23.4375\n",
      "iteration 3049 loss 2.6345534324645996, acc 21.875\n",
      "iteration 3050 loss 2.771164894104004, acc 25.0\n",
      "iteration 3051 loss 2.829725980758667, acc 20.3125\n",
      "iteration 3052 loss 2.730142831802368, acc 23.4375\n",
      "iteration 3053 loss 2.748969793319702, acc 23.4375\n",
      "iteration 3054 loss 2.586082935333252, acc 21.875\n",
      "iteration 3055 loss 2.5873236656188965, acc 21.875\n",
      "iteration 3056 loss 2.7245123386383057, acc 17.1875\n",
      "iteration 3057 loss 2.651189088821411, acc 17.1875\n",
      "iteration 3058 loss 2.4878437519073486, acc 21.875\n",
      "iteration 3059 loss 2.7093873023986816, acc 18.75\n",
      "iteration 3060 loss 2.679788827896118, acc 29.6875\n",
      "iteration 3061 loss 2.7472386360168457, acc 15.625\n",
      "iteration 3062 loss 2.7177748680114746, acc 15.625\n",
      "iteration 3063 loss 2.7948992252349854, acc 20.3125\n",
      "iteration 3064 loss 2.734124183654785, acc 20.3125\n",
      "iteration 3065 loss 2.5730960369110107, acc 26.5625\n",
      "iteration 3066 loss 2.786928653717041, acc 20.3125\n",
      "iteration 3067 loss 2.5217649936676025, acc 23.4375\n",
      "iteration 3068 loss 2.782711982727051, acc 25.0\n",
      "iteration 3069 loss 2.569906711578369, acc 28.125\n",
      "iteration 3070 loss 2.710414171218872, acc 15.625\n",
      "iteration 3071 loss 2.894101142883301, acc 20.3125\n",
      "iteration 3072 loss 2.765082836151123, acc 25.0\n",
      "iteration 3073 loss 2.5014994144439697, acc 28.125\n",
      "iteration 3074 loss 2.6227357387542725, acc 18.75\n",
      "iteration 3075 loss 2.933805465698242, acc 14.0625\n",
      "iteration 3076 loss 2.7814712524414062, acc 21.875\n",
      "iteration 3077 loss 2.6512060165405273, acc 25.0\n",
      "iteration 3078 loss 3.0232245922088623, acc 12.5\n",
      "iteration 3079 loss 2.5994842052459717, acc 34.375\n",
      "iteration 3080 loss 2.7642486095428467, acc 21.875\n",
      "iteration 3081 loss 2.8489813804626465, acc 20.3125\n",
      "iteration 3082 loss 2.6887354850769043, acc 18.75\n",
      "iteration 3083 loss 2.5585711002349854, acc 23.4375\n",
      "iteration 3084 loss 2.782336711883545, acc 15.625\n",
      "iteration 3085 loss 2.730020523071289, acc 18.75\n",
      "iteration 3086 loss 2.763676643371582, acc 20.3125\n",
      "iteration 3087 loss 2.6938655376434326, acc 23.4375\n",
      "iteration 3088 loss 2.757546901702881, acc 14.0625\n",
      "iteration 3089 loss 2.7319633960723877, acc 28.125\n",
      "iteration 3090 loss 2.6816039085388184, acc 17.1875\n",
      "iteration 3091 loss 2.6105592250823975, acc 23.4375\n",
      "iteration 3092 loss 2.612276077270508, acc 20.3125\n",
      "iteration 3093 loss 2.7872605323791504, acc 15.625\n",
      "iteration 3094 loss 2.864264488220215, acc 17.1875\n",
      "iteration 3095 loss 2.8357956409454346, acc 26.5625\n",
      "iteration 3096 loss 2.5831074714660645, acc 23.4375\n",
      "iteration 3097 loss 2.9151015281677246, acc 6.25\n",
      "iteration 3098 loss 2.7819113731384277, acc 17.1875\n",
      "iteration 3099 loss 2.748703718185425, acc 9.375\n",
      "iteration 3100 loss 2.521726608276367, acc 18.75\n",
      "iteration 3101 loss 2.90179181098938, acc 15.625\n",
      "iteration 3102 loss 2.735877513885498, acc 20.3125\n",
      "iteration 3103 loss 2.5181496143341064, acc 31.25\n",
      "iteration 3104 loss 2.7941079139709473, acc 17.1875\n",
      "iteration 3105 loss 2.824669599533081, acc 18.75\n",
      "iteration 3106 loss 2.469700336456299, acc 34.375\n",
      "iteration 3107 loss 2.986520528793335, acc 17.1875\n",
      "iteration 3108 loss 2.7924582958221436, acc 12.5\n",
      "iteration 3109 loss 2.821702718734741, acc 17.1875\n",
      "iteration 3110 loss 2.617081642150879, acc 25.0\n",
      "iteration 3111 loss 2.6839840412139893, acc 28.125\n",
      "iteration 3112 loss 2.4323370456695557, acc 25.0\n",
      "iteration 3113 loss 2.7679643630981445, acc 18.75\n",
      "iteration 3114 loss 2.6521639823913574, acc 25.0\n",
      "iteration 3115 loss 2.9642553329467773, acc 20.3125\n",
      "iteration 3116 loss 2.6572365760803223, acc 25.0\n",
      "iteration 3117 loss 2.7882564067840576, acc 15.625\n",
      "iteration 3118 loss 2.6473147869110107, acc 34.375\n",
      "iteration 3119 loss 2.6478641033172607, acc 26.5625\n",
      "iteration 3120 loss 2.6823554039001465, acc 26.5625\n",
      "iteration 3121 loss 2.8074300289154053, acc 18.75\n",
      "iteration 3122 loss 2.548828601837158, acc 26.5625\n",
      "iteration 3123 loss 2.6642420291900635, acc 21.875\n",
      "iteration 3124 loss 2.603924512863159, acc 21.875\n",
      "iteration 3125 loss 2.714569568634033, acc 21.875\n",
      "iteration 3126 loss 2.805826425552368, acc 26.5625\n",
      "iteration 3127 loss 2.7497527599334717, acc 20.3125\n",
      "iteration 3128 loss 2.7644598484039307, acc 18.75\n",
      "iteration 3129 loss 2.7249655723571777, acc 25.0\n",
      "iteration 3130 loss 2.7917540073394775, acc 21.875\n",
      "iteration 3131 loss 2.6451609134674072, acc 20.3125\n",
      "iteration 3132 loss 2.63143253326416, acc 26.5625\n",
      "iteration 3133 loss 2.7167937755584717, acc 21.875\n",
      "iteration 3134 loss 2.59433650970459, acc 23.4375\n",
      "iteration 3135 loss 2.5602214336395264, acc 25.0\n",
      "iteration 3136 loss 2.753699779510498, acc 23.4375\n",
      "iteration 3137 loss 2.66304874420166, acc 28.125\n",
      "iteration 3138 loss 2.629348039627075, acc 21.875\n",
      "iteration 3139 loss 2.8991315364837646, acc 14.0625\n",
      "iteration 3140 loss 2.8315141201019287, acc 12.5\n",
      "iteration 3141 loss 2.7209157943725586, acc 20.3125\n",
      "iteration 3142 loss 2.594564199447632, acc 23.4375\n",
      "iteration 3143 loss 2.6790130138397217, acc 23.4375\n",
      "iteration 3144 loss 2.595618486404419, acc 20.3125\n",
      "iteration 3145 loss 2.5884933471679688, acc 32.8125\n",
      "iteration 3146 loss 2.5971901416778564, acc 29.6875\n",
      "iteration 3147 loss 2.6978585720062256, acc 23.4375\n",
      "iteration 3148 loss 2.8558616638183594, acc 18.75\n",
      "iteration 3149 loss 2.6033694744110107, acc 23.4375\n",
      "iteration 3150 loss 2.734724998474121, acc 18.75\n",
      "iteration 3151 loss 2.7178795337677, acc 25.0\n",
      "iteration 3152 loss 2.591461181640625, acc 26.5625\n",
      "iteration 3153 loss 2.6051061153411865, acc 26.5625\n",
      "iteration 3154 loss 2.54050350189209, acc 25.0\n",
      "iteration 3155 loss 2.8066751956939697, acc 12.5\n",
      "iteration 3156 loss 2.6050057411193848, acc 25.0\n",
      "iteration 3157 loss 2.8046841621398926, acc 14.0625\n",
      "iteration 3158 loss 2.6838200092315674, acc 17.1875\n",
      "iteration 3159 loss 2.922395706176758, acc 17.1875\n",
      "iteration 3160 loss 2.5239014625549316, acc 28.125\n",
      "iteration 3161 loss 2.734872817993164, acc 23.4375\n",
      "iteration 3162 loss 2.771711826324463, acc 23.4375\n",
      "iteration 3163 loss 2.873009443283081, acc 14.0625\n",
      "iteration 3164 loss 2.560187816619873, acc 25.0\n",
      "iteration 3165 loss 3.0247066020965576, acc 7.8125\n",
      "iteration 3166 loss 2.6199452877044678, acc 25.0\n",
      "iteration 3167 loss 2.5974934101104736, acc 23.4375\n",
      "iteration 3168 loss 2.797926902770996, acc 21.875\n",
      "iteration 3169 loss 2.6141810417175293, acc 25.0\n",
      "iteration 3170 loss 2.609665870666504, acc 23.4375\n",
      "iteration 3171 loss 2.904541254043579, acc 17.1875\n",
      "iteration 3172 loss 2.855642795562744, acc 17.1875\n",
      "iteration 3173 loss 2.7069151401519775, acc 23.4375\n",
      "iteration 3174 loss 2.6282052993774414, acc 28.125\n",
      "iteration 3175 loss 2.782951831817627, acc 20.3125\n",
      "iteration 3176 loss 2.7708497047424316, acc 17.1875\n",
      "iteration 3177 loss 2.79665207862854, acc 12.5\n",
      "iteration 3178 loss 2.7484118938446045, acc 18.75\n",
      "iteration 3179 loss 2.6168417930603027, acc 20.3125\n",
      "iteration 3180 loss 2.7093822956085205, acc 20.3125\n",
      "iteration 3181 loss 2.5924298763275146, acc 25.0\n",
      "iteration 3182 loss 2.6774942874908447, acc 20.3125\n",
      "iteration 3183 loss 2.699718713760376, acc 18.75\n",
      "iteration 3184 loss 2.8139302730560303, acc 20.3125\n",
      "iteration 3185 loss 2.8382954597473145, acc 20.3125\n",
      "iteration 3186 loss 2.711793899536133, acc 20.3125\n",
      "iteration 3187 loss 2.6664938926696777, acc 17.1875\n",
      "iteration 3188 loss 2.695863962173462, acc 25.0\n",
      "iteration 3189 loss 2.771571159362793, acc 23.4375\n",
      "iteration 3190 loss 2.634685754776001, acc 21.875\n",
      "iteration 3191 loss 2.548192024230957, acc 18.75\n",
      "iteration 3192 loss 2.7696621417999268, acc 17.1875\n",
      "iteration 3193 loss 2.8048644065856934, acc 23.4375\n",
      "iteration 3194 loss 2.6826021671295166, acc 15.625\n",
      "iteration 3195 loss 2.888627290725708, acc 17.1875\n",
      "iteration 3196 loss 2.5822014808654785, acc 23.4375\n",
      "iteration 3197 loss 2.6786017417907715, acc 26.5625\n",
      "iteration 3198 loss 2.71244740486145, acc 26.5625\n",
      "iteration 3199 loss 2.744145631790161, acc 14.0625\n",
      "iteration 3200 loss 2.7514684200286865, acc 10.9375\n",
      "iteration 3201 loss 2.5811376571655273, acc 25.0\n",
      "iteration 3202 loss 2.6395466327667236, acc 25.0\n",
      "iteration 3203 loss 2.601651668548584, acc 23.4375\n",
      "iteration 3204 loss 2.618556499481201, acc 26.5625\n",
      "iteration 3205 loss 2.6437246799468994, acc 21.875\n",
      "iteration 3206 loss 2.6667025089263916, acc 26.5625\n",
      "iteration 3207 loss 2.6012766361236572, acc 23.4375\n",
      "iteration 3208 loss 2.443342924118042, acc 31.25\n",
      "iteration 3209 loss 2.679711103439331, acc 29.6875\n",
      "iteration 3210 loss 2.5060014724731445, acc 26.5625\n",
      "iteration 3211 loss 2.886690616607666, acc 10.9375\n",
      "iteration 3212 loss 2.732130765914917, acc 21.875\n",
      "iteration 3213 loss 2.55826997756958, acc 25.0\n",
      "iteration 3214 loss 2.7517459392547607, acc 23.4375\n",
      "iteration 3215 loss 2.7441956996917725, acc 17.1875\n",
      "iteration 3216 loss 2.7222423553466797, acc 18.75\n",
      "iteration 3217 loss 2.4174418449401855, acc 32.8125\n",
      "iteration 3218 loss 2.726463794708252, acc 25.0\n",
      "iteration 3219 loss 2.546511173248291, acc 26.5625\n",
      "iteration 3220 loss 2.8194503784179688, acc 14.0625\n",
      "iteration 3221 loss 2.7010035514831543, acc 23.4375\n",
      "iteration 3222 loss 2.6660780906677246, acc 21.875\n",
      "iteration 3223 loss 2.6592977046966553, acc 17.1875\n",
      "iteration 3224 loss 2.4879817962646484, acc 23.4375\n",
      "iteration 3225 loss 2.9027669429779053, acc 20.3125\n",
      "iteration 3226 loss 2.865041494369507, acc 17.1875\n",
      "iteration 3227 loss 2.5752060413360596, acc 31.25\n",
      "iteration 3228 loss 2.784522771835327, acc 17.1875\n",
      "iteration 3229 loss 2.7240662574768066, acc 26.5625\n",
      "iteration 3230 loss 2.8331985473632812, acc 15.625\n",
      "iteration 3231 loss 2.747164726257324, acc 28.125\n",
      "iteration 3232 loss 2.711355209350586, acc 23.4375\n",
      "iteration 3233 loss 2.6287431716918945, acc 31.25\n",
      "iteration 3234 loss 2.583735227584839, acc 28.125\n",
      "iteration 3235 loss 2.5589821338653564, acc 28.125\n",
      "iteration 3236 loss 2.723386764526367, acc 18.75\n",
      "iteration 3237 loss 2.9709150791168213, acc 14.0625\n",
      "iteration 3238 loss 2.7179830074310303, acc 26.5625\n",
      "iteration 3239 loss 2.5857253074645996, acc 26.5625\n",
      "iteration 3240 loss 2.8718461990356445, acc 14.0625\n",
      "iteration 3241 loss 2.7342770099639893, acc 21.875\n",
      "iteration 3242 loss 2.5363948345184326, acc 23.4375\n",
      "iteration 3243 loss 2.5536913871765137, acc 18.75\n",
      "iteration 3244 loss 2.717040538787842, acc 18.75\n",
      "iteration 3245 loss 2.817091226577759, acc 25.0\n",
      "iteration 3246 loss 3.0270326137542725, acc 12.5\n",
      "iteration 3247 loss 2.5213587284088135, acc 26.5625\n",
      "iteration 3248 loss 2.8358232975006104, acc 18.75\n",
      "iteration 3249 loss 2.6290035247802734, acc 18.75\n",
      "iteration 3250 loss 2.719987154006958, acc 21.875\n",
      "iteration 3251 loss 2.7254819869995117, acc 23.4375\n",
      "iteration 3252 loss 2.431708574295044, acc 28.125\n",
      "iteration 3253 loss 2.648898124694824, acc 28.125\n",
      "iteration 3254 loss 2.745954751968384, acc 18.75\n",
      "iteration 3255 loss 2.6170952320098877, acc 26.5625\n",
      "iteration 3256 loss 2.5477094650268555, acc 25.0\n",
      "iteration 3257 loss 2.903172731399536, acc 17.1875\n",
      "iteration 3258 loss 2.8048291206359863, acc 23.4375\n",
      "iteration 3259 loss 2.683893918991089, acc 15.625\n",
      "iteration 3260 loss 2.587063789367676, acc 17.1875\n",
      "iteration 3261 loss 2.758584976196289, acc 18.75\n",
      "iteration 3262 loss 2.6807844638824463, acc 25.0\n",
      "iteration 3263 loss 2.811070442199707, acc 31.25\n",
      "iteration 3264 loss 2.5564985275268555, acc 23.4375\n",
      "iteration 3265 loss 2.7867064476013184, acc 23.4375\n",
      "iteration 3266 loss 2.6030352115631104, acc 29.6875\n",
      "iteration 3267 loss 2.759676218032837, acc 20.3125\n",
      "iteration 3268 loss 2.642383575439453, acc 23.4375\n",
      "iteration 3269 loss 2.6254355907440186, acc 15.625\n",
      "iteration 3270 loss 2.718104839324951, acc 21.875\n",
      "iteration 3271 loss 2.6653835773468018, acc 28.125\n",
      "iteration 3272 loss 2.596583127975464, acc 18.75\n",
      "iteration 3273 loss 2.8807711601257324, acc 18.75\n",
      "iteration 3274 loss 2.8217456340789795, acc 15.625\n",
      "iteration 3275 loss 2.6136791706085205, acc 20.3125\n",
      "iteration 3276 loss 2.732825994491577, acc 25.0\n",
      "iteration 3277 loss 2.592542886734009, acc 20.3125\n",
      "iteration 3278 loss 2.6036860942840576, acc 25.0\n",
      "iteration 3279 loss 2.9029152393341064, acc 23.4375\n",
      "iteration 3280 loss 3.0034780502319336, acc 10.9375\n",
      "iteration 3281 loss 2.6149606704711914, acc 21.875\n",
      "iteration 3282 loss 2.6244351863861084, acc 31.25\n",
      "iteration 3283 loss 2.834019422531128, acc 20.3125\n",
      "iteration 3284 loss 2.764145612716675, acc 17.1875\n",
      "iteration 3285 loss 2.659325361251831, acc 21.875\n",
      "iteration 3286 loss 2.524461269378662, acc 26.5625\n",
      "iteration 3287 loss 2.573854923248291, acc 25.0\n",
      "iteration 3288 loss 2.7342336177825928, acc 25.0\n",
      "iteration 3289 loss 2.68135404586792, acc 25.0\n",
      "iteration 3290 loss 2.6340248584747314, acc 18.75\n",
      "iteration 3291 loss 2.5535993576049805, acc 18.75\n",
      "iteration 3292 loss 2.7174136638641357, acc 17.1875\n",
      "iteration 3293 loss 2.696444272994995, acc 23.4375\n",
      "iteration 3294 loss 2.7665343284606934, acc 18.75\n",
      "iteration 3295 loss 2.7885193824768066, acc 23.4375\n",
      "iteration 3296 loss 2.4821009635925293, acc 28.125\n",
      "iteration 3297 loss 2.859464645385742, acc 12.5\n",
      "iteration 3298 loss 2.758683443069458, acc 21.875\n",
      "iteration 3299 loss 2.8281257152557373, acc 18.75\n",
      "iteration 3300 loss 2.5359644889831543, acc 26.5625\n",
      "iteration 3301 loss 2.774998903274536, acc 18.75\n",
      "iteration 3302 loss 2.678426504135132, acc 23.4375\n",
      "iteration 3303 loss 2.447502374649048, acc 32.8125\n",
      "iteration 3304 loss 2.753446578979492, acc 25.0\n",
      "iteration 3305 loss 2.644754648208618, acc 29.6875\n",
      "iteration 3306 loss 2.695687770843506, acc 26.5625\n",
      "iteration 3307 loss 2.9296064376831055, acc 14.0625\n",
      "iteration 3308 loss 2.585390329360962, acc 25.0\n",
      "iteration 3309 loss 2.911432981491089, acc 14.0625\n",
      "iteration 3310 loss 2.629488945007324, acc 21.875\n",
      "iteration 3311 loss 2.922450542449951, acc 20.3125\n",
      "iteration 3312 loss 2.6724865436553955, acc 23.4375\n",
      "iteration 3313 loss 2.628730058670044, acc 15.625\n",
      "iteration 3314 loss 2.610593557357788, acc 18.75\n",
      "iteration 3315 loss 2.656649589538574, acc 18.75\n",
      "iteration 3316 loss 2.747865915298462, acc 20.3125\n",
      "iteration 3317 loss 2.72928524017334, acc 28.125\n",
      "iteration 3318 loss 2.6755475997924805, acc 21.875\n",
      "iteration 3319 loss 2.764209270477295, acc 12.5\n",
      "iteration 3320 loss 2.652759313583374, acc 15.625\n",
      "iteration 3321 loss 2.630136251449585, acc 21.875\n",
      "iteration 3322 loss 2.8043768405914307, acc 21.875\n",
      "iteration 3323 loss 2.6200902462005615, acc 20.3125\n",
      "iteration 3324 loss 2.7479262351989746, acc 20.3125\n",
      "iteration 3325 loss 2.6866507530212402, acc 18.75\n",
      "iteration 3326 loss 2.9736766815185547, acc 18.75\n",
      "iteration 3327 loss 2.779154062271118, acc 20.3125\n",
      "iteration 3328 loss 2.6031289100646973, acc 26.5625\n",
      "iteration 3329 loss 2.778158187866211, acc 18.75\n",
      "iteration 3330 loss 2.7481329441070557, acc 20.3125\n",
      "iteration 3331 loss 2.522287130355835, acc 31.25\n",
      "iteration 3332 loss 2.763472557067871, acc 25.0\n",
      "iteration 3333 loss 2.640998601913452, acc 21.875\n",
      "iteration 3334 loss 2.6077375411987305, acc 17.1875\n",
      "iteration 3335 loss 2.6483540534973145, acc 25.0\n",
      "iteration 3336 loss 2.7873220443725586, acc 15.625\n",
      "iteration 3337 loss 2.6116275787353516, acc 23.4375\n",
      "iteration 3338 loss 2.788036823272705, acc 14.0625\n",
      "iteration 3339 loss 2.7493748664855957, acc 12.5\n",
      "iteration 3340 loss 2.6688435077667236, acc 20.3125\n",
      "iteration 3341 loss 2.5983781814575195, acc 26.5625\n",
      "iteration 3342 loss 2.420708179473877, acc 31.25\n",
      "iteration 3343 loss 2.721583604812622, acc 15.625\n",
      "iteration 3344 loss 2.7756881713867188, acc 20.3125\n",
      "iteration 3345 loss 2.544069528579712, acc 26.5625\n",
      "iteration 3346 loss 2.631051540374756, acc 23.4375\n",
      "iteration 3347 loss 2.666524648666382, acc 26.5625\n",
      "iteration 3348 loss 2.308288335800171, acc 37.5\n",
      "iteration 3349 loss 2.8000385761260986, acc 20.3125\n",
      "iteration 3350 loss 2.664731502532959, acc 25.0\n",
      "iteration 3351 loss 2.6467599868774414, acc 25.0\n",
      "iteration 3352 loss 2.6598961353302, acc 31.25\n",
      "iteration 3353 loss 2.7338860034942627, acc 20.3125\n",
      "iteration 3354 loss 2.436734676361084, acc 29.6875\n",
      "iteration 3355 loss 2.651905059814453, acc 23.4375\n",
      "iteration 3356 loss 2.6780476570129395, acc 25.0\n",
      "iteration 3357 loss 2.8017168045043945, acc 15.625\n",
      "iteration 3358 loss 2.92042875289917, acc 18.75\n",
      "iteration 3359 loss 2.478358268737793, acc 32.8125\n",
      "iteration 3360 loss 2.709238052368164, acc 21.875\n",
      "iteration 3361 loss 2.7218596935272217, acc 20.3125\n",
      "iteration 3362 loss 2.729166030883789, acc 18.75\n",
      "iteration 3363 loss 2.555572748184204, acc 26.5625\n",
      "iteration 3364 loss 2.7484843730926514, acc 18.75\n",
      "iteration 3365 loss 2.7272183895111084, acc 21.875\n",
      "iteration 3366 loss 2.854576826095581, acc 14.0625\n",
      "iteration 3367 loss 2.743661880493164, acc 17.1875\n",
      "iteration 3368 loss 2.7244670391082764, acc 14.0625\n",
      "iteration 3369 loss 2.788463592529297, acc 14.0625\n",
      "iteration 3370 loss 2.704416513442993, acc 18.75\n",
      "iteration 3371 loss 2.520228862762451, acc 28.125\n",
      "iteration 3372 loss 2.4840731620788574, acc 31.25\n",
      "iteration 3373 loss 2.669161319732666, acc 20.3125\n",
      "iteration 3374 loss 2.652055025100708, acc 28.125\n",
      "iteration 3375 loss 2.836484670639038, acc 17.1875\n",
      "iteration 3376 loss 2.735502243041992, acc 21.875\n",
      "iteration 3377 loss 2.9032957553863525, acc 21.875\n",
      "iteration 3378 loss 2.7605838775634766, acc 18.75\n",
      "iteration 3379 loss 2.648228168487549, acc 23.4375\n",
      "iteration 3380 loss 2.496330499649048, acc 26.5625\n",
      "iteration 3381 loss 2.6152031421661377, acc 15.625\n",
      "iteration 3382 loss 2.8066928386688232, acc 15.625\n",
      "iteration 3383 loss 2.8715193271636963, acc 17.1875\n",
      "iteration 3384 loss 2.803008556365967, acc 17.1875\n",
      "iteration 3385 loss 2.810532331466675, acc 20.3125\n",
      "iteration 3386 loss 2.8851592540740967, acc 21.875\n",
      "iteration 3387 loss 2.7080187797546387, acc 23.4375\n",
      "iteration 3388 loss 2.766030788421631, acc 20.3125\n",
      "iteration 3389 loss 3.124368190765381, acc 12.5\n",
      "iteration 3390 loss 2.5666463375091553, acc 17.1875\n",
      "iteration 3391 loss 2.6426267623901367, acc 21.875\n",
      "iteration 3392 loss 2.7451183795928955, acc 20.3125\n",
      "iteration 3393 loss 2.804161310195923, acc 17.1875\n",
      "iteration 3394 loss 2.478221893310547, acc 28.125\n",
      "iteration 3395 loss 2.747615337371826, acc 17.1875\n",
      "iteration 3396 loss 2.6115353107452393, acc 26.5625\n",
      "iteration 3397 loss 2.677088499069214, acc 26.5625\n",
      "iteration 3398 loss 2.762329578399658, acc 20.3125\n",
      "iteration 3399 loss 2.7518272399902344, acc 21.875\n",
      "iteration 3400 loss 2.7087931632995605, acc 20.3125\n",
      "iteration 3401 loss 2.5038564205169678, acc 31.25\n",
      "iteration 3402 loss 2.5582451820373535, acc 26.5625\n",
      "iteration 3403 loss 2.696547508239746, acc 18.75\n",
      "iteration 3404 loss 2.6098363399505615, acc 18.75\n",
      "iteration 3405 loss 2.576141119003296, acc 29.6875\n",
      "iteration 3406 loss 2.766428232192993, acc 15.625\n",
      "iteration 3407 loss 2.4985287189483643, acc 26.5625\n",
      "iteration 3408 loss 2.8665268421173096, acc 17.1875\n",
      "iteration 3409 loss 2.7492568492889404, acc 20.3125\n",
      "iteration 3410 loss 2.4641520977020264, acc 28.125\n",
      "iteration 3411 loss 2.8072798252105713, acc 17.1875\n",
      "iteration 3412 loss 2.85799503326416, acc 18.75\n",
      "iteration 3413 loss 2.5797441005706787, acc 20.3125\n",
      "iteration 3414 loss 2.558724880218506, acc 21.875\n",
      "iteration 3415 loss 2.839179515838623, acc 17.1875\n",
      "iteration 3416 loss 2.6996397972106934, acc 17.1875\n",
      "iteration 3417 loss 2.611443042755127, acc 21.875\n",
      "iteration 3418 loss 2.8159031867980957, acc 20.3125\n",
      "iteration 3419 loss 2.6079506874084473, acc 25.0\n",
      "iteration 3420 loss 2.6739752292633057, acc 23.4375\n",
      "iteration 3421 loss 2.637293577194214, acc 23.4375\n",
      "iteration 3422 loss 2.7228541374206543, acc 17.1875\n",
      "iteration 3423 loss 2.7836201190948486, acc 23.4375\n",
      "iteration 3424 loss 3.0068225860595703, acc 10.9375\n",
      "iteration 3425 loss 2.9018800258636475, acc 18.75\n",
      "iteration 3426 loss 2.679380416870117, acc 17.1875\n",
      "iteration 3427 loss 2.6238186359405518, acc 25.0\n",
      "iteration 3428 loss 2.5717241764068604, acc 29.6875\n",
      "iteration 3429 loss 2.5161173343658447, acc 26.5625\n",
      "iteration 3430 loss 2.556854486465454, acc 25.0\n",
      "iteration 3431 loss 2.725193500518799, acc 26.5625\n",
      "iteration 3432 loss 2.6338179111480713, acc 26.5625\n",
      "iteration 3433 loss 2.9761602878570557, acc 14.0625\n",
      "iteration 3434 loss 2.783912181854248, acc 15.625\n",
      "iteration 3435 loss 2.724613666534424, acc 21.875\n",
      "iteration 3436 loss 2.685892343521118, acc 20.3125\n",
      "iteration 3437 loss 2.789616584777832, acc 18.75\n",
      "iteration 3438 loss 2.7098119258880615, acc 20.3125\n",
      "iteration 3439 loss 2.8209688663482666, acc 17.1875\n",
      "iteration 3440 loss 2.8114960193634033, acc 17.1875\n",
      "iteration 3441 loss 2.8136508464813232, acc 17.1875\n",
      "iteration 3442 loss 2.71673321723938, acc 18.75\n",
      "iteration 3443 loss 2.8023626804351807, acc 18.75\n",
      "iteration 3444 loss 2.551419734954834, acc 25.0\n",
      "iteration 3445 loss 2.8817272186279297, acc 12.5\n",
      "iteration 3446 loss 2.729262590408325, acc 25.0\n",
      "iteration 3447 loss 2.7132585048675537, acc 10.9375\n",
      "iteration 3448 loss 2.8074944019317627, acc 9.375\n",
      "iteration 3449 loss 2.682279109954834, acc 25.0\n",
      "iteration 3450 loss 2.675816297531128, acc 10.9375\n",
      "iteration 3451 loss 2.7373785972595215, acc 15.625\n",
      "iteration 3452 loss 2.6354591846466064, acc 14.0625\n",
      "iteration 3453 loss 2.718487024307251, acc 23.4375\n",
      "iteration 3454 loss 2.6067841053009033, acc 20.3125\n",
      "iteration 3455 loss 2.842068910598755, acc 17.1875\n",
      "iteration 3456 loss 2.716163396835327, acc 9.375\n",
      "iteration 3457 loss 2.9137699604034424, acc 15.625\n",
      "iteration 3458 loss 2.5411648750305176, acc 23.4375\n",
      "iteration 3459 loss 2.7715747356414795, acc 18.75\n",
      "iteration 3460 loss 2.675004482269287, acc 26.5625\n",
      "iteration 3461 loss 2.699019432067871, acc 25.0\n",
      "iteration 3462 loss 2.6933679580688477, acc 18.75\n",
      "iteration 3463 loss 2.629359483718872, acc 23.4375\n",
      "iteration 3464 loss 2.6514949798583984, acc 12.5\n",
      "iteration 3465 loss 2.703023672103882, acc 25.0\n",
      "iteration 3466 loss 2.793839693069458, acc 18.75\n",
      "iteration 3467 loss 2.6899237632751465, acc 25.0\n",
      "iteration 3468 loss 2.708578109741211, acc 25.0\n",
      "iteration 3469 loss 2.6549718379974365, acc 28.125\n",
      "iteration 3470 loss 2.6387336254119873, acc 17.1875\n",
      "iteration 3471 loss 2.731321096420288, acc 23.4375\n",
      "iteration 3472 loss 2.777561902999878, acc 18.75\n",
      "iteration 3473 loss 2.757211208343506, acc 21.875\n",
      "iteration 3474 loss 2.763606071472168, acc 26.5625\n",
      "iteration 3475 loss 2.54172420501709, acc 26.5625\n",
      "iteration 3476 loss 2.7624154090881348, acc 17.1875\n",
      "iteration 3477 loss 2.64750075340271, acc 28.125\n",
      "iteration 3478 loss 2.443429708480835, acc 34.375\n",
      "iteration 3479 loss 2.884509801864624, acc 23.4375\n",
      "iteration 3480 loss 2.6768858432769775, acc 23.4375\n",
      "iteration 3481 loss 2.489680051803589, acc 28.125\n",
      "iteration 3482 loss 2.594420909881592, acc 20.3125\n",
      "iteration 3483 loss 2.8249073028564453, acc 10.9375\n",
      "iteration 3484 loss 2.9313724040985107, acc 14.0625\n",
      "iteration 3485 loss 2.8110830783843994, acc 14.0625\n",
      "iteration 3486 loss 2.615614652633667, acc 28.125\n",
      "iteration 3487 loss 2.797244071960449, acc 18.75\n",
      "iteration 3488 loss 2.72164249420166, acc 25.0\n",
      "iteration 3489 loss 2.7248919010162354, acc 20.3125\n",
      "iteration 3490 loss 2.9145009517669678, acc 12.5\n",
      "iteration 3491 loss 2.6787264347076416, acc 18.75\n",
      "iteration 3492 loss 2.668813943862915, acc 25.0\n",
      "iteration 3493 loss 2.720989465713501, acc 18.75\n",
      "iteration 3494 loss 2.5440824031829834, acc 25.0\n",
      "iteration 3495 loss 2.650728702545166, acc 21.875\n",
      "iteration 3496 loss 2.7814807891845703, acc 15.625\n",
      "iteration 3497 loss 2.8160550594329834, acc 21.875\n",
      "iteration 3498 loss 2.5124828815460205, acc 28.125\n",
      "iteration 3499 loss 2.9267921447753906, acc 17.1875\n",
      "iteration 3500 loss 2.5342769622802734, acc 29.6875\n",
      "iteration 3501 loss 2.7095041275024414, acc 23.4375\n",
      "iteration 3502 loss 2.8327691555023193, acc 14.0625\n",
      "iteration 3503 loss 2.7097437381744385, acc 28.125\n",
      "iteration 3504 loss 2.612490177154541, acc 20.3125\n",
      "iteration 3505 loss 2.672527551651001, acc 17.1875\n",
      "iteration 3506 loss 2.7552294731140137, acc 15.625\n",
      "iteration 3507 loss 2.6268746852874756, acc 18.75\n",
      "iteration 3508 loss 2.531322956085205, acc 25.0\n",
      "iteration 3509 loss 2.8919663429260254, acc 17.1875\n",
      "iteration 3510 loss 2.6571950912475586, acc 18.75\n",
      "iteration 3511 loss 2.544398069381714, acc 18.75\n",
      "iteration 3512 loss 2.6055734157562256, acc 25.0\n",
      "iteration 3513 loss 2.7212870121002197, acc 14.0625\n",
      "iteration 3514 loss 2.7008912563323975, acc 20.3125\n",
      "iteration 3515 loss 2.6309328079223633, acc 25.0\n",
      "iteration 3516 loss 2.507631778717041, acc 31.25\n",
      "iteration 3517 loss 2.7126784324645996, acc 18.75\n",
      "iteration 3518 loss 2.7118124961853027, acc 12.5\n",
      "iteration 3519 loss 2.804027557373047, acc 18.75\n",
      "iteration 3520 loss 2.6981892585754395, acc 23.4375\n",
      "iteration 3521 loss 2.548325300216675, acc 29.6875\n",
      "iteration 3522 loss 2.575538396835327, acc 28.125\n",
      "iteration 3523 loss 2.4850289821624756, acc 28.125\n",
      "iteration 3524 loss 2.829227924346924, acc 12.5\n",
      "iteration 3525 loss 2.8746025562286377, acc 18.75\n",
      "iteration 3526 loss 2.8019516468048096, acc 17.1875\n",
      "iteration 3527 loss 2.8569416999816895, acc 20.3125\n",
      "iteration 3528 loss 2.7764008045196533, acc 17.1875\n",
      "iteration 3529 loss 2.8212339878082275, acc 18.75\n",
      "iteration 3530 loss 2.6129722595214844, acc 23.4375\n",
      "iteration 3531 loss 2.8305089473724365, acc 18.75\n",
      "iteration 3532 loss 2.8987648487091064, acc 9.375\n",
      "iteration 3533 loss 2.6383368968963623, acc 18.75\n",
      "iteration 3534 loss 2.624849319458008, acc 28.125\n",
      "iteration 3535 loss 2.62380313873291, acc 23.4375\n",
      "iteration 3536 loss 2.8833320140838623, acc 10.9375\n",
      "iteration 3537 loss 2.630859136581421, acc 18.75\n",
      "iteration 3538 loss 2.7799229621887207, acc 12.5\n",
      "iteration 3539 loss 2.8168511390686035, acc 23.4375\n",
      "iteration 3540 loss 2.414210557937622, acc 39.0625\n",
      "iteration 3541 loss 2.594959259033203, acc 26.5625\n",
      "iteration 3542 loss 2.8689136505126953, acc 23.4375\n",
      "iteration 3543 loss 2.865492343902588, acc 10.9375\n",
      "iteration 3544 loss 2.8383288383483887, acc 21.875\n",
      "iteration 3545 loss 2.742847204208374, acc 21.875\n",
      "iteration 3546 loss 2.9201717376708984, acc 14.0625\n",
      "iteration 3547 loss 2.508444309234619, acc 25.0\n",
      "iteration 3548 loss 2.6772165298461914, acc 23.4375\n",
      "iteration 3549 loss 2.668229579925537, acc 28.125\n",
      "iteration 3550 loss 2.6297736167907715, acc 21.875\n",
      "iteration 3551 loss 2.8123910427093506, acc 26.5625\n",
      "iteration 3552 loss 2.527031183242798, acc 25.0\n",
      "iteration 3553 loss 2.566497802734375, acc 32.8125\n",
      "iteration 3554 loss 2.8969638347625732, acc 14.0625\n",
      "iteration 3555 loss 2.992664337158203, acc 14.0625\n",
      "iteration 3556 loss 2.6376774311065674, acc 21.875\n",
      "iteration 3557 loss 2.5975656509399414, acc 25.0\n",
      "iteration 3558 loss 2.8025569915771484, acc 18.75\n",
      "iteration 3559 loss 2.724940776824951, acc 29.6875\n",
      "iteration 3560 loss 2.672316551208496, acc 20.3125\n",
      "iteration 3561 loss 2.7372524738311768, acc 15.625\n",
      "iteration 3562 loss 2.6916916370391846, acc 23.4375\n",
      "iteration 3563 loss 2.528275728225708, acc 28.125\n",
      "iteration 3564 loss 2.7504446506500244, acc 18.75\n",
      "iteration 3565 loss 2.6882293224334717, acc 23.4375\n",
      "iteration 3566 loss 2.7319071292877197, acc 17.1875\n",
      "iteration 3567 loss 2.790132522583008, acc 18.75\n",
      "iteration 3568 loss 2.628911256790161, acc 21.875\n",
      "iteration 3569 loss 2.9475107192993164, acc 9.375\n",
      "iteration 3570 loss 2.546562671661377, acc 25.0\n",
      "iteration 3571 loss 2.7923803329467773, acc 18.75\n",
      "iteration 3572 loss 2.5189499855041504, acc 21.875\n",
      "iteration 3573 loss 2.6530086994171143, acc 18.75\n",
      "iteration 3574 loss 2.4751861095428467, acc 40.625\n",
      "iteration 3575 loss 2.9375462532043457, acc 20.3125\n",
      "iteration 3576 loss 2.7583348751068115, acc 21.875\n",
      "iteration 3577 loss 2.84975528717041, acc 14.0625\n",
      "iteration 3578 loss 2.5440640449523926, acc 26.5625\n",
      "iteration 3579 loss 2.933316946029663, acc 12.5\n",
      "iteration 3580 loss 2.7950069904327393, acc 25.0\n",
      "iteration 3581 loss 2.9048550128936768, acc 21.875\n",
      "iteration 3582 loss 2.569883108139038, acc 26.5625\n",
      "iteration 3583 loss 2.708552598953247, acc 18.75\n",
      "iteration 3584 loss 2.6416871547698975, acc 20.3125\n",
      "iteration 3585 loss 2.728316307067871, acc 23.4375\n",
      "iteration 3586 loss 2.7561097145080566, acc 17.1875\n",
      "iteration 3587 loss 2.596261501312256, acc 25.0\n",
      "iteration 3588 loss 2.8007891178131104, acc 23.4375\n",
      "iteration 3589 loss 2.7872235774993896, acc 20.3125\n",
      "iteration 3590 loss 2.7840073108673096, acc 25.0\n",
      "iteration 3591 loss 2.848496913909912, acc 15.625\n",
      "iteration 3592 loss 2.6529390811920166, acc 23.4375\n",
      "iteration 3593 loss 2.7437851428985596, acc 28.125\n",
      "iteration 3594 loss 2.4831504821777344, acc 34.375\n",
      "iteration 3595 loss 2.563519239425659, acc 28.125\n",
      "iteration 3596 loss 2.8360135555267334, acc 20.3125\n",
      "iteration 3597 loss 2.812307834625244, acc 15.625\n",
      "iteration 3598 loss 2.5244758129119873, acc 20.3125\n",
      "iteration 3599 loss 2.6855359077453613, acc 28.125\n",
      "iteration 3600 loss 2.7721664905548096, acc 14.0625\n",
      "iteration 3601 loss 2.703918218612671, acc 21.875\n",
      "iteration 3602 loss 2.8416941165924072, acc 10.9375\n",
      "iteration 3603 loss 2.779644250869751, acc 7.8125\n",
      "iteration 3604 loss 2.631225109100342, acc 12.5\n",
      "iteration 3605 loss 2.7143890857696533, acc 26.5625\n",
      "iteration 3606 loss 2.748004913330078, acc 21.875\n",
      "iteration 3607 loss 2.693739891052246, acc 23.4375\n",
      "iteration 3608 loss 2.6285524368286133, acc 25.0\n",
      "iteration 3609 loss 2.9161183834075928, acc 15.625\n",
      "iteration 3610 loss 2.706530809402466, acc 20.3125\n",
      "iteration 3611 loss 2.506831169128418, acc 28.125\n",
      "iteration 3612 loss 2.8815665245056152, acc 18.75\n",
      "iteration 3613 loss 2.783036947250366, acc 23.4375\n",
      "iteration 3614 loss 2.836651086807251, acc 26.5625\n",
      "iteration 3615 loss 2.7523372173309326, acc 18.75\n",
      "iteration 3616 loss 2.766864061355591, acc 20.3125\n",
      "iteration 3617 loss 2.593179225921631, acc 28.125\n",
      "iteration 3618 loss 2.6971161365509033, acc 23.4375\n",
      "iteration 3619 loss 2.7888622283935547, acc 15.625\n",
      "iteration 3620 loss 2.7567741870880127, acc 18.75\n",
      "iteration 3621 loss 2.919548273086548, acc 17.1875\n",
      "iteration 3622 loss 2.6148366928100586, acc 23.4375\n",
      "iteration 3623 loss 2.6957366466522217, acc 26.5625\n",
      "iteration 3624 loss 2.592548370361328, acc 25.0\n",
      "iteration 3625 loss 2.7314958572387695, acc 23.4375\n",
      "iteration 3626 loss 2.8359107971191406, acc 18.75\n",
      "iteration 3627 loss 2.5814783573150635, acc 29.6875\n",
      "iteration 3628 loss 2.6286654472351074, acc 25.0\n",
      "iteration 3629 loss 2.6210250854492188, acc 23.4375\n",
      "iteration 3630 loss 2.76225209236145, acc 17.1875\n",
      "iteration 3631 loss 2.453490734100342, acc 31.25\n",
      "iteration 3632 loss 2.739171266555786, acc 23.4375\n",
      "iteration 3633 loss 2.7910525798797607, acc 18.75\n",
      "iteration 3634 loss 2.720968246459961, acc 21.875\n",
      "iteration 3635 loss 2.407578468322754, acc 29.6875\n",
      "iteration 3636 loss 2.6076371669769287, acc 25.0\n",
      "iteration 3637 loss 2.514037847518921, acc 25.0\n",
      "iteration 3638 loss 2.6495628356933594, acc 21.875\n",
      "iteration 3639 loss 2.590740203857422, acc 23.4375\n",
      "iteration 3640 loss 2.7158350944519043, acc 23.4375\n",
      "iteration 3641 loss 2.873413324356079, acc 20.3125\n",
      "iteration 3642 loss 2.815918207168579, acc 14.0625\n",
      "iteration 3643 loss 2.6957099437713623, acc 17.1875\n",
      "iteration 3644 loss 2.7257227897644043, acc 12.5\n",
      "iteration 3645 loss 2.7151708602905273, acc 17.1875\n",
      "iteration 3646 loss 2.6157188415527344, acc 21.875\n",
      "iteration 3647 loss 2.6465632915496826, acc 26.5625\n",
      "iteration 3648 loss 2.695697784423828, acc 20.3125\n",
      "iteration 3649 loss 2.8633031845092773, acc 15.625\n",
      "iteration 3650 loss 2.6064112186431885, acc 15.625\n",
      "iteration 3651 loss 2.741898536682129, acc 18.75\n",
      "iteration 3652 loss 2.753969669342041, acc 18.75\n",
      "iteration 3653 loss 2.6337335109710693, acc 15.625\n",
      "iteration 3654 loss 2.620619058609009, acc 21.875\n",
      "iteration 3655 loss 2.6757466793060303, acc 12.5\n",
      "iteration 3656 loss 2.7453320026397705, acc 21.875\n",
      "iteration 3657 loss 2.6086788177490234, acc 26.5625\n",
      "iteration 3658 loss 2.612440824508667, acc 23.4375\n",
      "iteration 3659 loss 2.762655258178711, acc 21.875\n",
      "iteration 3660 loss 2.9115052223205566, acc 21.875\n",
      "iteration 3661 loss 2.4770314693450928, acc 32.8125\n",
      "iteration 3662 loss 2.669358491897583, acc 25.0\n",
      "iteration 3663 loss 2.7116427421569824, acc 23.4375\n",
      "iteration 3664 loss 2.6387643814086914, acc 20.3125\n",
      "iteration 3665 loss 2.7309231758117676, acc 26.5625\n",
      "iteration 3666 loss 2.5161967277526855, acc 21.875\n",
      "iteration 3667 loss 2.6373486518859863, acc 23.4375\n",
      "iteration 3668 loss 2.704737901687622, acc 18.75\n",
      "iteration 3669 loss 2.611783742904663, acc 26.5625\n",
      "iteration 3670 loss 2.3509228229522705, acc 34.375\n",
      "iteration 3671 loss 2.879432439804077, acc 23.4375\n",
      "iteration 3672 loss 2.7799904346466064, acc 23.4375\n",
      "iteration 3673 loss 2.6439337730407715, acc 21.875\n",
      "iteration 3674 loss 2.9545040130615234, acc 20.3125\n",
      "iteration 3675 loss 2.5468504428863525, acc 31.25\n",
      "iteration 3676 loss 2.6618435382843018, acc 17.1875\n",
      "iteration 3677 loss 2.776087999343872, acc 20.3125\n",
      "iteration 3678 loss 2.6776316165924072, acc 17.1875\n",
      "iteration 3679 loss 2.6126747131347656, acc 18.75\n",
      "iteration 3680 loss 2.7326672077178955, acc 20.3125\n",
      "iteration 3681 loss 2.8653316497802734, acc 6.25\n",
      "iteration 3682 loss 2.705763339996338, acc 20.3125\n",
      "iteration 3683 loss 2.91879940032959, acc 15.625\n",
      "iteration 3684 loss 2.7520382404327393, acc 20.3125\n",
      "iteration 3685 loss 2.644028425216675, acc 17.1875\n",
      "iteration 3686 loss 2.8372280597686768, acc 15.625\n",
      "iteration 3687 loss 2.6316978931427, acc 23.4375\n",
      "iteration 3688 loss 2.774635076522827, acc 15.625\n",
      "iteration 3689 loss 2.627941370010376, acc 17.1875\n",
      "iteration 3690 loss 2.6141164302825928, acc 23.4375\n",
      "iteration 3691 loss 2.699030876159668, acc 17.1875\n",
      "iteration 3692 loss 2.6913864612579346, acc 25.0\n",
      "iteration 3693 loss 2.778456926345825, acc 25.0\n",
      "iteration 3694 loss 2.7668685913085938, acc 18.75\n",
      "iteration 3695 loss 2.6807610988616943, acc 18.75\n",
      "iteration 3696 loss 2.6819419860839844, acc 21.875\n",
      "iteration 3697 loss 2.7299656867980957, acc 21.875\n",
      "iteration 3698 loss 3.062140941619873, acc 17.1875\n",
      "iteration 3699 loss 2.7411465644836426, acc 20.3125\n",
      "iteration 3700 loss 2.5520122051239014, acc 21.875\n",
      "iteration 3701 loss 2.680027961730957, acc 15.625\n",
      "iteration 3702 loss 2.7593271732330322, acc 20.3125\n",
      "iteration 3703 loss 2.4276063442230225, acc 31.25\n",
      "iteration 3704 loss 2.5709176063537598, acc 25.0\n",
      "iteration 3705 loss 2.8446812629699707, acc 17.1875\n",
      "iteration 3706 loss 2.8942713737487793, acc 18.75\n",
      "iteration 3707 loss 2.5960135459899902, acc 18.75\n",
      "iteration 3708 loss 2.504692792892456, acc 28.125\n",
      "iteration 3709 loss 2.662426710128784, acc 20.3125\n",
      "iteration 3710 loss 2.7143211364746094, acc 25.0\n",
      "iteration 3711 loss 2.5812206268310547, acc 28.125\n",
      "iteration 3712 loss 2.7069904804229736, acc 21.875\n",
      "iteration 3713 loss 2.572270631790161, acc 34.375\n",
      "iteration 3714 loss 2.697171211242676, acc 29.6875\n",
      "iteration 3715 loss 2.5531153678894043, acc 25.0\n",
      "iteration 3716 loss 2.6653964519500732, acc 25.0\n",
      "iteration 3717 loss 2.825885772705078, acc 18.75\n",
      "iteration 3718 loss 2.520552396774292, acc 17.1875\n",
      "iteration 3719 loss 2.7739479541778564, acc 15.625\n",
      "iteration 3720 loss 2.866741895675659, acc 14.0625\n",
      "iteration 3721 loss 2.6863741874694824, acc 25.0\n",
      "iteration 3722 loss 2.8445558547973633, acc 17.1875\n",
      "iteration 3723 loss 2.7098536491394043, acc 20.3125\n",
      "iteration 3724 loss 2.6759393215179443, acc 14.0625\n",
      "iteration 3725 loss 2.819535493850708, acc 17.1875\n",
      "iteration 3726 loss 2.7821781635284424, acc 20.3125\n",
      "iteration 3727 loss 2.780668258666992, acc 21.875\n",
      "iteration 3728 loss 2.61506986618042, acc 21.875\n",
      "iteration 3729 loss 2.6097631454467773, acc 25.0\n",
      "iteration 3730 loss 2.936107873916626, acc 14.0625\n",
      "iteration 3731 loss 2.803948402404785, acc 15.625\n",
      "iteration 3732 loss 2.60819935798645, acc 21.875\n",
      "iteration 3733 loss 2.5786900520324707, acc 23.4375\n",
      "iteration 3734 loss 2.7282555103302, acc 23.4375\n",
      "iteration 3735 loss 2.8803539276123047, acc 18.75\n",
      "iteration 3736 loss 2.6819992065429688, acc 18.75\n",
      "iteration 3737 loss 2.6453726291656494, acc 26.5625\n",
      "iteration 3738 loss 2.595294713973999, acc 29.6875\n",
      "iteration 3739 loss 2.834554433822632, acc 20.3125\n",
      "iteration 3740 loss 2.6388649940490723, acc 26.5625\n",
      "iteration 3741 loss 2.5848548412323, acc 28.125\n",
      "iteration 3742 loss 2.532801866531372, acc 25.0\n",
      "iteration 3743 loss 2.724961996078491, acc 18.75\n",
      "iteration 3744 loss 2.6333694458007812, acc 29.6875\n",
      "iteration 3745 loss 2.596111536026001, acc 21.875\n",
      "iteration 3746 loss 2.900538444519043, acc 12.5\n",
      "iteration 3747 loss 2.503081798553467, acc 23.4375\n",
      "iteration 3748 loss 2.7554361820220947, acc 21.875\n",
      "iteration 3749 loss 2.8431854248046875, acc 18.75\n",
      "iteration 3750 loss 2.8544297218322754, acc 17.1875\n",
      "iteration 3751 loss 2.819167375564575, acc 18.75\n",
      "iteration 3752 loss 2.582752227783203, acc 31.25\n",
      "iteration 3753 loss 2.6286838054656982, acc 26.5625\n",
      "iteration 3754 loss 2.7143752574920654, acc 23.4375\n",
      "iteration 3755 loss 2.9140875339508057, acc 10.9375\n",
      "iteration 3756 loss 2.7097198963165283, acc 21.875\n",
      "iteration 3757 loss 2.9515771865844727, acc 15.625\n",
      "iteration 3758 loss 2.7209901809692383, acc 14.0625\n",
      "iteration 3759 loss 2.660945415496826, acc 26.5625\n",
      "iteration 3760 loss 2.619108200073242, acc 23.4375\n",
      "iteration 3761 loss 2.6585886478424072, acc 23.4375\n",
      "iteration 3762 loss 2.649710178375244, acc 21.875\n",
      "iteration 3763 loss 2.721083879470825, acc 21.875\n",
      "iteration 3764 loss 2.7301247119903564, acc 15.625\n",
      "iteration 3765 loss 2.6994595527648926, acc 20.3125\n",
      "iteration 3766 loss 2.691737174987793, acc 26.5625\n",
      "iteration 3767 loss 2.682723045349121, acc 20.3125\n",
      "iteration 3768 loss 2.710937976837158, acc 20.3125\n",
      "iteration 3769 loss 2.6012415885925293, acc 25.0\n",
      "iteration 3770 loss 2.531236171722412, acc 32.8125\n",
      "iteration 3771 loss 2.649597644805908, acc 25.0\n",
      "iteration 3772 loss 2.4640655517578125, acc 29.6875\n",
      "iteration 3773 loss 2.487652063369751, acc 29.6875\n",
      "iteration 3774 loss 2.7226719856262207, acc 21.875\n",
      "iteration 3775 loss 2.4804489612579346, acc 29.6875\n",
      "iteration 3776 loss 2.6920831203460693, acc 14.0625\n",
      "iteration 3777 loss 2.684112548828125, acc 17.1875\n",
      "iteration 3778 loss 2.6001648902893066, acc 18.75\n",
      "iteration 3779 loss 2.636931896209717, acc 28.125\n",
      "iteration 3780 loss 2.6371469497680664, acc 21.875\n",
      "iteration 3781 loss 2.665555953979492, acc 15.625\n",
      "iteration 3782 loss 2.8949332237243652, acc 10.9375\n",
      "iteration 3783 loss 2.718366861343384, acc 21.875\n",
      "iteration 3784 loss 2.8768253326416016, acc 12.5\n",
      "iteration 3785 loss 2.7811033725738525, acc 23.4375\n",
      "iteration 3786 loss 2.4801111221313477, acc 29.6875\n",
      "iteration 3787 loss 2.68947434425354, acc 20.3125\n",
      "iteration 3788 loss 2.8175103664398193, acc 17.1875\n",
      "iteration 3789 loss 2.5400497913360596, acc 28.125\n",
      "iteration 3790 loss 2.7662978172302246, acc 17.1875\n",
      "iteration 3791 loss 2.855470895767212, acc 15.625\n",
      "iteration 3792 loss 2.625887632369995, acc 15.625\n",
      "iteration 3793 loss 2.6000561714172363, acc 25.0\n",
      "iteration 3794 loss 2.6780714988708496, acc 23.4375\n",
      "iteration 3795 loss 2.667160749435425, acc 20.3125\n",
      "iteration 3796 loss 2.6377837657928467, acc 18.75\n",
      "iteration 3797 loss 2.5693247318267822, acc 28.125\n",
      "iteration 3798 loss 2.6591596603393555, acc 23.4375\n",
      "iteration 3799 loss 2.706108570098877, acc 20.3125\n",
      "iteration 3800 loss 2.611569404602051, acc 26.5625\n",
      "iteration 3801 loss 2.676008939743042, acc 23.4375\n",
      "iteration 3802 loss 2.54546856880188, acc 26.5625\n",
      "iteration 3803 loss 2.9124457836151123, acc 14.0625\n",
      "iteration 3804 loss 2.770784378051758, acc 20.3125\n",
      "iteration 3805 loss 2.5838756561279297, acc 32.8125\n",
      "iteration 3806 loss 2.9684829711914062, acc 14.0625\n",
      "iteration 3807 loss 2.625000476837158, acc 12.5\n",
      "iteration 3808 loss 2.70237398147583, acc 23.4375\n",
      "iteration 3809 loss 2.622591257095337, acc 21.875\n",
      "iteration 3810 loss 2.908406972885132, acc 21.875\n",
      "iteration 3811 loss 2.5105042457580566, acc 29.6875\n",
      "iteration 3812 loss 2.603675603866577, acc 29.6875\n",
      "iteration 3813 loss 2.7236175537109375, acc 21.875\n",
      "iteration 3814 loss 2.544546365737915, acc 25.0\n",
      "iteration 3815 loss 2.7102396488189697, acc 21.875\n",
      "iteration 3816 loss 2.67755389213562, acc 20.3125\n",
      "iteration 3817 loss 2.658216953277588, acc 18.75\n",
      "iteration 3818 loss 2.700836420059204, acc 25.0\n",
      "iteration 3819 loss 2.820681095123291, acc 17.1875\n",
      "iteration 3820 loss 2.616368293762207, acc 25.0\n",
      "iteration 3821 loss 2.6828384399414062, acc 28.125\n",
      "iteration 3822 loss 2.779188632965088, acc 15.625\n",
      "iteration 3823 loss 2.5074939727783203, acc 26.5625\n",
      "iteration 3824 loss 2.919126272201538, acc 21.875\n",
      "iteration 3825 loss 2.5979013442993164, acc 21.875\n",
      "iteration 3826 loss 2.6879775524139404, acc 29.6875\n",
      "iteration 3827 loss 2.706221342086792, acc 25.0\n",
      "iteration 3828 loss 2.7499845027923584, acc 20.3125\n",
      "iteration 3829 loss 2.655911445617676, acc 26.5625\n",
      "iteration 3830 loss 2.584357738494873, acc 26.5625\n",
      "iteration 3831 loss 2.7175493240356445, acc 23.4375\n",
      "iteration 3832 loss 2.704880714416504, acc 18.75\n",
      "iteration 3833 loss 2.5621092319488525, acc 21.875\n",
      "iteration 3834 loss 2.655024528503418, acc 29.6875\n",
      "iteration 3835 loss 2.831606864929199, acc 12.5\n",
      "iteration 3836 loss 2.6864700317382812, acc 20.3125\n",
      "iteration 3837 loss 2.434422731399536, acc 26.5625\n",
      "iteration 3838 loss 2.792322874069214, acc 21.875\n",
      "iteration 3839 loss 2.8982980251312256, acc 14.0625\n",
      "iteration 3840 loss 2.6873927116394043, acc 17.1875\n",
      "iteration 3841 loss 2.8176581859588623, acc 14.0625\n",
      "iteration 3842 loss 2.525911569595337, acc 26.5625\n",
      "iteration 3843 loss 2.5841522216796875, acc 25.0\n",
      "iteration 3844 loss 2.762298107147217, acc 18.75\n",
      "iteration 3845 loss 2.7168009281158447, acc 17.1875\n",
      "iteration 3846 loss 2.860011339187622, acc 9.375\n",
      "iteration 3847 loss 2.7223260402679443, acc 26.5625\n",
      "iteration 3848 loss 2.621873140335083, acc 23.4375\n",
      "iteration 3849 loss 2.7398898601531982, acc 26.5625\n",
      "iteration 3850 loss 2.61503267288208, acc 25.0\n",
      "iteration 3851 loss 2.8044419288635254, acc 17.1875\n",
      "iteration 3852 loss 2.864025592803955, acc 10.9375\n",
      "iteration 3853 loss 2.526066780090332, acc 28.125\n",
      "iteration 3854 loss 2.6848695278167725, acc 20.3125\n",
      "iteration 3855 loss 2.6063594818115234, acc 25.0\n",
      "iteration 3856 loss 2.6992440223693848, acc 28.125\n",
      "iteration 3857 loss 2.5849602222442627, acc 20.3125\n",
      "iteration 3858 loss 2.5607547760009766, acc 25.0\n",
      "iteration 3859 loss 2.6386988162994385, acc 21.875\n",
      "iteration 3860 loss 2.695293664932251, acc 25.0\n",
      "iteration 3861 loss 2.597825765609741, acc 23.4375\n",
      "iteration 3862 loss 2.721097230911255, acc 18.75\n",
      "iteration 3863 loss 2.796746015548706, acc 20.3125\n",
      "iteration 3864 loss 2.5979557037353516, acc 29.6875\n",
      "iteration 3865 loss 2.679180860519409, acc 25.0\n",
      "iteration 3866 loss 2.8770523071289062, acc 15.625\n",
      "iteration 3867 loss 2.6970717906951904, acc 20.3125\n",
      "iteration 3868 loss 2.8243331909179688, acc 20.3125\n",
      "iteration 3869 loss 2.7047762870788574, acc 23.4375\n",
      "iteration 3870 loss 2.866446018218994, acc 12.5\n",
      "iteration 3871 loss 2.7908763885498047, acc 17.1875\n",
      "iteration 3872 loss 2.8366832733154297, acc 23.4375\n",
      "iteration 3873 loss 2.7390878200531006, acc 18.75\n",
      "iteration 3874 loss 2.6081743240356445, acc 26.5625\n",
      "iteration 3875 loss 2.7716612815856934, acc 20.3125\n",
      "iteration 3876 loss 2.706944704055786, acc 18.75\n",
      "iteration 3877 loss 2.745328187942505, acc 17.1875\n",
      "iteration 3878 loss 2.864954948425293, acc 18.75\n",
      "iteration 3879 loss 2.516099214553833, acc 31.25\n",
      "iteration 3880 loss 2.8402299880981445, acc 14.0625\n",
      "iteration 3881 loss 2.69272780418396, acc 20.3125\n",
      "iteration 3882 loss 2.580073833465576, acc 23.4375\n",
      "iteration 3883 loss 2.8359320163726807, acc 18.75\n",
      "iteration 3884 loss 2.8335163593292236, acc 20.3125\n",
      "iteration 3885 loss 2.805232524871826, acc 17.1875\n",
      "iteration 3886 loss 2.616032361984253, acc 25.0\n",
      "iteration 3887 loss 2.7960736751556396, acc 18.75\n",
      "iteration 3888 loss 2.574164390563965, acc 25.0\n",
      "iteration 3889 loss 2.7789549827575684, acc 25.0\n",
      "iteration 3890 loss 2.468292474746704, acc 25.0\n",
      "iteration 3891 loss 2.7627310752868652, acc 23.4375\n",
      "iteration 3892 loss 2.7565255165100098, acc 15.625\n",
      "iteration 3893 loss 2.749990701675415, acc 26.5625\n",
      "iteration 3894 loss 2.737431764602661, acc 17.1875\n",
      "iteration 3895 loss 2.7458760738372803, acc 18.75\n",
      "iteration 3896 loss 2.5931785106658936, acc 23.4375\n",
      "iteration 3897 loss 2.8874776363372803, acc 21.875\n",
      "iteration 3898 loss 2.4034178256988525, acc 35.9375\n",
      "iteration 3899 loss 2.6954827308654785, acc 18.75\n",
      "iteration 3900 loss 2.8008975982666016, acc 17.1875\n",
      "iteration 3901 loss 2.6266748905181885, acc 25.0\n",
      "iteration 3902 loss 2.6526918411254883, acc 28.125\n",
      "iteration 3903 loss 2.6902856826782227, acc 26.5625\n",
      "iteration 3904 loss 2.7336692810058594, acc 26.5625\n",
      "iteration 3905 loss 2.8466663360595703, acc 26.5625\n",
      "iteration 3906 loss 2.7980687618255615, acc 21.875\n",
      "iteration 3907 loss 2.6624562740325928, acc 23.4375\n",
      "iteration 3908 loss 2.7007241249084473, acc 15.625\n",
      "iteration 3909 loss 2.671679973602295, acc 28.125\n",
      "iteration 3910 loss 2.89936900138855, acc 21.875\n",
      "iteration 3911 loss 2.686109781265259, acc 26.5625\n",
      "iteration 3912 loss 2.5936336517333984, acc 28.125\n",
      "iteration 3913 loss 2.777352809906006, acc 23.4375\n",
      "iteration 3914 loss 2.5707175731658936, acc 21.875\n",
      "iteration 3915 loss 2.891144037246704, acc 18.75\n",
      "iteration 3916 loss 2.5675442218780518, acc 18.75\n",
      "iteration 3917 loss 2.4844515323638916, acc 23.4375\n",
      "iteration 3918 loss 2.8600714206695557, acc 18.75\n",
      "iteration 3919 loss 2.669389486312866, acc 18.75\n",
      "iteration 3920 loss 2.5400168895721436, acc 23.4375\n",
      "iteration 3921 loss 2.7697792053222656, acc 25.0\n",
      "iteration 3922 loss 2.3729684352874756, acc 26.5625\n",
      "iteration 3923 loss 2.6598141193389893, acc 15.625\n",
      "iteration 3924 loss 2.8143417835235596, acc 25.0\n",
      "iteration 3925 loss 2.7537686824798584, acc 20.3125\n",
      "iteration 3926 loss 2.6130878925323486, acc 25.0\n",
      "iteration 3927 loss 2.645815849304199, acc 21.875\n",
      "iteration 3928 loss 2.8440637588500977, acc 12.5\n",
      "iteration 3929 loss 2.601351737976074, acc 29.6875\n",
      "iteration 3930 loss 2.7975451946258545, acc 18.75\n",
      "iteration 3931 loss 2.479292392730713, acc 32.8125\n",
      "iteration 3932 loss 2.6141815185546875, acc 20.3125\n",
      "iteration 3933 loss 2.4591376781463623, acc 25.0\n",
      "iteration 3934 loss 2.743774652481079, acc 20.3125\n",
      "iteration 3935 loss 2.881518840789795, acc 14.0625\n",
      "iteration 3936 loss 2.811821699142456, acc 15.625\n",
      "iteration 3937 loss 2.8083550930023193, acc 21.875\n",
      "iteration 3938 loss 2.741405487060547, acc 15.625\n",
      "iteration 3939 loss 2.7532947063446045, acc 18.75\n",
      "iteration 3940 loss 2.6889283657073975, acc 15.625\n",
      "iteration 3941 loss 2.757084846496582, acc 17.1875\n",
      "iteration 3942 loss 2.5422637462615967, acc 25.0\n",
      "iteration 3943 loss 2.513535737991333, acc 26.5625\n",
      "iteration 3944 loss 2.5595438480377197, acc 23.4375\n",
      "iteration 3945 loss 2.7377328872680664, acc 23.4375\n",
      "iteration 3946 loss 2.640495777130127, acc 20.3125\n",
      "iteration 3947 loss 2.8091156482696533, acc 15.625\n",
      "iteration 3948 loss 2.733705759048462, acc 20.3125\n",
      "iteration 3949 loss 2.6532235145568848, acc 18.75\n",
      "iteration 3950 loss 2.694370985031128, acc 25.0\n",
      "iteration 3951 loss 2.6864962577819824, acc 25.0\n",
      "iteration 3952 loss 2.8202476501464844, acc 15.625\n",
      "iteration 3953 loss 2.5693116188049316, acc 25.0\n",
      "iteration 3954 loss 2.944714307785034, acc 17.1875\n",
      "iteration 3955 loss 2.6975953578948975, acc 18.75\n",
      "iteration 3956 loss 2.6136269569396973, acc 26.5625\n",
      "iteration 3957 loss 2.7351927757263184, acc 23.4375\n",
      "iteration 3958 loss 2.495295763015747, acc 25.0\n",
      "iteration 3959 loss 2.625056505203247, acc 25.0\n",
      "iteration 3960 loss 2.5044946670532227, acc 32.8125\n",
      "iteration 3961 loss 2.8181838989257812, acc 18.75\n",
      "iteration 3962 loss 2.501305103302002, acc 26.5625\n",
      "iteration 3963 loss 2.6841511726379395, acc 28.125\n",
      "iteration 3964 loss 2.4959394931793213, acc 26.5625\n",
      "iteration 3965 loss 2.466043710708618, acc 25.0\n",
      "iteration 3966 loss 2.3929691314697266, acc 31.25\n",
      "iteration 3967 loss 2.6825270652770996, acc 20.3125\n",
      "iteration 3968 loss 2.7781999111175537, acc 17.1875\n",
      "iteration 3969 loss 2.546144485473633, acc 23.4375\n",
      "iteration 3970 loss 2.712636947631836, acc 23.4375\n",
      "iteration 3971 loss 2.651890516281128, acc 26.5625\n",
      "iteration 3972 loss 2.527402400970459, acc 28.125\n",
      "iteration 3973 loss 2.7455835342407227, acc 21.875\n",
      "iteration 3974 loss 2.74752140045166, acc 20.3125\n",
      "iteration 3975 loss 2.898503065109253, acc 18.75\n",
      "iteration 3976 loss 2.531796932220459, acc 31.25\n",
      "iteration 3977 loss 2.5544257164001465, acc 23.4375\n",
      "iteration 3978 loss 2.6750540733337402, acc 26.5625\n",
      "iteration 3979 loss 2.6125190258026123, acc 28.125\n",
      "iteration 3980 loss 2.883612871170044, acc 20.3125\n",
      "iteration 3981 loss 2.5178956985473633, acc 32.8125\n",
      "iteration 3982 loss 2.5148682594299316, acc 32.8125\n",
      "iteration 3983 loss 2.6478335857391357, acc 21.875\n",
      "iteration 3984 loss 2.7659544944763184, acc 26.5625\n",
      "iteration 3985 loss 2.828137159347534, acc 17.1875\n",
      "iteration 3986 loss 2.5864241123199463, acc 31.25\n",
      "iteration 3987 loss 2.60101318359375, acc 21.875\n",
      "iteration 3988 loss 2.8708863258361816, acc 18.75\n",
      "iteration 3989 loss 2.7505595684051514, acc 23.4375\n",
      "iteration 3990 loss 2.8800292015075684, acc 18.75\n",
      "iteration 3991 loss 2.586857795715332, acc 20.3125\n",
      "iteration 3992 loss 2.7764251232147217, acc 15.625\n",
      "iteration 3993 loss 2.8038318157196045, acc 25.0\n",
      "iteration 3994 loss 2.7325141429901123, acc 23.4375\n",
      "iteration 3995 loss 2.7247395515441895, acc 25.0\n",
      "iteration 3996 loss 2.470094680786133, acc 28.125\n",
      "iteration 3997 loss 2.6765167713165283, acc 32.8125\n",
      "iteration 3998 loss 2.367386817932129, acc 26.5625\n",
      "iteration 3999 loss 2.569875717163086, acc 25.0\n",
      "iteration 4000 loss 2.603576898574829, acc 20.3125\n",
      "iteration 4001 loss 2.8915469646453857, acc 14.0625\n",
      "iteration 4002 loss 2.687983274459839, acc 20.3125\n",
      "iteration 4003 loss 2.7053942680358887, acc 20.3125\n",
      "iteration 4004 loss 2.458712100982666, acc 29.6875\n",
      "iteration 4005 loss 2.575347900390625, acc 28.125\n",
      "iteration 4006 loss 2.727902889251709, acc 21.875\n",
      "iteration 4007 loss 2.641155242919922, acc 34.375\n",
      "iteration 4008 loss 2.7301435470581055, acc 21.875\n",
      "iteration 4009 loss 2.7947540283203125, acc 17.1875\n",
      "iteration 4010 loss 2.68405818939209, acc 23.4375\n",
      "iteration 4011 loss 2.6182210445404053, acc 17.1875\n",
      "iteration 4012 loss 2.6717817783355713, acc 21.875\n",
      "iteration 4013 loss 2.6644248962402344, acc 25.0\n",
      "iteration 4014 loss 2.6835129261016846, acc 21.875\n",
      "iteration 4015 loss 2.631265163421631, acc 15.625\n",
      "iteration 4016 loss 2.711054801940918, acc 25.0\n",
      "iteration 4017 loss 2.557716131210327, acc 21.875\n",
      "iteration 4018 loss 2.776672601699829, acc 18.75\n",
      "iteration 4019 loss 2.65706467628479, acc 14.0625\n",
      "iteration 4020 loss 2.7126665115356445, acc 21.875\n",
      "iteration 4021 loss 2.531893014907837, acc 23.4375\n",
      "iteration 4022 loss 2.648839235305786, acc 18.75\n",
      "iteration 4023 loss 2.7032957077026367, acc 18.75\n",
      "iteration 4024 loss 2.608043909072876, acc 18.75\n",
      "iteration 4025 loss 2.753272294998169, acc 23.4375\n",
      "iteration 4026 loss 2.531468391418457, acc 26.5625\n",
      "iteration 4027 loss 2.764289140701294, acc 21.875\n",
      "iteration 4028 loss 2.6518781185150146, acc 21.875\n",
      "iteration 4029 loss 2.6395459175109863, acc 18.75\n",
      "iteration 4030 loss 2.686040163040161, acc 28.125\n",
      "iteration 4031 loss 2.7401058673858643, acc 17.1875\n",
      "iteration 4032 loss 2.565200090408325, acc 26.5625\n",
      "iteration 4033 loss 2.973473072052002, acc 17.1875\n",
      "iteration 4034 loss 2.539552688598633, acc 29.6875\n",
      "iteration 4035 loss 2.729907751083374, acc 21.875\n",
      "iteration 4036 loss 2.4853479862213135, acc 25.0\n",
      "iteration 4037 loss 2.5618300437927246, acc 28.125\n",
      "iteration 4038 loss 2.8341004848480225, acc 23.4375\n",
      "iteration 4039 loss 2.765103816986084, acc 21.875\n",
      "iteration 4040 loss 2.605276107788086, acc 21.875\n",
      "iteration 4041 loss 2.8103702068328857, acc 26.5625\n",
      "iteration 4042 loss 2.77717661857605, acc 17.1875\n",
      "iteration 4043 loss 2.3755433559417725, acc 40.625\n",
      "iteration 4044 loss 2.804652452468872, acc 17.1875\n",
      "iteration 4045 loss 2.706653118133545, acc 23.4375\n",
      "iteration 4046 loss 2.65899920463562, acc 28.125\n",
      "iteration 4047 loss 2.631287097930908, acc 23.4375\n",
      "iteration 4048 loss 2.625345230102539, acc 21.875\n",
      "iteration 4049 loss 2.8540070056915283, acc 12.5\n",
      "iteration 4050 loss 2.669520854949951, acc 20.3125\n",
      "iteration 4051 loss 2.658688545227051, acc 26.5625\n",
      "iteration 4052 loss 2.697319746017456, acc 21.875\n",
      "iteration 4053 loss 2.754100799560547, acc 20.3125\n",
      "iteration 4054 loss 2.731210947036743, acc 23.4375\n",
      "iteration 4055 loss 2.6063971519470215, acc 20.3125\n",
      "iteration 4056 loss 2.6629748344421387, acc 23.4375\n",
      "iteration 4057 loss 2.5522685050964355, acc 23.4375\n",
      "iteration 4058 loss 2.8143651485443115, acc 15.625\n",
      "iteration 4059 loss 2.5245232582092285, acc 28.125\n",
      "iteration 4060 loss 2.7172441482543945, acc 20.3125\n",
      "iteration 4061 loss 2.574718952178955, acc 28.125\n",
      "iteration 4062 loss 2.69114089012146, acc 21.875\n",
      "iteration 4063 loss 2.5987720489501953, acc 26.5625\n",
      "iteration 4064 loss 2.6085281372070312, acc 35.9375\n",
      "iteration 4065 loss 2.8246333599090576, acc 20.3125\n",
      "iteration 4066 loss 2.906113386154175, acc 15.625\n",
      "iteration 4067 loss 2.8169288635253906, acc 15.625\n",
      "iteration 4068 loss 2.7682948112487793, acc 18.75\n",
      "iteration 4069 loss 2.5740416049957275, acc 25.0\n",
      "iteration 4070 loss 2.5717051029205322, acc 28.125\n",
      "iteration 4071 loss 2.7645044326782227, acc 18.75\n",
      "iteration 4072 loss 2.5834076404571533, acc 25.0\n",
      "iteration 4073 loss 2.4337785243988037, acc 34.375\n",
      "iteration 4074 loss 2.628211736679077, acc 29.6875\n",
      "iteration 4075 loss 2.6209332942962646, acc 21.875\n",
      "iteration 4076 loss 2.472660779953003, acc 37.5\n",
      "iteration 4077 loss 2.6025543212890625, acc 18.75\n",
      "iteration 4078 loss 2.7232024669647217, acc 21.875\n",
      "iteration 4079 loss 2.819359302520752, acc 10.9375\n",
      "iteration 4080 loss 2.422410726547241, acc 23.4375\n",
      "iteration 4081 loss 2.7962875366210938, acc 18.75\n",
      "iteration 4082 loss 2.635122537612915, acc 25.0\n",
      "iteration 4083 loss 2.811842918395996, acc 28.125\n",
      "iteration 4084 loss 2.6430461406707764, acc 20.3125\n",
      "iteration 4085 loss 2.6419265270233154, acc 21.875\n",
      "iteration 4086 loss 2.670417547225952, acc 25.0\n",
      "iteration 4087 loss 2.513288974761963, acc 29.6875\n",
      "iteration 4088 loss 2.742264747619629, acc 15.625\n",
      "iteration 4089 loss 2.669666051864624, acc 20.3125\n",
      "iteration 4090 loss 2.6184771060943604, acc 29.6875\n",
      "iteration 4091 loss 2.6413052082061768, acc 26.5625\n",
      "iteration 4092 loss 2.6485581398010254, acc 21.875\n",
      "iteration 4093 loss 2.7719359397888184, acc 15.625\n",
      "iteration 4094 loss 2.6494455337524414, acc 28.125\n",
      "iteration 4095 loss 2.7242257595062256, acc 20.3125\n",
      "iteration 4096 loss 2.6685070991516113, acc 25.0\n",
      "iteration 4097 loss 2.681910753250122, acc 25.0\n",
      "iteration 4098 loss 2.7416627407073975, acc 26.5625\n",
      "iteration 4099 loss 2.360603094100952, acc 32.8125\n",
      "iteration 4100 loss 2.5777018070220947, acc 20.3125\n",
      "iteration 4101 loss 2.759061098098755, acc 21.875\n",
      "iteration 4102 loss 2.7158448696136475, acc 23.4375\n",
      "iteration 4103 loss 2.826650381088257, acc 18.75\n",
      "iteration 4104 loss 2.5678322315216064, acc 21.875\n",
      "iteration 4105 loss 2.7993576526641846, acc 21.875\n",
      "iteration 4106 loss 2.9053902626037598, acc 17.1875\n",
      "iteration 4107 loss 2.530477285385132, acc 20.3125\n",
      "iteration 4108 loss 2.6420629024505615, acc 17.1875\n",
      "iteration 4109 loss 2.5744149684906006, acc 25.0\n",
      "iteration 4110 loss 2.6265969276428223, acc 25.0\n",
      "iteration 4111 loss 2.630171298980713, acc 21.875\n",
      "iteration 4112 loss 2.723749876022339, acc 20.3125\n",
      "iteration 4113 loss 2.6425516605377197, acc 26.5625\n",
      "iteration 4114 loss 2.669997215270996, acc 26.5625\n",
      "iteration 4115 loss 2.689830780029297, acc 26.5625\n",
      "iteration 4116 loss 2.439732551574707, acc 29.6875\n",
      "iteration 4117 loss 2.829439640045166, acc 18.75\n",
      "iteration 4118 loss 2.629375696182251, acc 25.0\n",
      "iteration 4119 loss 2.718280553817749, acc 26.5625\n",
      "iteration 4120 loss 2.7582926750183105, acc 23.4375\n",
      "iteration 4121 loss 2.7184338569641113, acc 25.0\n",
      "iteration 4122 loss 2.783722400665283, acc 18.75\n",
      "iteration 4123 loss 2.5838310718536377, acc 21.875\n",
      "iteration 4124 loss 2.681056022644043, acc 23.4375\n",
      "iteration 4125 loss 2.9047164916992188, acc 10.9375\n",
      "iteration 4126 loss 2.7175469398498535, acc 18.75\n",
      "iteration 4127 loss 2.61067533493042, acc 26.5625\n",
      "iteration 4128 loss 2.7625980377197266, acc 21.875\n",
      "iteration 4129 loss 2.68807315826416, acc 21.875\n",
      "iteration 4130 loss 2.5469412803649902, acc 28.125\n",
      "iteration 4131 loss 2.6461310386657715, acc 26.5625\n",
      "iteration 4132 loss 2.6678366661071777, acc 20.3125\n",
      "iteration 4133 loss 2.749992847442627, acc 20.3125\n",
      "iteration 4134 loss 2.785918712615967, acc 18.75\n",
      "iteration 4135 loss 2.642062187194824, acc 25.0\n",
      "iteration 4136 loss 2.6907296180725098, acc 26.5625\n",
      "iteration 4137 loss 2.820756673812866, acc 18.75\n",
      "iteration 4138 loss 2.5380666255950928, acc 23.4375\n",
      "iteration 4139 loss 2.663712978363037, acc 18.75\n",
      "iteration 4140 loss 2.607370615005493, acc 26.5625\n",
      "iteration 4141 loss 2.6483547687530518, acc 21.875\n",
      "iteration 4142 loss 2.46836256980896, acc 31.25\n",
      "iteration 4143 loss 2.902491331100464, acc 15.625\n",
      "iteration 4144 loss 2.707933187484741, acc 21.875\n",
      "iteration 4145 loss 2.824859142303467, acc 21.875\n",
      "iteration 4146 loss 2.7632851600646973, acc 20.3125\n",
      "iteration 4147 loss 2.4162514209747314, acc 26.5625\n",
      "iteration 4148 loss 2.683523416519165, acc 15.625\n",
      "iteration 4149 loss 2.7687442302703857, acc 21.875\n",
      "iteration 4150 loss 2.6720457077026367, acc 17.1875\n",
      "iteration 4151 loss 2.4952948093414307, acc 34.375\n",
      "iteration 4152 loss 2.4713633060455322, acc 31.25\n",
      "iteration 4153 loss 2.621244430541992, acc 29.6875\n",
      "iteration 4154 loss 2.778545379638672, acc 14.0625\n",
      "iteration 4155 loss 2.8011581897735596, acc 20.3125\n",
      "iteration 4156 loss 2.75109601020813, acc 20.3125\n",
      "iteration 4157 loss 2.555461883544922, acc 26.5625\n",
      "iteration 4158 loss 2.592533826828003, acc 23.4375\n",
      "iteration 4159 loss 2.6036882400512695, acc 21.875\n",
      "iteration 4160 loss 2.60977840423584, acc 26.5625\n",
      "iteration 4161 loss 2.7554121017456055, acc 18.75\n",
      "iteration 4162 loss 2.632159471511841, acc 25.0\n",
      "iteration 4163 loss 2.9332220554351807, acc 14.0625\n",
      "iteration 4164 loss 2.665663719177246, acc 21.875\n",
      "iteration 4165 loss 2.65492844581604, acc 25.0\n",
      "iteration 4166 loss 2.753237247467041, acc 20.3125\n",
      "iteration 4167 loss 2.647364616394043, acc 21.875\n",
      "iteration 4168 loss 2.6738033294677734, acc 21.875\n",
      "iteration 4169 loss 2.490098714828491, acc 21.875\n",
      "iteration 4170 loss 2.7628862857818604, acc 18.75\n",
      "iteration 4171 loss 2.6130058765411377, acc 18.75\n",
      "iteration 4172 loss 2.7745041847229004, acc 17.1875\n",
      "iteration 4173 loss 2.646977186203003, acc 21.875\n",
      "iteration 4174 loss 2.8709449768066406, acc 18.75\n",
      "iteration 4175 loss 2.589031934738159, acc 26.5625\n",
      "iteration 4176 loss 2.7249557971954346, acc 17.1875\n",
      "iteration 4177 loss 2.6786510944366455, acc 20.3125\n",
      "iteration 4178 loss 2.358896017074585, acc 37.5\n",
      "iteration 4179 loss 2.6357266902923584, acc 14.0625\n",
      "iteration 4180 loss 2.818596363067627, acc 23.4375\n",
      "iteration 4181 loss 2.654327869415283, acc 21.875\n",
      "iteration 4182 loss 2.704481840133667, acc 20.3125\n",
      "iteration 4183 loss 2.579864978790283, acc 20.3125\n",
      "iteration 4184 loss 2.6599960327148438, acc 20.3125\n",
      "iteration 4185 loss 2.6866703033447266, acc 17.1875\n",
      "iteration 4186 loss 2.6098241806030273, acc 29.6875\n",
      "iteration 4187 loss 2.7683568000793457, acc 18.75\n",
      "iteration 4188 loss 2.7448809146881104, acc 26.5625\n",
      "iteration 4189 loss 2.817991256713867, acc 18.75\n",
      "iteration 4190 loss 2.7819879055023193, acc 15.625\n",
      "iteration 4191 loss 2.68151593208313, acc 25.0\n",
      "iteration 4192 loss 2.682898998260498, acc 23.4375\n",
      "iteration 4193 loss 2.9888510704040527, acc 10.9375\n",
      "iteration 4194 loss 2.66922926902771, acc 21.875\n",
      "iteration 4195 loss 2.6829259395599365, acc 21.875\n",
      "iteration 4196 loss 2.766484498977661, acc 21.875\n",
      "iteration 4197 loss 2.6212661266326904, acc 25.0\n",
      "iteration 4198 loss 2.743361711502075, acc 15.625\n",
      "iteration 4199 loss 2.648787260055542, acc 17.1875\n",
      "iteration 4200 loss 2.6576058864593506, acc 20.3125\n",
      "iteration 4201 loss 2.6004414558410645, acc 29.6875\n",
      "iteration 4202 loss 2.686826705932617, acc 20.3125\n",
      "iteration 4203 loss 2.688563823699951, acc 15.625\n",
      "iteration 4204 loss 2.6257646083831787, acc 25.0\n",
      "iteration 4205 loss 2.6217739582061768, acc 18.75\n",
      "iteration 4206 loss 2.844590187072754, acc 18.75\n",
      "iteration 4207 loss 2.55930495262146, acc 25.0\n",
      "iteration 4208 loss 2.753622055053711, acc 17.1875\n",
      "iteration 4209 loss 2.7029969692230225, acc 18.75\n",
      "iteration 4210 loss 2.78285813331604, acc 21.875\n",
      "iteration 4211 loss 2.6367452144622803, acc 25.0\n",
      "iteration 4212 loss 2.460373878479004, acc 28.125\n",
      "iteration 4213 loss 2.7673375606536865, acc 18.75\n",
      "iteration 4214 loss 2.5469963550567627, acc 20.3125\n",
      "iteration 4215 loss 2.7355761528015137, acc 21.875\n",
      "iteration 4216 loss 2.4361143112182617, acc 31.25\n",
      "iteration 4217 loss 2.5297980308532715, acc 25.0\n",
      "iteration 4218 loss 2.6301112174987793, acc 25.0\n",
      "iteration 4219 loss 2.565413236618042, acc 21.875\n",
      "iteration 4220 loss 2.560560464859009, acc 18.75\n",
      "iteration 4221 loss 2.647759199142456, acc 25.0\n",
      "iteration 4222 loss 2.7756218910217285, acc 25.0\n",
      "iteration 4223 loss 2.57094669342041, acc 28.125\n",
      "iteration 4224 loss 2.8324637413024902, acc 18.75\n",
      "iteration 4225 loss 2.6473348140716553, acc 20.3125\n",
      "iteration 4226 loss 2.769883632659912, acc 14.0625\n",
      "iteration 4227 loss 2.712773084640503, acc 23.4375\n",
      "iteration 4228 loss 2.730811834335327, acc 21.875\n",
      "iteration 4229 loss 2.458669662475586, acc 25.0\n",
      "iteration 4230 loss 2.587200164794922, acc 18.75\n",
      "iteration 4231 loss 2.792562484741211, acc 21.875\n",
      "iteration 4232 loss 2.7886126041412354, acc 20.3125\n",
      "iteration 4233 loss 2.7695319652557373, acc 20.3125\n",
      "iteration 4234 loss 2.4401967525482178, acc 29.6875\n",
      "iteration 4235 loss 2.591543197631836, acc 21.875\n",
      "iteration 4236 loss 2.869459629058838, acc 21.875\n",
      "iteration 4237 loss 2.7277414798736572, acc 18.75\n",
      "iteration 4238 loss 2.734687328338623, acc 18.75\n",
      "iteration 4239 loss 2.845315456390381, acc 14.0625\n",
      "iteration 4240 loss 2.868257999420166, acc 10.9375\n",
      "iteration 4241 loss 2.735698938369751, acc 23.4375\n",
      "iteration 4242 loss 2.81040358543396, acc 21.875\n",
      "iteration 4243 loss 2.795799970626831, acc 17.1875\n",
      "iteration 4244 loss 2.711758852005005, acc 20.3125\n",
      "iteration 4245 loss 2.792750358581543, acc 10.9375\n",
      "iteration 4246 loss 2.6637425422668457, acc 20.3125\n",
      "iteration 4247 loss 2.738682508468628, acc 20.3125\n",
      "iteration 4248 loss 3.04059100151062, acc 9.375\n",
      "iteration 4249 loss 2.592397451400757, acc 18.75\n",
      "iteration 4250 loss 2.651970386505127, acc 28.125\n",
      "iteration 4251 loss 2.8694751262664795, acc 23.4375\n",
      "iteration 4252 loss 2.4295921325683594, acc 31.25\n",
      "iteration 4253 loss 2.5378053188323975, acc 21.875\n",
      "iteration 4254 loss 2.6491589546203613, acc 26.5625\n",
      "iteration 4255 loss 2.7160325050354004, acc 21.875\n",
      "iteration 4256 loss 2.649522542953491, acc 21.875\n",
      "iteration 4257 loss 2.4074819087982178, acc 29.6875\n",
      "iteration 4258 loss 2.827857255935669, acc 21.875\n",
      "iteration 4259 loss 2.5439767837524414, acc 23.4375\n",
      "iteration 4260 loss 3.0335817337036133, acc 12.5\n",
      "iteration 4261 loss 2.542731761932373, acc 26.5625\n",
      "iteration 4262 loss 2.567478656768799, acc 31.25\n",
      "iteration 4263 loss 2.9295854568481445, acc 18.75\n",
      "iteration 4264 loss 2.6426467895507812, acc 21.875\n",
      "iteration 4265 loss 2.7093279361724854, acc 23.4375\n",
      "iteration 4266 loss 2.750404119491577, acc 18.75\n",
      "iteration 4267 loss 2.611588478088379, acc 23.4375\n",
      "iteration 4268 loss 2.809910297393799, acc 17.1875\n",
      "iteration 4269 loss 2.7766895294189453, acc 18.75\n",
      "iteration 4270 loss 2.7292914390563965, acc 29.6875\n",
      "iteration 4271 loss 2.5531067848205566, acc 28.125\n",
      "iteration 4272 loss 2.6764490604400635, acc 23.4375\n",
      "iteration 4273 loss 2.7736377716064453, acc 23.4375\n",
      "iteration 4274 loss 2.5867817401885986, acc 18.75\n",
      "iteration 4275 loss 2.8537046909332275, acc 18.75\n",
      "iteration 4276 loss 2.612751007080078, acc 23.4375\n",
      "iteration 4277 loss 2.6402077674865723, acc 23.4375\n",
      "iteration 4278 loss 2.4172325134277344, acc 29.6875\n",
      "iteration 4279 loss 2.801485061645508, acc 14.0625\n",
      "iteration 4280 loss 2.7594187259674072, acc 12.5\n",
      "iteration 4281 loss 2.5786426067352295, acc 28.125\n",
      "iteration 4282 loss 2.7230560779571533, acc 20.3125\n",
      "iteration 4283 loss 2.6622462272644043, acc 26.5625\n",
      "iteration 4284 loss 2.787651538848877, acc 17.1875\n",
      "iteration 4285 loss 2.67802095413208, acc 17.1875\n",
      "iteration 4286 loss 2.7280325889587402, acc 18.75\n",
      "iteration 4287 loss 2.4711389541625977, acc 34.375\n",
      "iteration 4288 loss 2.738572120666504, acc 17.1875\n",
      "iteration 4289 loss 2.468905210494995, acc 32.8125\n",
      "iteration 4290 loss 2.7798140048980713, acc 18.75\n",
      "iteration 4291 loss 2.4686641693115234, acc 25.0\n",
      "iteration 4292 loss 2.8368334770202637, acc 21.875\n",
      "iteration 4293 loss 2.7566256523132324, acc 18.75\n",
      "iteration 4294 loss 2.780702590942383, acc 18.75\n",
      "iteration 4295 loss 2.652723789215088, acc 21.875\n",
      "iteration 4296 loss 2.5943493843078613, acc 18.75\n",
      "iteration 4297 loss 2.734971046447754, acc 14.0625\n",
      "iteration 4298 loss 2.7296130657196045, acc 18.75\n",
      "iteration 4299 loss 2.703695058822632, acc 18.75\n",
      "iteration 4300 loss 2.8295509815216064, acc 20.3125\n",
      "iteration 4301 loss 2.6886370182037354, acc 15.625\n",
      "iteration 4302 loss 2.8711562156677246, acc 20.3125\n",
      "iteration 4303 loss 2.6820168495178223, acc 17.1875\n",
      "iteration 4304 loss 2.667285919189453, acc 21.875\n",
      "iteration 4305 loss 2.780280828475952, acc 12.5\n",
      "iteration 4306 loss 2.6414287090301514, acc 18.75\n",
      "iteration 4307 loss 2.681368350982666, acc 21.875\n",
      "iteration 4308 loss 2.4606096744537354, acc 34.375\n",
      "iteration 4309 loss 2.716170072555542, acc 23.4375\n",
      "iteration 4310 loss 2.8873097896575928, acc 17.1875\n",
      "iteration 4311 loss 2.644953489303589, acc 25.0\n",
      "iteration 4312 loss 2.8065085411071777, acc 20.3125\n",
      "iteration 4313 loss 2.924837589263916, acc 23.4375\n",
      "iteration 4314 loss 2.7141053676605225, acc 21.875\n",
      "iteration 4315 loss 2.674715757369995, acc 17.1875\n",
      "iteration 4316 loss 2.7620153427124023, acc 18.75\n",
      "iteration 4317 loss 2.8482446670532227, acc 17.1875\n",
      "iteration 4318 loss 2.734546422958374, acc 15.625\n",
      "iteration 4319 loss 2.6890430450439453, acc 23.4375\n",
      "iteration 4320 loss 2.850212574005127, acc 18.75\n",
      "iteration 4321 loss 2.7798585891723633, acc 10.9375\n",
      "iteration 4322 loss 2.605609893798828, acc 18.75\n",
      "iteration 4323 loss 2.649512529373169, acc 17.1875\n",
      "iteration 4324 loss 2.567540407180786, acc 21.875\n",
      "iteration 4325 loss 2.684966564178467, acc 20.3125\n",
      "iteration 4326 loss 2.804668664932251, acc 17.1875\n",
      "iteration 4327 loss 2.922693967819214, acc 17.1875\n",
      "iteration 4328 loss 2.7289438247680664, acc 23.4375\n",
      "iteration 4329 loss 2.5166499614715576, acc 28.125\n",
      "iteration 4330 loss 2.6115174293518066, acc 25.0\n",
      "iteration 4331 loss 2.7913427352905273, acc 15.625\n",
      "iteration 4332 loss 2.5603902339935303, acc 20.3125\n",
      "iteration 4333 loss 2.6397628784179688, acc 18.75\n",
      "iteration 4334 loss 2.6544485092163086, acc 31.25\n",
      "iteration 4335 loss 2.775555372238159, acc 20.3125\n",
      "iteration 4336 loss 2.754650592803955, acc 20.3125\n",
      "iteration 4337 loss 2.6748969554901123, acc 26.5625\n",
      "iteration 4338 loss 2.7045962810516357, acc 17.1875\n",
      "iteration 4339 loss 2.853703737258911, acc 7.8125\n",
      "iteration 4340 loss 2.74711012840271, acc 10.9375\n",
      "iteration 4341 loss 2.672081232070923, acc 12.5\n",
      "iteration 4342 loss 2.7149436473846436, acc 29.6875\n",
      "iteration 4343 loss 2.7636892795562744, acc 20.3125\n",
      "iteration 4344 loss 3.018867254257202, acc 17.1875\n",
      "iteration 4345 loss 2.5749728679656982, acc 25.0\n",
      "iteration 4346 loss 2.594801187515259, acc 17.1875\n",
      "iteration 4347 loss 2.802345037460327, acc 17.1875\n",
      "iteration 4348 loss 2.486196994781494, acc 17.1875\n",
      "iteration 4349 loss 2.8281922340393066, acc 15.625\n",
      "iteration 4350 loss 2.7365667819976807, acc 20.3125\n",
      "iteration 4351 loss 2.7984445095062256, acc 21.875\n",
      "iteration 4352 loss 2.963477849960327, acc 17.1875\n",
      "iteration 4353 loss 2.688844919204712, acc 25.0\n",
      "iteration 4354 loss 2.536757469177246, acc 25.0\n",
      "iteration 4355 loss 2.7598252296447754, acc 12.5\n",
      "iteration 4356 loss 2.6046743392944336, acc 21.875\n",
      "iteration 4357 loss 2.800058126449585, acc 18.75\n",
      "iteration 4358 loss 2.4986703395843506, acc 29.6875\n",
      "iteration 4359 loss 2.733325242996216, acc 25.0\n",
      "iteration 4360 loss 2.540727376937866, acc 23.4375\n",
      "iteration 4361 loss 2.8635408878326416, acc 20.3125\n",
      "iteration 4362 loss 2.544233560562134, acc 20.3125\n",
      "iteration 4363 loss 2.882023334503174, acc 15.625\n",
      "iteration 4364 loss 2.514768123626709, acc 29.6875\n",
      "iteration 4365 loss 2.711526870727539, acc 15.625\n",
      "iteration 4366 loss 2.781215190887451, acc 14.0625\n",
      "iteration 4367 loss 2.7269771099090576, acc 23.4375\n",
      "iteration 4368 loss 2.71258282661438, acc 17.1875\n",
      "iteration 4369 loss 2.507331609725952, acc 28.125\n",
      "iteration 4370 loss 2.6971001625061035, acc 20.3125\n",
      "iteration 4371 loss 2.707202911376953, acc 26.5625\n",
      "iteration 4372 loss 2.6592977046966553, acc 20.3125\n",
      "iteration 4373 loss 2.589430570602417, acc 23.4375\n",
      "iteration 4374 loss 2.680701494216919, acc 21.875\n",
      "iteration 4375 loss 2.684675693511963, acc 17.1875\n",
      "iteration 4376 loss 2.595252275466919, acc 20.3125\n",
      "iteration 4377 loss 2.6933677196502686, acc 31.25\n",
      "iteration 4378 loss 2.98567271232605, acc 14.0625\n",
      "iteration 4379 loss 2.6577796936035156, acc 21.875\n",
      "iteration 4380 loss 2.7508702278137207, acc 10.9375\n",
      "iteration 4381 loss 2.677034854888916, acc 23.4375\n",
      "iteration 4382 loss 2.634824275970459, acc 15.625\n",
      "iteration 4383 loss 2.592310905456543, acc 20.3125\n",
      "iteration 4384 loss 2.855884552001953, acc 10.9375\n",
      "iteration 4385 loss 2.714752197265625, acc 17.1875\n",
      "iteration 4386 loss 2.7830255031585693, acc 25.0\n",
      "iteration 4387 loss 2.5339314937591553, acc 25.0\n",
      "iteration 4388 loss 2.714428186416626, acc 20.3125\n",
      "iteration 4389 loss 2.7141008377075195, acc 14.0625\n",
      "iteration 4390 loss 2.685905933380127, acc 26.5625\n",
      "iteration 4391 loss 2.7391855716705322, acc 23.4375\n",
      "iteration 4392 loss 2.5517635345458984, acc 34.375\n",
      "iteration 4393 loss 2.472869634628296, acc 29.6875\n",
      "iteration 4394 loss 2.739427328109741, acc 15.625\n",
      "iteration 4395 loss 2.765791893005371, acc 20.3125\n",
      "iteration 4396 loss 2.679431915283203, acc 23.4375\n",
      "iteration 4397 loss 2.5296568870544434, acc 15.625\n",
      "iteration 4398 loss 2.543444871902466, acc 25.0\n",
      "iteration 4399 loss 2.7263922691345215, acc 17.1875\n",
      "iteration 4400 loss 2.6804051399230957, acc 10.9375\n",
      "iteration 4401 loss 2.945279836654663, acc 7.8125\n",
      "iteration 4402 loss 2.5198330879211426, acc 14.0625\n",
      "iteration 4403 loss 2.544429302215576, acc 18.75\n",
      "iteration 4404 loss 2.691189765930176, acc 21.875\n",
      "iteration 4405 loss 2.560303211212158, acc 26.5625\n",
      "iteration 4406 loss 2.5839624404907227, acc 23.4375\n",
      "iteration 4407 loss 2.552704334259033, acc 26.5625\n",
      "iteration 4408 loss 2.733731746673584, acc 14.0625\n",
      "iteration 4409 loss 2.660048246383667, acc 21.875\n",
      "iteration 4410 loss 2.6209285259246826, acc 14.0625\n",
      "iteration 4411 loss 2.5438120365142822, acc 26.5625\n",
      "iteration 4412 loss 2.684720754623413, acc 20.3125\n",
      "iteration 4413 loss 2.679388999938965, acc 17.1875\n",
      "iteration 4414 loss 2.8753910064697266, acc 25.0\n",
      "iteration 4415 loss 2.4936904907226562, acc 31.25\n",
      "iteration 4416 loss 2.532827615737915, acc 25.0\n",
      "iteration 4417 loss 2.807093620300293, acc 21.875\n",
      "iteration 4418 loss 2.64854097366333, acc 26.5625\n",
      "iteration 4419 loss 2.57000470161438, acc 23.4375\n",
      "iteration 4420 loss 2.885000467300415, acc 17.1875\n",
      "iteration 4421 loss 2.654078483581543, acc 25.0\n",
      "iteration 4422 loss 2.592352867126465, acc 23.4375\n",
      "iteration 4423 loss 2.427053928375244, acc 29.6875\n",
      "iteration 4424 loss 2.819952964782715, acc 17.1875\n",
      "iteration 4425 loss 2.7153992652893066, acc 20.3125\n",
      "iteration 4426 loss 2.8297083377838135, acc 26.5625\n",
      "iteration 4427 loss 2.759063243865967, acc 23.4375\n",
      "iteration 4428 loss 2.6168692111968994, acc 18.75\n",
      "iteration 4429 loss 2.5647823810577393, acc 26.5625\n",
      "iteration 4430 loss 2.8065056800842285, acc 20.3125\n",
      "iteration 4431 loss 2.8696300983428955, acc 17.1875\n",
      "iteration 4432 loss 2.5067389011383057, acc 25.0\n",
      "iteration 4433 loss 2.650506019592285, acc 23.4375\n",
      "iteration 4434 loss 2.538520336151123, acc 28.125\n",
      "iteration 4435 loss 2.678940534591675, acc 17.1875\n",
      "iteration 4436 loss 2.791665554046631, acc 17.1875\n",
      "iteration 4437 loss 2.617626428604126, acc 28.125\n",
      "iteration 4438 loss 2.7149267196655273, acc 20.3125\n",
      "iteration 4439 loss 3.011509895324707, acc 7.8125\n",
      "iteration 4440 loss 2.8133163452148438, acc 14.0625\n",
      "iteration 4441 loss 2.72208309173584, acc 25.0\n",
      "iteration 4442 loss 2.6128742694854736, acc 21.875\n",
      "iteration 4443 loss 2.7074129581451416, acc 17.1875\n",
      "iteration 4444 loss 2.940507173538208, acc 14.0625\n",
      "iteration 4445 loss 2.774885892868042, acc 17.1875\n",
      "iteration 4446 loss 2.6653261184692383, acc 20.3125\n",
      "iteration 4447 loss 2.7860467433929443, acc 23.4375\n",
      "iteration 4448 loss 2.666330099105835, acc 17.1875\n",
      "iteration 4449 loss 2.784137010574341, acc 21.875\n",
      "iteration 4450 loss 2.560619354248047, acc 26.5625\n",
      "iteration 4451 loss 2.837960720062256, acc 17.1875\n",
      "iteration 4452 loss 2.6820521354675293, acc 23.4375\n",
      "iteration 4453 loss 2.7498764991760254, acc 20.3125\n",
      "iteration 4454 loss 2.83385968208313, acc 17.1875\n",
      "iteration 4455 loss 2.91945743560791, acc 17.1875\n",
      "iteration 4456 loss 2.561282157897949, acc 26.5625\n",
      "iteration 4457 loss 2.763277053833008, acc 20.3125\n",
      "iteration 4458 loss 2.710878610610962, acc 20.3125\n",
      "iteration 4459 loss 2.897643566131592, acc 12.5\n",
      "iteration 4460 loss 2.494779109954834, acc 34.375\n",
      "iteration 4461 loss 2.6835451126098633, acc 21.875\n",
      "iteration 4462 loss 2.7057442665100098, acc 25.0\n",
      "iteration 4463 loss 2.9475889205932617, acc 15.625\n",
      "iteration 4464 loss 2.6177127361297607, acc 25.0\n",
      "iteration 4465 loss 2.759324073791504, acc 20.3125\n",
      "iteration 4466 loss 2.859727621078491, acc 21.875\n",
      "iteration 4467 loss 2.6069769859313965, acc 29.6875\n",
      "iteration 4468 loss 2.6293630599975586, acc 28.125\n",
      "iteration 4469 loss 2.550037145614624, acc 35.9375\n",
      "iteration 4470 loss 2.5321879386901855, acc 20.3125\n",
      "iteration 4471 loss 2.669236660003662, acc 23.4375\n",
      "iteration 4472 loss 2.7358031272888184, acc 21.875\n",
      "iteration 4473 loss 2.6317954063415527, acc 23.4375\n",
      "iteration 4474 loss 2.5130772590637207, acc 25.0\n",
      "iteration 4475 loss 2.488123655319214, acc 28.125\n",
      "iteration 4476 loss 2.7674643993377686, acc 20.3125\n",
      "iteration 4477 loss 2.718290328979492, acc 20.3125\n",
      "iteration 4478 loss 2.6763110160827637, acc 25.0\n",
      "iteration 4479 loss 2.6458237171173096, acc 20.3125\n",
      "iteration 4480 loss 2.56626296043396, acc 21.875\n",
      "iteration 4481 loss 2.5085952281951904, acc 26.5625\n",
      "iteration 4482 loss 2.8168489933013916, acc 15.625\n",
      "iteration 4483 loss 2.556889295578003, acc 26.5625\n",
      "iteration 4484 loss 2.9741365909576416, acc 15.625\n",
      "iteration 4485 loss 2.709653615951538, acc 14.0625\n",
      "iteration 4486 loss 2.6170477867126465, acc 29.6875\n",
      "iteration 4487 loss 2.6802196502685547, acc 21.875\n",
      "iteration 4488 loss 2.7478713989257812, acc 18.75\n",
      "iteration 4489 loss 2.6637656688690186, acc 23.4375\n",
      "iteration 4490 loss 2.8407092094421387, acc 17.1875\n",
      "iteration 4491 loss 2.3630659580230713, acc 32.8125\n",
      "iteration 4492 loss 3.0131335258483887, acc 12.5\n",
      "iteration 4493 loss 2.665135383605957, acc 18.75\n",
      "iteration 4494 loss 2.5210728645324707, acc 28.125\n",
      "iteration 4495 loss 2.5240237712860107, acc 23.4375\n",
      "iteration 4496 loss 2.918644666671753, acc 20.3125\n",
      "iteration 4497 loss 2.60768723487854, acc 21.875\n",
      "iteration 4498 loss 2.7172930240631104, acc 23.4375\n",
      "iteration 4499 loss 2.815850019454956, acc 15.625\n",
      "iteration 4500 loss 2.779881715774536, acc 23.4375\n",
      "iteration 4501 loss 2.6430983543395996, acc 25.0\n",
      "iteration 4502 loss 2.5092825889587402, acc 29.6875\n",
      "iteration 4503 loss 2.6732144355773926, acc 25.0\n",
      "iteration 4504 loss 2.5257303714752197, acc 21.875\n",
      "iteration 4505 loss 2.524840831756592, acc 34.375\n",
      "iteration 4506 loss 2.734431505203247, acc 17.1875\n",
      "iteration 4507 loss 2.841966152191162, acc 23.4375\n",
      "iteration 4508 loss 2.739932060241699, acc 21.875\n",
      "iteration 4509 loss 2.6973705291748047, acc 18.75\n",
      "iteration 4510 loss 2.7740015983581543, acc 17.1875\n",
      "iteration 4511 loss 2.80698299407959, acc 18.75\n",
      "iteration 4512 loss 2.569122076034546, acc 20.3125\n",
      "iteration 4513 loss 2.5774521827697754, acc 18.75\n",
      "iteration 4514 loss 2.5587809085845947, acc 18.75\n",
      "iteration 4515 loss 2.5740139484405518, acc 15.625\n",
      "iteration 4516 loss 2.68157958984375, acc 25.0\n",
      "iteration 4517 loss 2.514974355697632, acc 23.4375\n",
      "iteration 4518 loss 2.7098329067230225, acc 21.875\n",
      "iteration 4519 loss 2.7805185317993164, acc 18.75\n",
      "iteration 4520 loss 2.535641670227051, acc 25.0\n",
      "iteration 4521 loss 2.534485101699829, acc 28.125\n",
      "iteration 4522 loss 2.633079767227173, acc 23.4375\n",
      "iteration 4523 loss 2.691763401031494, acc 21.875\n",
      "iteration 4524 loss 2.829172134399414, acc 20.3125\n",
      "iteration 4525 loss 2.576355218887329, acc 25.0\n",
      "iteration 4526 loss 2.8902158737182617, acc 14.0625\n",
      "iteration 4527 loss 2.8467681407928467, acc 26.5625\n",
      "iteration 4528 loss 2.648952007293701, acc 21.875\n",
      "iteration 4529 loss 2.702719211578369, acc 18.75\n",
      "iteration 4530 loss 2.5985498428344727, acc 26.5625\n",
      "iteration 4531 loss 2.7714743614196777, acc 23.4375\n",
      "iteration 4532 loss 2.861693859100342, acc 21.875\n",
      "iteration 4533 loss 2.7547595500946045, acc 23.4375\n",
      "iteration 4534 loss 2.631084442138672, acc 20.3125\n",
      "iteration 4535 loss 2.7167298793792725, acc 15.625\n",
      "iteration 4536 loss 2.7046797275543213, acc 21.875\n",
      "iteration 4537 loss 2.8075106143951416, acc 17.1875\n",
      "iteration 4538 loss 2.6593997478485107, acc 25.0\n",
      "iteration 4539 loss 2.5086612701416016, acc 29.6875\n",
      "iteration 4540 loss 2.443671703338623, acc 29.6875\n",
      "iteration 4541 loss 2.712761402130127, acc 20.3125\n",
      "iteration 4542 loss 2.6292884349823, acc 28.125\n",
      "iteration 4543 loss 2.4493558406829834, acc 32.8125\n",
      "iteration 4544 loss 3.082707405090332, acc 7.8125\n",
      "iteration 4545 loss 2.502455949783325, acc 35.9375\n",
      "iteration 4546 loss 2.436704635620117, acc 25.0\n",
      "iteration 4547 loss 2.7590699195861816, acc 18.75\n",
      "iteration 4548 loss 2.8246426582336426, acc 20.3125\n",
      "iteration 4549 loss 2.8139050006866455, acc 18.75\n",
      "iteration 4550 loss 2.9359824657440186, acc 10.9375\n",
      "iteration 4551 loss 2.7140560150146484, acc 15.625\n",
      "iteration 4552 loss 2.616486072540283, acc 17.1875\n",
      "iteration 4553 loss 2.6920101642608643, acc 17.1875\n",
      "iteration 4554 loss 2.6854772567749023, acc 28.125\n",
      "iteration 4555 loss 2.6344752311706543, acc 21.875\n",
      "iteration 4556 loss 2.719770669937134, acc 18.75\n",
      "iteration 4557 loss 2.8822226524353027, acc 17.1875\n",
      "iteration 4558 loss 2.535083293914795, acc 25.0\n",
      "iteration 4559 loss 2.6692867279052734, acc 18.75\n",
      "iteration 4560 loss 2.5894017219543457, acc 23.4375\n",
      "iteration 4561 loss 2.688412666320801, acc 15.625\n",
      "iteration 4562 loss 2.575071096420288, acc 32.8125\n",
      "iteration 4563 loss 2.681668281555176, acc 18.75\n",
      "iteration 4564 loss 2.691572904586792, acc 25.0\n",
      "iteration 4565 loss 2.5323946475982666, acc 28.125\n",
      "iteration 4566 loss 2.775129795074463, acc 18.75\n",
      "iteration 4567 loss 2.4635603427886963, acc 29.6875\n",
      "iteration 4568 loss 2.760173797607422, acc 18.75\n",
      "iteration 4569 loss 2.887481212615967, acc 12.5\n",
      "iteration 4570 loss 2.907792091369629, acc 17.1875\n",
      "iteration 4571 loss 2.6674365997314453, acc 21.875\n",
      "iteration 4572 loss 2.8154850006103516, acc 23.4375\n",
      "iteration 4573 loss 2.682040214538574, acc 25.0\n",
      "iteration 4574 loss 2.6848738193511963, acc 23.4375\n",
      "iteration 4575 loss 2.5434012413024902, acc 26.5625\n",
      "iteration 4576 loss 2.770822525024414, acc 15.625\n",
      "iteration 4577 loss 2.7207629680633545, acc 23.4375\n",
      "iteration 4578 loss 2.821486473083496, acc 20.3125\n",
      "iteration 4579 loss 2.6442737579345703, acc 23.4375\n",
      "iteration 4580 loss 2.4426236152648926, acc 34.375\n",
      "iteration 4581 loss 2.9082553386688232, acc 14.0625\n",
      "iteration 4582 loss 2.5794880390167236, acc 26.5625\n",
      "iteration 4583 loss 2.772768974304199, acc 21.875\n",
      "iteration 4584 loss 2.5076487064361572, acc 32.8125\n",
      "iteration 4585 loss 2.7911460399627686, acc 20.3125\n",
      "iteration 4586 loss 2.7574105262756348, acc 21.875\n",
      "iteration 4587 loss 2.635723114013672, acc 20.3125\n",
      "iteration 4588 loss 2.6909825801849365, acc 23.4375\n",
      "iteration 4589 loss 2.736009120941162, acc 18.75\n",
      "iteration 4590 loss 2.6766064167022705, acc 31.25\n",
      "iteration 4591 loss 2.5438930988311768, acc 31.25\n",
      "iteration 4592 loss 2.5425243377685547, acc 31.25\n",
      "iteration 4593 loss 2.667038917541504, acc 21.875\n",
      "iteration 4594 loss 2.9227049350738525, acc 17.1875\n",
      "iteration 4595 loss 2.6287567615509033, acc 26.5625\n",
      "iteration 4596 loss 2.601640224456787, acc 28.125\n",
      "iteration 4597 loss 2.3658924102783203, acc 34.375\n",
      "iteration 4598 loss 2.6270532608032227, acc 23.4375\n",
      "iteration 4599 loss 2.7284343242645264, acc 17.1875\n",
      "iteration 4600 loss 2.6715850830078125, acc 21.875\n",
      "iteration 4601 loss 2.846774101257324, acc 21.875\n",
      "iteration 4602 loss 2.4355578422546387, acc 23.4375\n",
      "iteration 4603 loss 2.754056453704834, acc 18.75\n",
      "iteration 4604 loss 2.795764684677124, acc 20.3125\n",
      "iteration 4605 loss 2.5883359909057617, acc 17.1875\n",
      "iteration 4606 loss 2.784575939178467, acc 17.1875\n",
      "iteration 4607 loss 2.757275342941284, acc 15.625\n",
      "iteration 4608 loss 2.720647096633911, acc 20.3125\n",
      "iteration 4609 loss 2.794863700866699, acc 14.0625\n",
      "iteration 4610 loss 2.6896140575408936, acc 17.1875\n",
      "iteration 4611 loss 2.648165225982666, acc 25.0\n",
      "iteration 4612 loss 2.6258955001831055, acc 26.5625\n",
      "iteration 4613 loss 2.5999345779418945, acc 25.0\n",
      "iteration 4614 loss 2.5770797729492188, acc 23.4375\n",
      "iteration 4615 loss 2.5850212574005127, acc 18.75\n",
      "iteration 4616 loss 2.5603175163269043, acc 29.6875\n",
      "iteration 4617 loss 2.703230857849121, acc 17.1875\n",
      "iteration 4618 loss 2.7102127075195312, acc 18.75\n",
      "iteration 4619 loss 2.53462290763855, acc 26.5625\n",
      "iteration 4620 loss 2.480140447616577, acc 21.875\n",
      "iteration 4621 loss 2.71087646484375, acc 20.3125\n",
      "iteration 4622 loss 2.892204999923706, acc 20.3125\n",
      "iteration 4623 loss 2.622262716293335, acc 20.3125\n",
      "iteration 4624 loss 2.5942184925079346, acc 20.3125\n",
      "iteration 4625 loss 2.8513031005859375, acc 25.0\n",
      "iteration 4626 loss 2.7192935943603516, acc 23.4375\n",
      "iteration 4627 loss 2.8273918628692627, acc 15.625\n",
      "iteration 4628 loss 2.816800594329834, acc 18.75\n",
      "iteration 4629 loss 2.611179828643799, acc 25.0\n",
      "iteration 4630 loss 2.6011385917663574, acc 20.3125\n",
      "iteration 4631 loss 2.7281012535095215, acc 15.625\n",
      "iteration 4632 loss 2.7201032638549805, acc 20.3125\n",
      "iteration 4633 loss 2.7792062759399414, acc 25.0\n",
      "iteration 4634 loss 2.787902355194092, acc 26.5625\n",
      "iteration 4635 loss 2.8683223724365234, acc 14.0625\n",
      "iteration 4636 loss 2.6163556575775146, acc 18.75\n",
      "iteration 4637 loss 2.440319776535034, acc 29.6875\n",
      "iteration 4638 loss 2.6829352378845215, acc 18.75\n",
      "iteration 4639 loss 2.5433509349823, acc 26.5625\n",
      "iteration 4640 loss 2.808685064315796, acc 25.0\n",
      "iteration 4641 loss 2.5737357139587402, acc 26.5625\n",
      "iteration 4642 loss 2.91068434715271, acc 12.5\n",
      "iteration 4643 loss 2.488269329071045, acc 31.25\n",
      "iteration 4644 loss 2.617251396179199, acc 20.3125\n",
      "iteration 4645 loss 2.652071475982666, acc 25.0\n",
      "iteration 4646 loss 2.8674569129943848, acc 20.3125\n",
      "iteration 4647 loss 2.7658443450927734, acc 17.1875\n",
      "iteration 4648 loss 2.702826738357544, acc 20.3125\n",
      "iteration 4649 loss 2.601811170578003, acc 25.0\n",
      "iteration 4650 loss 2.6928164958953857, acc 28.125\n",
      "iteration 4651 loss 2.8132755756378174, acc 18.75\n",
      "iteration 4652 loss 2.8276195526123047, acc 17.1875\n",
      "iteration 4653 loss 2.758850574493408, acc 21.875\n",
      "iteration 4654 loss 2.778423309326172, acc 20.3125\n",
      "iteration 4655 loss 2.519838333129883, acc 28.125\n",
      "iteration 4656 loss 2.6689469814300537, acc 20.3125\n",
      "iteration 4657 loss 2.5187346935272217, acc 25.0\n",
      "iteration 4658 loss 2.4252331256866455, acc 25.0\n",
      "iteration 4659 loss 2.8245930671691895, acc 17.1875\n",
      "iteration 4660 loss 2.908188819885254, acc 14.0625\n",
      "iteration 4661 loss 2.6768665313720703, acc 23.4375\n",
      "iteration 4662 loss 2.6483664512634277, acc 26.5625\n",
      "iteration 4663 loss 2.4313623905181885, acc 34.375\n",
      "iteration 4664 loss 2.8238003253936768, acc 20.3125\n",
      "iteration 4665 loss 2.8591134548187256, acc 15.625\n",
      "iteration 4666 loss 2.770066499710083, acc 12.5\n",
      "iteration 4667 loss 2.7359676361083984, acc 23.4375\n",
      "iteration 4668 loss 2.700653076171875, acc 17.1875\n",
      "iteration 4669 loss 2.877861976623535, acc 20.3125\n",
      "iteration 4670 loss 2.7497096061706543, acc 15.625\n",
      "iteration 4671 loss 2.7100257873535156, acc 18.75\n",
      "iteration 4672 loss 2.5425865650177, acc 32.8125\n",
      "iteration 4673 loss 2.974358558654785, acc 14.0625\n",
      "iteration 4674 loss 2.760225772857666, acc 18.75\n",
      "iteration 4675 loss 2.6904196739196777, acc 23.4375\n",
      "iteration 4676 loss 2.6288249492645264, acc 26.5625\n",
      "iteration 4677 loss 2.831575632095337, acc 18.75\n",
      "iteration 4678 loss 2.7724008560180664, acc 28.125\n",
      "iteration 4679 loss 2.8272159099578857, acc 14.0625\n",
      "iteration 4680 loss 2.5627405643463135, acc 21.875\n",
      "iteration 4681 loss 2.7228219509124756, acc 15.625\n",
      "iteration 4682 loss 2.54412841796875, acc 31.25\n",
      "iteration 4683 loss 2.7923526763916016, acc 18.75\n",
      "iteration 4684 loss 2.756024122238159, acc 25.0\n",
      "iteration 4685 loss 2.762441635131836, acc 14.0625\n",
      "iteration 4686 loss 2.641869306564331, acc 25.0\n",
      "iteration 4687 loss 2.6815030574798584, acc 21.875\n",
      "iteration 4688 loss 2.6066579818725586, acc 28.125\n",
      "iteration 4689 loss 2.571596145629883, acc 23.4375\n",
      "iteration 4690 loss 2.7909348011016846, acc 15.625\n",
      "iteration 4691 loss 2.8573226928710938, acc 25.0\n",
      "iteration 4692 loss 2.531599521636963, acc 26.5625\n",
      "iteration 4693 loss 2.7576770782470703, acc 14.0625\n",
      "iteration 4694 loss 2.5630037784576416, acc 25.0\n",
      "iteration 4695 loss 2.6927716732025146, acc 21.875\n",
      "iteration 4696 loss 2.770678758621216, acc 18.75\n",
      "iteration 4697 loss 2.6622965335845947, acc 20.3125\n",
      "iteration 4698 loss 2.6186130046844482, acc 28.125\n",
      "iteration 4699 loss 2.817204475402832, acc 15.625\n",
      "iteration 4700 loss 2.636291265487671, acc 26.5625\n",
      "iteration 4701 loss 2.647763967514038, acc 21.875\n",
      "iteration 4702 loss 2.626235008239746, acc 26.5625\n",
      "iteration 4703 loss 2.598782777786255, acc 28.125\n",
      "iteration 4704 loss 2.6411221027374268, acc 26.5625\n",
      "iteration 4705 loss 2.899465560913086, acc 15.625\n",
      "iteration 4706 loss 2.9272687435150146, acc 15.625\n",
      "iteration 4707 loss 2.5874884128570557, acc 25.0\n",
      "iteration 4708 loss 2.8646786212921143, acc 23.4375\n",
      "iteration 4709 loss 2.8595471382141113, acc 12.5\n",
      "iteration 4710 loss 3.0046985149383545, acc 12.5\n",
      "iteration 4711 loss 2.6881070137023926, acc 28.125\n",
      "iteration 4712 loss 2.5843594074249268, acc 28.125\n",
      "iteration 4713 loss 2.8283767700195312, acc 15.625\n",
      "iteration 4714 loss 2.5717613697052, acc 20.3125\n",
      "iteration 4715 loss 2.5154271125793457, acc 34.375\n",
      "iteration 4716 loss 2.48232102394104, acc 31.25\n",
      "iteration 4717 loss 2.9156453609466553, acc 14.0625\n",
      "iteration 4718 loss 2.590336799621582, acc 21.875\n",
      "iteration 4719 loss 2.5093183517456055, acc 21.875\n",
      "iteration 4720 loss 2.644665002822876, acc 31.25\n",
      "iteration 4721 loss 2.6454050540924072, acc 23.4375\n",
      "iteration 4722 loss 2.822990655899048, acc 10.9375\n",
      "iteration 4723 loss 2.5533671379089355, acc 25.0\n",
      "iteration 4724 loss 2.8686771392822266, acc 15.625\n",
      "iteration 4725 loss 2.6427459716796875, acc 20.3125\n",
      "iteration 4726 loss 2.8117616176605225, acc 20.3125\n",
      "iteration 4727 loss 2.5240681171417236, acc 21.875\n",
      "iteration 4728 loss 2.625089406967163, acc 23.4375\n",
      "iteration 4729 loss 2.763883113861084, acc 18.75\n",
      "iteration 4730 loss 2.67872953414917, acc 18.75\n",
      "iteration 4731 loss 2.705472946166992, acc 21.875\n",
      "iteration 4732 loss 2.6639113426208496, acc 17.1875\n",
      "iteration 4733 loss 2.674123525619507, acc 20.3125\n",
      "iteration 4734 loss 2.622011661529541, acc 17.1875\n",
      "iteration 4735 loss 2.5718226432800293, acc 29.6875\n",
      "iteration 4736 loss 2.890162467956543, acc 25.0\n",
      "iteration 4737 loss 2.514313220977783, acc 29.6875\n",
      "iteration 4738 loss 2.754467248916626, acc 21.875\n",
      "iteration 4739 loss 2.717660665512085, acc 20.3125\n",
      "iteration 4740 loss 2.908236026763916, acc 7.8125\n",
      "iteration 4741 loss 2.8514604568481445, acc 12.5\n",
      "iteration 4742 loss 2.5380001068115234, acc 18.75\n",
      "iteration 4743 loss 2.910806655883789, acc 12.5\n",
      "iteration 4744 loss 2.4945735931396484, acc 25.0\n",
      "iteration 4745 loss 2.6459577083587646, acc 25.0\n",
      "iteration 4746 loss 2.684509754180908, acc 23.4375\n",
      "iteration 4747 loss 2.725271224975586, acc 17.1875\n",
      "iteration 4748 loss 2.7795305252075195, acc 18.75\n",
      "iteration 4749 loss 2.6183853149414062, acc 26.5625\n",
      "iteration 4750 loss 2.7114157676696777, acc 20.3125\n",
      "iteration 4751 loss 2.6954903602600098, acc 18.75\n",
      "iteration 4752 loss 2.798715591430664, acc 25.0\n",
      "iteration 4753 loss 2.607128143310547, acc 23.4375\n",
      "iteration 4754 loss 2.782200574874878, acc 17.1875\n",
      "iteration 4755 loss 2.5727603435516357, acc 31.25\n",
      "iteration 4756 loss 2.6698269844055176, acc 23.4375\n",
      "iteration 4757 loss 2.6247200965881348, acc 21.875\n",
      "iteration 4758 loss 2.4825963973999023, acc 34.375\n",
      "iteration 4759 loss 2.7491607666015625, acc 23.4375\n",
      "iteration 4760 loss 2.572942018508911, acc 29.6875\n",
      "iteration 4761 loss 2.873115062713623, acc 20.3125\n",
      "iteration 4762 loss 2.664177179336548, acc 21.875\n",
      "iteration 4763 loss 2.5221359729766846, acc 28.125\n",
      "iteration 4764 loss 2.5289688110351562, acc 29.6875\n",
      "iteration 4765 loss 2.5074572563171387, acc 29.6875\n",
      "iteration 4766 loss 2.77921986579895, acc 25.0\n",
      "iteration 4767 loss 2.5383949279785156, acc 29.6875\n",
      "iteration 4768 loss 2.629460573196411, acc 29.6875\n",
      "iteration 4769 loss 2.4233102798461914, acc 31.25\n",
      "iteration 4770 loss 2.858091354370117, acc 15.625\n",
      "iteration 4771 loss 2.5773818492889404, acc 17.1875\n",
      "iteration 4772 loss 2.722118616104126, acc 20.3125\n",
      "iteration 4773 loss 2.791733980178833, acc 18.75\n",
      "iteration 4774 loss 2.727409601211548, acc 12.5\n",
      "iteration 4775 loss 2.743215560913086, acc 23.4375\n",
      "iteration 4776 loss 2.6681830883026123, acc 15.625\n",
      "iteration 4777 loss 2.8624112606048584, acc 12.5\n",
      "iteration 4778 loss 2.66298508644104, acc 29.6875\n",
      "iteration 4779 loss 2.663729429244995, acc 17.1875\n",
      "iteration 4780 loss 2.6148924827575684, acc 17.1875\n",
      "iteration 4781 loss 2.6394920349121094, acc 25.0\n",
      "iteration 4782 loss 2.482036590576172, acc 26.5625\n",
      "iteration 4783 loss 2.866945505142212, acc 17.1875\n",
      "iteration 4784 loss 2.6260032653808594, acc 20.3125\n",
      "iteration 4785 loss 2.5104923248291016, acc 28.125\n",
      "iteration 4786 loss 2.5292351245880127, acc 29.6875\n",
      "iteration 4787 loss 2.911214590072632, acc 14.0625\n",
      "iteration 4788 loss 2.8567733764648438, acc 18.75\n",
      "iteration 4789 loss 2.734952688217163, acc 18.75\n",
      "iteration 4790 loss 2.595916986465454, acc 25.0\n",
      "iteration 4791 loss 2.7034084796905518, acc 9.375\n",
      "iteration 4792 loss 2.7772433757781982, acc 25.0\n",
      "iteration 4793 loss 2.5517468452453613, acc 32.8125\n",
      "iteration 4794 loss 2.7257094383239746, acc 21.875\n",
      "iteration 4795 loss 2.92476224899292, acc 14.0625\n",
      "iteration 4796 loss 2.654052972793579, acc 26.5625\n",
      "iteration 4797 loss 2.751239776611328, acc 23.4375\n",
      "iteration 4798 loss 2.615581750869751, acc 25.0\n",
      "iteration 4799 loss 2.5250768661499023, acc 28.125\n",
      "iteration 4800 loss 2.4788382053375244, acc 26.5625\n",
      "iteration 4801 loss 2.7663769721984863, acc 21.875\n",
      "iteration 4802 loss 2.6812708377838135, acc 20.3125\n",
      "iteration 4803 loss 2.718491792678833, acc 23.4375\n",
      "iteration 4804 loss 2.5252606868743896, acc 35.9375\n",
      "iteration 4805 loss 2.531299114227295, acc 26.5625\n",
      "iteration 4806 loss 2.683879852294922, acc 23.4375\n",
      "iteration 4807 loss 2.5733652114868164, acc 23.4375\n",
      "iteration 4808 loss 2.760075807571411, acc 20.3125\n",
      "iteration 4809 loss 2.661996841430664, acc 23.4375\n",
      "iteration 4810 loss 2.775831937789917, acc 18.75\n",
      "iteration 4811 loss 2.5984690189361572, acc 21.875\n",
      "iteration 4812 loss 2.81152606010437, acc 14.0625\n",
      "iteration 4813 loss 2.93566632270813, acc 17.1875\n",
      "iteration 4814 loss 2.546110153198242, acc 25.0\n",
      "iteration 4815 loss 2.6967079639434814, acc 21.875\n",
      "iteration 4816 loss 2.8333535194396973, acc 20.3125\n",
      "iteration 4817 loss 2.764524221420288, acc 12.5\n",
      "iteration 4818 loss 2.5605690479278564, acc 26.5625\n",
      "iteration 4819 loss 2.788001298904419, acc 20.3125\n",
      "iteration 4820 loss 2.559924364089966, acc 29.6875\n",
      "iteration 4821 loss 2.859483480453491, acc 15.625\n",
      "iteration 4822 loss 2.546365976333618, acc 26.5625\n",
      "iteration 4823 loss 2.622593641281128, acc 21.875\n",
      "iteration 4824 loss 2.5242581367492676, acc 20.3125\n",
      "iteration 4825 loss 2.752009868621826, acc 21.875\n",
      "iteration 4826 loss 2.612144708633423, acc 17.1875\n",
      "iteration 4827 loss 2.7389540672302246, acc 17.1875\n",
      "iteration 4828 loss 2.7827627658843994, acc 21.875\n",
      "iteration 4829 loss 2.519665002822876, acc 28.125\n",
      "iteration 4830 loss 2.5938844680786133, acc 18.75\n",
      "iteration 4831 loss 2.9326789379119873, acc 12.5\n",
      "iteration 4832 loss 2.5786471366882324, acc 23.4375\n",
      "iteration 4833 loss 2.56935453414917, acc 28.125\n",
      "iteration 4834 loss 2.8518214225769043, acc 17.1875\n",
      "iteration 4835 loss 2.6584858894348145, acc 23.4375\n",
      "iteration 4836 loss 2.759042263031006, acc 17.1875\n",
      "iteration 4837 loss 2.646121025085449, acc 25.0\n",
      "iteration 4838 loss 2.829252004623413, acc 9.375\n",
      "iteration 4839 loss 2.653616428375244, acc 25.0\n",
      "iteration 4840 loss 2.7306294441223145, acc 23.4375\n",
      "iteration 4841 loss 2.523972511291504, acc 25.0\n",
      "iteration 4842 loss 2.7355129718780518, acc 29.6875\n",
      "iteration 4843 loss 2.6960315704345703, acc 23.4375\n",
      "iteration 4844 loss 2.723551034927368, acc 17.1875\n",
      "iteration 4845 loss 2.6902554035186768, acc 23.4375\n",
      "iteration 4846 loss 2.863748550415039, acc 12.5\n",
      "iteration 4847 loss 2.7597618103027344, acc 17.1875\n",
      "iteration 4848 loss 2.8119113445281982, acc 21.875\n",
      "iteration 4849 loss 2.726996421813965, acc 10.9375\n",
      "iteration 4850 loss 2.504366159439087, acc 26.5625\n",
      "iteration 4851 loss 2.664618968963623, acc 21.875\n",
      "iteration 4852 loss 2.8034772872924805, acc 17.1875\n",
      "iteration 4853 loss 2.7839293479919434, acc 14.0625\n",
      "iteration 4854 loss 2.6419365406036377, acc 18.75\n",
      "iteration 4855 loss 2.596007823944092, acc 26.5625\n",
      "iteration 4856 loss 2.708514928817749, acc 23.4375\n",
      "iteration 4857 loss 2.829460620880127, acc 23.4375\n",
      "iteration 4858 loss 2.704892635345459, acc 23.4375\n",
      "iteration 4859 loss 2.5624916553497314, acc 29.6875\n",
      "iteration 4860 loss 2.713061571121216, acc 18.75\n",
      "iteration 4861 loss 2.836354970932007, acc 17.1875\n",
      "iteration 4862 loss 2.747525930404663, acc 26.5625\n",
      "iteration 4863 loss 2.7622499465942383, acc 14.0625\n",
      "iteration 4864 loss 2.69581937789917, acc 26.5625\n",
      "iteration 4865 loss 2.514374017715454, acc 29.6875\n",
      "iteration 4866 loss 2.815525770187378, acc 20.3125\n",
      "iteration 4867 loss 2.4524381160736084, acc 28.125\n",
      "iteration 4868 loss 2.6825127601623535, acc 23.4375\n",
      "iteration 4869 loss 2.701014995574951, acc 26.5625\n",
      "iteration 4870 loss 2.651768207550049, acc 20.3125\n",
      "iteration 4871 loss 2.61879301071167, acc 26.5625\n",
      "iteration 4872 loss 2.696166753768921, acc 20.3125\n",
      "iteration 4873 loss 2.6620285511016846, acc 25.0\n",
      "iteration 4874 loss 2.9510657787323, acc 14.0625\n",
      "iteration 4875 loss 2.444990396499634, acc 29.6875\n",
      "iteration 4876 loss 2.7057220935821533, acc 21.875\n",
      "iteration 4877 loss 2.5849883556365967, acc 23.4375\n",
      "iteration 4878 loss 2.643719434738159, acc 23.4375\n",
      "iteration 4879 loss 2.476724624633789, acc 31.25\n",
      "iteration 4880 loss 2.624913454055786, acc 26.5625\n",
      "iteration 4881 loss 2.7323801517486572, acc 20.3125\n",
      "iteration 4882 loss 2.691371202468872, acc 20.3125\n",
      "iteration 4883 loss 2.7596664428710938, acc 15.625\n",
      "iteration 4884 loss 3.0087461471557617, acc 18.75\n",
      "iteration 4885 loss 2.881742238998413, acc 20.3125\n",
      "iteration 4886 loss 2.5482065677642822, acc 31.25\n",
      "iteration 4887 loss 2.808861017227173, acc 23.4375\n",
      "iteration 4888 loss 2.863957405090332, acc 12.5\n",
      "iteration 4889 loss 2.673199415206909, acc 18.75\n",
      "iteration 4890 loss 2.631108522415161, acc 12.5\n",
      "iteration 4891 loss 2.5610008239746094, acc 29.6875\n",
      "iteration 4892 loss 2.6385159492492676, acc 15.625\n",
      "iteration 4893 loss 2.7171308994293213, acc 12.5\n",
      "iteration 4894 loss 2.6793227195739746, acc 23.4375\n",
      "iteration 4895 loss 2.7155425548553467, acc 17.1875\n",
      "iteration 4896 loss 2.7565078735351562, acc 23.4375\n",
      "iteration 4897 loss 2.782949924468994, acc 25.0\n",
      "iteration 4898 loss 2.8652400970458984, acc 17.1875\n",
      "iteration 4899 loss 2.783169746398926, acc 15.625\n",
      "iteration 4900 loss 2.7676210403442383, acc 18.75\n",
      "iteration 4901 loss 2.5226807594299316, acc 28.125\n",
      "iteration 4902 loss 2.72281813621521, acc 23.4375\n",
      "iteration 4903 loss 2.813143491744995, acc 17.1875\n",
      "iteration 4904 loss 2.6739675998687744, acc 25.0\n",
      "iteration 4905 loss 2.724778175354004, acc 18.75\n",
      "iteration 4906 loss 2.712919235229492, acc 21.875\n",
      "iteration 4907 loss 2.5143518447875977, acc 35.9375\n",
      "iteration 4908 loss 2.6309781074523926, acc 21.875\n",
      "iteration 4909 loss 2.526620626449585, acc 31.25\n",
      "iteration 4910 loss 2.8586394786834717, acc 21.875\n",
      "iteration 4911 loss 2.865104913711548, acc 15.625\n",
      "iteration 4912 loss 2.695237636566162, acc 17.1875\n",
      "iteration 4913 loss 2.7359397411346436, acc 18.75\n",
      "iteration 4914 loss 2.8286030292510986, acc 23.4375\n",
      "iteration 4915 loss 2.662353754043579, acc 21.875\n",
      "iteration 4916 loss 2.797299861907959, acc 18.75\n",
      "iteration 4917 loss 2.7524349689483643, acc 23.4375\n",
      "iteration 4918 loss 2.834763526916504, acc 17.1875\n",
      "iteration 4919 loss 2.6649346351623535, acc 23.4375\n",
      "iteration 4920 loss 2.868671178817749, acc 17.1875\n",
      "iteration 4921 loss 2.7356197834014893, acc 18.75\n",
      "iteration 4922 loss 2.6120994091033936, acc 25.0\n",
      "iteration 4923 loss 2.6964094638824463, acc 29.6875\n",
      "iteration 4924 loss 2.5889956951141357, acc 18.75\n",
      "iteration 4925 loss 2.7446587085723877, acc 18.75\n",
      "iteration 4926 loss 2.713632822036743, acc 18.75\n",
      "iteration 4927 loss 2.5100793838500977, acc 28.125\n",
      "iteration 4928 loss 2.706312656402588, acc 21.875\n",
      "iteration 4929 loss 2.7163760662078857, acc 20.3125\n",
      "iteration 4930 loss 2.7730250358581543, acc 15.625\n",
      "iteration 4931 loss 2.728148937225342, acc 18.75\n",
      "iteration 4932 loss 2.527082920074463, acc 25.0\n",
      "iteration 4933 loss 2.7590954303741455, acc 17.1875\n",
      "iteration 4934 loss 2.7120723724365234, acc 17.1875\n",
      "iteration 4935 loss 2.8542158603668213, acc 21.875\n",
      "iteration 4936 loss 2.53017520904541, acc 26.5625\n",
      "iteration 4937 loss 2.7957651615142822, acc 17.1875\n",
      "iteration 4938 loss 2.703444004058838, acc 25.0\n",
      "iteration 4939 loss 2.5893075466156006, acc 20.3125\n",
      "iteration 4940 loss 2.7479703426361084, acc 18.75\n",
      "iteration 4941 loss 2.5149519443511963, acc 25.0\n",
      "iteration 4942 loss 2.746788263320923, acc 18.75\n",
      "iteration 4943 loss 2.5263431072235107, acc 21.875\n",
      "iteration 4944 loss 2.7072436809539795, acc 14.0625\n",
      "iteration 4945 loss 2.6494317054748535, acc 18.75\n",
      "iteration 4946 loss 2.655294895172119, acc 21.875\n",
      "iteration 4947 loss 2.637314796447754, acc 29.6875\n",
      "iteration 4948 loss 2.6229426860809326, acc 21.875\n",
      "iteration 4949 loss 2.7311458587646484, acc 18.75\n",
      "iteration 4950 loss 2.7694811820983887, acc 20.3125\n",
      "iteration 4951 loss 2.4588818550109863, acc 23.4375\n",
      "iteration 4952 loss 2.64688777923584, acc 21.875\n",
      "iteration 4953 loss 3.015533685684204, acc 9.375\n",
      "iteration 4954 loss 2.8104562759399414, acc 17.1875\n",
      "iteration 4955 loss 2.744033098220825, acc 15.625\n",
      "iteration 4956 loss 2.637784242630005, acc 18.75\n",
      "iteration 4957 loss 2.636784315109253, acc 18.75\n",
      "iteration 4958 loss 2.5095155239105225, acc 26.5625\n",
      "iteration 4959 loss 2.7474350929260254, acc 17.1875\n",
      "iteration 4960 loss 2.909461498260498, acc 10.9375\n",
      "iteration 4961 loss 2.7683069705963135, acc 12.5\n",
      "iteration 4962 loss 2.8095812797546387, acc 12.5\n",
      "iteration 4963 loss 2.559305429458618, acc 20.3125\n",
      "iteration 4964 loss 2.670016050338745, acc 23.4375\n",
      "iteration 4965 loss 2.600799083709717, acc 23.4375\n",
      "iteration 4966 loss 2.7185165882110596, acc 18.75\n",
      "iteration 4967 loss 2.7993977069854736, acc 20.3125\n",
      "iteration 4968 loss 2.647585868835449, acc 25.0\n",
      "iteration 4969 loss 2.752732038497925, acc 21.875\n",
      "iteration 4970 loss 2.4428679943084717, acc 28.125\n",
      "iteration 4971 loss 2.652297019958496, acc 15.625\n",
      "iteration 4972 loss 2.658897638320923, acc 31.25\n",
      "iteration 4973 loss 2.6156840324401855, acc 18.75\n",
      "iteration 4974 loss 2.836043119430542, acc 6.25\n",
      "iteration 4975 loss 2.7948415279388428, acc 17.1875\n",
      "iteration 4976 loss 2.803973436355591, acc 15.625\n",
      "iteration 4977 loss 2.673078775405884, acc 25.0\n",
      "iteration 4978 loss 2.422415256500244, acc 29.6875\n",
      "iteration 4979 loss 2.764577865600586, acc 17.1875\n",
      "iteration 4980 loss 2.581482410430908, acc 25.0\n",
      "iteration 4981 loss 2.5664877891540527, acc 32.8125\n",
      "iteration 4982 loss 2.634166717529297, acc 26.5625\n",
      "iteration 4983 loss 2.661951780319214, acc 20.3125\n",
      "iteration 4984 loss 2.720672607421875, acc 25.0\n",
      "iteration 4985 loss 2.69384765625, acc 25.0\n",
      "iteration 4986 loss 2.6118252277374268, acc 20.3125\n",
      "iteration 4987 loss 2.5577406883239746, acc 26.5625\n",
      "iteration 4988 loss 2.759838819503784, acc 17.1875\n",
      "iteration 4989 loss 2.6160905361175537, acc 20.3125\n",
      "iteration 4990 loss 2.7020132541656494, acc 23.4375\n",
      "iteration 4991 loss 2.603541851043701, acc 25.0\n",
      "iteration 4992 loss 2.5874240398406982, acc 14.0625\n",
      "iteration 4993 loss 2.665903091430664, acc 23.4375\n",
      "iteration 4994 loss 2.646820545196533, acc 23.4375\n",
      "iteration 4995 loss 2.702521324157715, acc 20.3125\n",
      "iteration 4996 loss 2.680608034133911, acc 21.875\n",
      "iteration 4997 loss 2.727648973464966, acc 23.4375\n",
      "iteration 4998 loss 2.599534511566162, acc 25.0\n",
      "iteration 4999 loss 2.544268846511841, acc 26.5625\n",
      "iteration 5000 loss 2.722975254058838, acc 21.875\n",
      "iteration 5001 loss 2.6434154510498047, acc 21.875\n",
      "iteration 5002 loss 2.7919199466705322, acc 23.4375\n",
      "iteration 5003 loss 2.620753049850464, acc 18.75\n",
      "iteration 5004 loss 2.883451461791992, acc 18.75\n",
      "iteration 5005 loss 2.7957887649536133, acc 21.875\n",
      "iteration 5006 loss 2.648130416870117, acc 18.75\n",
      "iteration 5007 loss 2.807216167449951, acc 20.3125\n",
      "iteration 5008 loss 2.504728317260742, acc 25.0\n",
      "iteration 5009 loss 2.8599493503570557, acc 10.9375\n",
      "iteration 5010 loss 2.802147388458252, acc 23.4375\n",
      "iteration 5011 loss 2.6533985137939453, acc 25.0\n",
      "iteration 5012 loss 2.8164076805114746, acc 15.625\n",
      "iteration 5013 loss 2.945725679397583, acc 14.0625\n",
      "iteration 5014 loss 2.6216297149658203, acc 23.4375\n",
      "iteration 5015 loss 2.6699395179748535, acc 21.875\n",
      "iteration 5016 loss 2.8677005767822266, acc 17.1875\n",
      "iteration 5017 loss 2.670297622680664, acc 26.5625\n",
      "iteration 5018 loss 2.6942625045776367, acc 21.875\n",
      "iteration 5019 loss 2.719562292098999, acc 25.0\n",
      "iteration 5020 loss 2.8092615604400635, acc 21.875\n",
      "iteration 5021 loss 2.4601902961730957, acc 32.8125\n",
      "iteration 5022 loss 2.734144449234009, acc 21.875\n",
      "iteration 5023 loss 2.653607130050659, acc 21.875\n",
      "iteration 5024 loss 2.866560935974121, acc 14.0625\n",
      "iteration 5025 loss 2.753835439682007, acc 15.625\n",
      "iteration 5026 loss 2.80932879447937, acc 18.75\n",
      "iteration 5027 loss 2.832061767578125, acc 20.3125\n",
      "iteration 5028 loss 2.824150800704956, acc 20.3125\n",
      "iteration 5029 loss 2.8096261024475098, acc 14.0625\n",
      "iteration 5030 loss 2.7059247493743896, acc 17.1875\n",
      "iteration 5031 loss 2.683957576751709, acc 18.75\n",
      "iteration 5032 loss 2.8985605239868164, acc 15.625\n",
      "iteration 5033 loss 2.619091749191284, acc 20.3125\n",
      "iteration 5034 loss 2.698826313018799, acc 21.875\n",
      "iteration 5035 loss 2.6073567867279053, acc 23.4375\n",
      "iteration 5036 loss 2.6636412143707275, acc 18.75\n",
      "iteration 5037 loss 2.6004960536956787, acc 20.3125\n",
      "iteration 5038 loss 2.621753454208374, acc 17.1875\n",
      "iteration 5039 loss 2.638967990875244, acc 17.1875\n",
      "iteration 5040 loss 2.765620470046997, acc 20.3125\n",
      "iteration 5041 loss 2.480844020843506, acc 21.875\n",
      "iteration 5042 loss 2.719937324523926, acc 17.1875\n",
      "iteration 5043 loss 2.7017393112182617, acc 15.625\n",
      "iteration 5044 loss 2.6431033611297607, acc 26.5625\n",
      "iteration 5045 loss 2.6670432090759277, acc 21.875\n",
      "iteration 5046 loss 2.681481122970581, acc 20.3125\n",
      "iteration 5047 loss 2.7838542461395264, acc 14.0625\n",
      "iteration 5048 loss 2.5755715370178223, acc 17.1875\n",
      "iteration 5049 loss 2.8581550121307373, acc 15.625\n",
      "iteration 5050 loss 2.675793409347534, acc 37.5\n",
      "iteration 5051 loss 2.418325185775757, acc 31.25\n",
      "iteration 5052 loss 2.722048044204712, acc 21.875\n",
      "iteration 5053 loss 2.419612169265747, acc 32.8125\n",
      "iteration 5054 loss 2.8072831630706787, acc 14.0625\n",
      "iteration 5055 loss 2.6057815551757812, acc 26.5625\n",
      "iteration 5056 loss 2.8210489749908447, acc 15.625\n",
      "iteration 5057 loss 2.666825532913208, acc 28.125\n",
      "iteration 5058 loss 2.576098680496216, acc 32.8125\n",
      "iteration 5059 loss 2.757143497467041, acc 25.0\n",
      "iteration 5060 loss 2.6863811016082764, acc 18.75\n",
      "iteration 5061 loss 2.5182650089263916, acc 25.0\n",
      "iteration 5062 loss 2.499636173248291, acc 29.6875\n",
      "iteration 5063 loss 2.421427011489868, acc 28.125\n",
      "iteration 5064 loss 2.719775676727295, acc 23.4375\n",
      "iteration 5065 loss 2.7104787826538086, acc 26.5625\n",
      "iteration 5066 loss 2.709359645843506, acc 25.0\n",
      "iteration 5067 loss 2.531087875366211, acc 28.125\n",
      "iteration 5068 loss 2.5530617237091064, acc 18.75\n",
      "iteration 5069 loss 2.9681198596954346, acc 14.0625\n",
      "iteration 5070 loss 2.7812445163726807, acc 18.75\n",
      "iteration 5071 loss 2.6008853912353516, acc 28.125\n",
      "iteration 5072 loss 2.482117176055908, acc 25.0\n",
      "iteration 5073 loss 2.4771034717559814, acc 23.4375\n",
      "iteration 5074 loss 2.791048288345337, acc 21.875\n",
      "iteration 5075 loss 2.8231725692749023, acc 26.5625\n",
      "iteration 5076 loss 2.5753023624420166, acc 20.3125\n",
      "iteration 5077 loss 2.688946485519409, acc 17.1875\n",
      "iteration 5078 loss 2.587344169616699, acc 28.125\n",
      "iteration 5079 loss 2.44199800491333, acc 31.25\n",
      "iteration 5080 loss 2.4969992637634277, acc 28.125\n",
      "iteration 5081 loss 2.7277722358703613, acc 18.75\n",
      "iteration 5082 loss 2.5791242122650146, acc 23.4375\n",
      "iteration 5083 loss 2.5389020442962646, acc 26.5625\n",
      "iteration 5084 loss 2.7898919582366943, acc 15.625\n",
      "iteration 5085 loss 2.8053197860717773, acc 21.875\n",
      "iteration 5086 loss 2.6639788150787354, acc 18.75\n",
      "iteration 5087 loss 2.7749693393707275, acc 18.75\n",
      "iteration 5088 loss 2.573237895965576, acc 28.125\n",
      "iteration 5089 loss 2.785619020462036, acc 23.4375\n",
      "iteration 5090 loss 2.7236196994781494, acc 21.875\n",
      "iteration 5091 loss 2.56565260887146, acc 25.0\n",
      "iteration 5092 loss 2.6808698177337646, acc 18.75\n",
      "iteration 5093 loss 2.863312005996704, acc 18.75\n",
      "iteration 5094 loss 2.6271421909332275, acc 21.875\n",
      "iteration 5095 loss 2.6898069381713867, acc 21.875\n",
      "iteration 5096 loss 2.5945732593536377, acc 21.875\n",
      "iteration 5097 loss 2.6251018047332764, acc 31.25\n",
      "iteration 5098 loss 2.8700952529907227, acc 20.3125\n",
      "iteration 5099 loss 2.6931915283203125, acc 18.75\n",
      "iteration 5100 loss 2.6081619262695312, acc 21.875\n",
      "iteration 5101 loss 2.8624989986419678, acc 17.1875\n",
      "iteration 5102 loss 2.655689239501953, acc 17.1875\n",
      "iteration 5103 loss 2.5858709812164307, acc 23.4375\n",
      "iteration 5104 loss 2.808152675628662, acc 18.75\n",
      "iteration 5105 loss 2.7249419689178467, acc 23.4375\n",
      "iteration 5106 loss 2.694624900817871, acc 25.0\n",
      "iteration 5107 loss 2.644841194152832, acc 21.875\n",
      "iteration 5108 loss 2.5537517070770264, acc 26.5625\n",
      "iteration 5109 loss 2.758429527282715, acc 12.5\n",
      "iteration 5110 loss 2.7434334754943848, acc 18.75\n",
      "iteration 5111 loss 2.78218936920166, acc 18.75\n",
      "iteration 5112 loss 2.795987129211426, acc 18.75\n",
      "iteration 5113 loss 2.574920177459717, acc 37.5\n",
      "iteration 5114 loss 2.871595859527588, acc 12.5\n",
      "iteration 5115 loss 2.570610761642456, acc 25.0\n",
      "iteration 5116 loss 2.907351016998291, acc 18.75\n",
      "iteration 5117 loss 2.599712610244751, acc 20.3125\n",
      "iteration 5118 loss 2.532902717590332, acc 21.875\n",
      "iteration 5119 loss 2.5718278884887695, acc 20.3125\n",
      "iteration 5120 loss 2.7337613105773926, acc 20.3125\n",
      "iteration 5121 loss 2.682255983352661, acc 20.3125\n",
      "iteration 5122 loss 2.6854488849639893, acc 25.0\n",
      "iteration 5123 loss 2.629911184310913, acc 17.1875\n",
      "iteration 5124 loss 2.75604248046875, acc 20.3125\n",
      "iteration 5125 loss 2.7100448608398438, acc 14.0625\n",
      "iteration 5126 loss 2.527194023132324, acc 26.5625\n",
      "iteration 5127 loss 2.8077290058135986, acc 17.1875\n",
      "iteration 5128 loss 2.709817409515381, acc 18.75\n",
      "iteration 5129 loss 2.752883195877075, acc 18.75\n",
      "iteration 5130 loss 2.684654712677002, acc 23.4375\n",
      "iteration 5131 loss 2.7432613372802734, acc 20.3125\n",
      "iteration 5132 loss 2.6680164337158203, acc 20.3125\n",
      "iteration 5133 loss 2.89351224899292, acc 20.3125\n",
      "iteration 5134 loss 2.7396953105926514, acc 20.3125\n",
      "iteration 5135 loss 2.86177134513855, acc 12.5\n",
      "iteration 5136 loss 2.840017795562744, acc 25.0\n",
      "iteration 5137 loss 2.7189078330993652, acc 23.4375\n",
      "iteration 5138 loss 2.735602617263794, acc 17.1875\n",
      "iteration 5139 loss 2.8671302795410156, acc 17.1875\n",
      "iteration 5140 loss 2.7655677795410156, acc 21.875\n",
      "iteration 5141 loss 2.790147304534912, acc 10.9375\n",
      "iteration 5142 loss 2.745795488357544, acc 25.0\n",
      "iteration 5143 loss 2.5730981826782227, acc 21.875\n",
      "iteration 5144 loss 2.6781771183013916, acc 21.875\n",
      "iteration 5145 loss 2.878960609436035, acc 15.625\n",
      "iteration 5146 loss 2.754277229309082, acc 18.75\n",
      "iteration 5147 loss 2.687128782272339, acc 21.875\n",
      "iteration 5148 loss 2.5511467456817627, acc 25.0\n",
      "iteration 5149 loss 2.763368844985962, acc 15.625\n",
      "iteration 5150 loss 2.691873073577881, acc 20.3125\n",
      "iteration 5151 loss 2.7209413051605225, acc 20.3125\n",
      "iteration 5152 loss 2.670504093170166, acc 17.1875\n",
      "iteration 5153 loss 2.5463526248931885, acc 25.0\n",
      "iteration 5154 loss 2.6698994636535645, acc 20.3125\n",
      "iteration 5155 loss 2.7414638996124268, acc 17.1875\n",
      "iteration 5156 loss 2.693359613418579, acc 18.75\n",
      "iteration 5157 loss 2.58140230178833, acc 21.875\n",
      "iteration 5158 loss 2.675189733505249, acc 20.3125\n",
      "iteration 5159 loss 2.5258424282073975, acc 29.6875\n",
      "iteration 5160 loss 2.847975254058838, acc 20.3125\n",
      "iteration 5161 loss 2.5337860584259033, acc 21.875\n",
      "iteration 5162 loss 2.5999386310577393, acc 18.75\n",
      "iteration 5163 loss 2.798814535140991, acc 20.3125\n",
      "iteration 5164 loss 2.6609063148498535, acc 18.75\n",
      "iteration 5165 loss 2.8053812980651855, acc 25.0\n",
      "iteration 5166 loss 2.6088709831237793, acc 25.0\n",
      "iteration 5167 loss 2.7096667289733887, acc 25.0\n",
      "iteration 5168 loss 2.681612491607666, acc 15.625\n",
      "iteration 5169 loss 2.594895839691162, acc 23.4375\n",
      "iteration 5170 loss 2.6212494373321533, acc 25.0\n",
      "iteration 5171 loss 2.9160315990448, acc 17.1875\n",
      "iteration 5172 loss 2.6946630477905273, acc 23.4375\n",
      "iteration 5173 loss 2.557344675064087, acc 29.6875\n",
      "iteration 5174 loss 2.7764439582824707, acc 20.3125\n",
      "iteration 5175 loss 2.563523292541504, acc 29.6875\n",
      "iteration 5176 loss 2.7854065895080566, acc 20.3125\n",
      "iteration 5177 loss 2.3854176998138428, acc 35.9375\n",
      "iteration 5178 loss 2.5591259002685547, acc 31.25\n",
      "iteration 5179 loss 2.690742015838623, acc 14.0625\n",
      "iteration 5180 loss 2.64961838722229, acc 21.875\n",
      "iteration 5181 loss 2.523057460784912, acc 28.125\n",
      "iteration 5182 loss 2.5700342655181885, acc 25.0\n",
      "iteration 5183 loss 2.678471326828003, acc 21.875\n",
      "iteration 5184 loss 2.58266544342041, acc 29.6875\n",
      "iteration 5185 loss 2.829275131225586, acc 15.625\n",
      "iteration 5186 loss 2.5280840396881104, acc 26.5625\n",
      "iteration 5187 loss 2.601238965988159, acc 15.625\n",
      "iteration 5188 loss 2.7965564727783203, acc 18.75\n",
      "iteration 5189 loss 2.649744749069214, acc 26.5625\n",
      "iteration 5190 loss 2.6048574447631836, acc 21.875\n",
      "iteration 5191 loss 2.920789957046509, acc 26.5625\n",
      "iteration 5192 loss 2.81465220451355, acc 23.4375\n",
      "iteration 5193 loss 2.692312717437744, acc 21.875\n",
      "iteration 5194 loss 2.9940755367279053, acc 15.625\n",
      "iteration 5195 loss 2.8726553916931152, acc 18.75\n",
      "iteration 5196 loss 2.643202066421509, acc 28.125\n",
      "iteration 5197 loss 2.842961549758911, acc 14.0625\n",
      "iteration 5198 loss 2.6461191177368164, acc 20.3125\n",
      "iteration 5199 loss 2.8300201892852783, acc 14.0625\n",
      "iteration 5200 loss 2.7148020267486572, acc 18.75\n",
      "iteration 5201 loss 2.685065746307373, acc 20.3125\n",
      "iteration 5202 loss 2.628589630126953, acc 23.4375\n",
      "iteration 5203 loss 2.784496545791626, acc 15.625\n",
      "iteration 5204 loss 2.810006856918335, acc 15.625\n",
      "iteration 5205 loss 2.574075937271118, acc 34.375\n",
      "iteration 5206 loss 2.4111874103546143, acc 28.125\n",
      "iteration 5207 loss 2.7409861087799072, acc 20.3125\n",
      "iteration 5208 loss 2.494029998779297, acc 29.6875\n",
      "iteration 5209 loss 2.727022886276245, acc 23.4375\n",
      "iteration 5210 loss 2.8732750415802, acc 17.1875\n",
      "iteration 5211 loss 2.807849407196045, acc 18.75\n",
      "iteration 5212 loss 2.606436014175415, acc 29.6875\n",
      "iteration 5213 loss 2.495262622833252, acc 26.5625\n",
      "iteration 5214 loss 2.466501235961914, acc 29.6875\n",
      "iteration 5215 loss 2.8118016719818115, acc 15.625\n",
      "iteration 5216 loss 2.724978446960449, acc 25.0\n",
      "iteration 5217 loss 2.917032241821289, acc 15.625\n",
      "iteration 5218 loss 2.67879581451416, acc 23.4375\n",
      "iteration 5219 loss 2.6148648262023926, acc 26.5625\n",
      "iteration 5220 loss 2.4613571166992188, acc 29.6875\n",
      "iteration 5221 loss 2.4876596927642822, acc 20.3125\n",
      "iteration 5222 loss 2.6300055980682373, acc 21.875\n",
      "iteration 5223 loss 2.632891893386841, acc 25.0\n",
      "iteration 5224 loss 2.637669086456299, acc 25.0\n",
      "iteration 5225 loss 2.6199722290039062, acc 28.125\n",
      "iteration 5226 loss 2.654052257537842, acc 23.4375\n",
      "iteration 5227 loss 2.6579363346099854, acc 23.4375\n",
      "iteration 5228 loss 2.7177577018737793, acc 15.625\n",
      "iteration 5229 loss 2.7401645183563232, acc 29.6875\n",
      "iteration 5230 loss 2.6939475536346436, acc 17.1875\n",
      "iteration 5231 loss 2.5889666080474854, acc 28.125\n",
      "iteration 5232 loss 2.743428945541382, acc 20.3125\n",
      "iteration 5233 loss 2.699695348739624, acc 21.875\n",
      "iteration 5234 loss 2.6940181255340576, acc 18.75\n",
      "iteration 5235 loss 2.4940154552459717, acc 25.0\n",
      "iteration 5236 loss 2.9707460403442383, acc 12.5\n",
      "iteration 5237 loss 2.6097800731658936, acc 25.0\n",
      "iteration 5238 loss 2.4628405570983887, acc 35.9375\n",
      "iteration 5239 loss 2.599109172821045, acc 31.25\n",
      "iteration 5240 loss 2.6828393936157227, acc 20.3125\n",
      "iteration 5241 loss 2.6630377769470215, acc 21.875\n",
      "iteration 5242 loss 2.6625876426696777, acc 23.4375\n",
      "iteration 5243 loss 2.507838726043701, acc 29.6875\n",
      "iteration 5244 loss 2.6748921871185303, acc 14.0625\n",
      "iteration 5245 loss 2.707530975341797, acc 20.3125\n",
      "iteration 5246 loss 2.520026206970215, acc 25.0\n",
      "iteration 5247 loss 2.631520986557007, acc 15.625\n",
      "iteration 5248 loss 2.970412254333496, acc 21.875\n",
      "iteration 5249 loss 2.5953924655914307, acc 26.5625\n",
      "iteration 5250 loss 2.583376169204712, acc 26.5625\n",
      "iteration 5251 loss 2.8313393592834473, acc 15.625\n",
      "iteration 5252 loss 2.8703551292419434, acc 25.0\n",
      "iteration 5253 loss 2.7849478721618652, acc 20.3125\n",
      "iteration 5254 loss 2.668496608734131, acc 20.3125\n",
      "iteration 5255 loss 2.691803455352783, acc 21.875\n",
      "iteration 5256 loss 2.813326835632324, acc 20.3125\n",
      "iteration 5257 loss 2.6074588298797607, acc 25.0\n",
      "iteration 5258 loss 2.6200015544891357, acc 20.3125\n",
      "iteration 5259 loss 2.714756727218628, acc 20.3125\n",
      "iteration 5260 loss 2.7414047718048096, acc 23.4375\n",
      "iteration 5261 loss 2.841036796569824, acc 18.75\n",
      "iteration 5262 loss 2.453401803970337, acc 35.9375\n",
      "iteration 5263 loss 2.6646857261657715, acc 20.3125\n",
      "iteration 5264 loss 2.4503307342529297, acc 31.25\n",
      "iteration 5265 loss 2.6879470348358154, acc 25.0\n",
      "iteration 5266 loss 2.7788519859313965, acc 25.0\n",
      "iteration 5267 loss 2.4350807666778564, acc 28.125\n",
      "iteration 5268 loss 2.5804686546325684, acc 28.125\n",
      "iteration 5269 loss 2.898556709289551, acc 15.625\n",
      "iteration 5270 loss 2.9458863735198975, acc 9.375\n",
      "iteration 5271 loss 2.7301342487335205, acc 18.75\n",
      "iteration 5272 loss 2.5978431701660156, acc 17.1875\n",
      "iteration 5273 loss 2.5941879749298096, acc 23.4375\n",
      "iteration 5274 loss 2.698688268661499, acc 21.875\n",
      "iteration 5275 loss 2.515883207321167, acc 25.0\n",
      "iteration 5276 loss 2.545850992202759, acc 29.6875\n",
      "iteration 5277 loss 2.743546724319458, acc 23.4375\n",
      "iteration 5278 loss 2.596475601196289, acc 31.25\n",
      "iteration 5279 loss 2.7904000282287598, acc 25.0\n",
      "iteration 5280 loss 2.6797595024108887, acc 20.3125\n",
      "iteration 5281 loss 2.785413980484009, acc 17.1875\n",
      "iteration 5282 loss 2.482194185256958, acc 31.25\n",
      "iteration 5283 loss 2.7508318424224854, acc 21.875\n",
      "iteration 5284 loss 2.700376033782959, acc 15.625\n",
      "iteration 5285 loss 2.7280373573303223, acc 23.4375\n",
      "iteration 5286 loss 2.9016435146331787, acc 18.75\n",
      "iteration 5287 loss 2.604703187942505, acc 23.4375\n",
      "iteration 5288 loss 2.519327163696289, acc 25.0\n",
      "iteration 5289 loss 2.6353440284729004, acc 20.3125\n",
      "iteration 5290 loss 2.6763088703155518, acc 26.5625\n",
      "iteration 5291 loss 2.448450803756714, acc 26.5625\n",
      "iteration 5292 loss 2.7564759254455566, acc 18.75\n",
      "iteration 5293 loss 2.619749069213867, acc 31.25\n",
      "iteration 5294 loss 2.9433534145355225, acc 14.0625\n",
      "iteration 5295 loss 2.8817007541656494, acc 12.5\n",
      "iteration 5296 loss 2.6716811656951904, acc 26.5625\n",
      "iteration 5297 loss 2.7726492881774902, acc 12.5\n",
      "iteration 5298 loss 2.6006765365600586, acc 21.875\n",
      "iteration 5299 loss 2.724074125289917, acc 10.9375\n",
      "iteration 5300 loss 2.7874391078948975, acc 23.4375\n",
      "iteration 5301 loss 2.726818323135376, acc 20.3125\n",
      "iteration 5302 loss 2.7389886379241943, acc 21.875\n",
      "iteration 5303 loss 2.710066318511963, acc 28.125\n",
      "iteration 5304 loss 2.942922353744507, acc 15.625\n",
      "iteration 5305 loss 2.5851261615753174, acc 25.0\n",
      "iteration 5306 loss 2.5603911876678467, acc 25.0\n",
      "iteration 5307 loss 2.726490020751953, acc 20.3125\n",
      "iteration 5308 loss 2.7780888080596924, acc 25.0\n",
      "iteration 5309 loss 2.7343595027923584, acc 14.0625\n",
      "iteration 5310 loss 2.655892848968506, acc 26.5625\n",
      "iteration 5311 loss 2.808063268661499, acc 18.75\n",
      "iteration 5312 loss 2.749098300933838, acc 18.75\n",
      "iteration 5313 loss 2.5411503314971924, acc 32.8125\n",
      "iteration 5314 loss 2.77424955368042, acc 21.875\n",
      "iteration 5315 loss 2.800575017929077, acc 15.625\n",
      "iteration 5316 loss 2.852108955383301, acc 18.75\n",
      "iteration 5317 loss 2.679454803466797, acc 20.3125\n",
      "iteration 5318 loss 2.7336266040802, acc 23.4375\n",
      "iteration 5319 loss 2.5943057537078857, acc 28.125\n",
      "iteration 5320 loss 2.563746452331543, acc 21.875\n",
      "iteration 5321 loss 2.7552850246429443, acc 23.4375\n",
      "iteration 5322 loss 2.5277178287506104, acc 20.3125\n",
      "iteration 5323 loss 2.731773853302002, acc 21.875\n",
      "iteration 5324 loss 2.9505863189697266, acc 21.875\n",
      "iteration 5325 loss 2.72308349609375, acc 18.75\n",
      "iteration 5326 loss 2.7661099433898926, acc 18.75\n",
      "iteration 5327 loss 2.533079147338867, acc 28.125\n",
      "iteration 5328 loss 2.5064094066619873, acc 23.4375\n",
      "iteration 5329 loss 2.629263401031494, acc 20.3125\n",
      "iteration 5330 loss 2.909618377685547, acc 15.625\n",
      "iteration 5331 loss 2.5770103931427, acc 28.125\n",
      "iteration 5332 loss 2.6356730461120605, acc 20.3125\n",
      "iteration 5333 loss 2.742377996444702, acc 21.875\n",
      "iteration 5334 loss 2.5650577545166016, acc 23.4375\n",
      "iteration 5335 loss 2.7232401371002197, acc 10.9375\n",
      "iteration 5336 loss 2.87282657623291, acc 18.75\n",
      "iteration 5337 loss 2.4934253692626953, acc 26.5625\n",
      "iteration 5338 loss 2.8981986045837402, acc 15.625\n",
      "iteration 5339 loss 2.5296428203582764, acc 28.125\n",
      "iteration 5340 loss 2.6987924575805664, acc 21.875\n",
      "iteration 5341 loss 2.58609938621521, acc 31.25\n",
      "iteration 5342 loss 2.7154293060302734, acc 23.4375\n",
      "iteration 5343 loss 2.7330610752105713, acc 29.6875\n",
      "iteration 5344 loss 2.539179801940918, acc 34.375\n",
      "iteration 5345 loss 2.7118823528289795, acc 23.4375\n",
      "iteration 5346 loss 2.869694232940674, acc 23.4375\n",
      "iteration 5347 loss 2.666111469268799, acc 15.625\n",
      "iteration 5348 loss 2.763791561126709, acc 17.1875\n",
      "iteration 5349 loss 2.7901504039764404, acc 15.625\n",
      "iteration 5350 loss 2.666196823120117, acc 28.125\n",
      "iteration 5351 loss 2.5429303646087646, acc 26.5625\n",
      "iteration 5352 loss 2.7280433177948, acc 23.4375\n",
      "iteration 5353 loss 2.730518102645874, acc 21.875\n",
      "iteration 5354 loss 2.625025749206543, acc 18.75\n",
      "iteration 5355 loss 2.7496066093444824, acc 15.625\n",
      "iteration 5356 loss 2.6009440422058105, acc 25.0\n",
      "iteration 5357 loss 2.6274232864379883, acc 18.75\n",
      "iteration 5358 loss 2.689398765563965, acc 17.1875\n",
      "iteration 5359 loss 2.5631844997406006, acc 34.375\n",
      "iteration 5360 loss 2.6258909702301025, acc 20.3125\n",
      "iteration 5361 loss 2.670769214630127, acc 25.0\n",
      "iteration 5362 loss 2.765902519226074, acc 23.4375\n",
      "iteration 5363 loss 2.647845983505249, acc 26.5625\n",
      "iteration 5364 loss 2.546104907989502, acc 21.875\n",
      "iteration 5365 loss 2.639315128326416, acc 25.0\n",
      "iteration 5366 loss 2.892087697982788, acc 15.625\n",
      "iteration 5367 loss 2.4736318588256836, acc 32.8125\n",
      "iteration 5368 loss 2.7249794006347656, acc 26.5625\n",
      "iteration 5369 loss 2.751819133758545, acc 23.4375\n",
      "iteration 5370 loss 2.6494967937469482, acc 26.5625\n",
      "iteration 5371 loss 2.6987802982330322, acc 23.4375\n",
      "iteration 5372 loss 2.5041143894195557, acc 23.4375\n",
      "iteration 5373 loss 2.792192220687866, acc 23.4375\n",
      "iteration 5374 loss 2.8094232082366943, acc 21.875\n",
      "iteration 5375 loss 2.62996768951416, acc 23.4375\n",
      "iteration 5376 loss 2.7181129455566406, acc 18.75\n",
      "iteration 5377 loss 2.7399890422821045, acc 21.875\n",
      "iteration 5378 loss 2.6158761978149414, acc 14.0625\n",
      "iteration 5379 loss 2.858877658843994, acc 12.5\n",
      "iteration 5380 loss 2.5011401176452637, acc 20.3125\n",
      "iteration 5381 loss 2.7056069374084473, acc 17.1875\n",
      "iteration 5382 loss 2.656501531600952, acc 26.5625\n",
      "iteration 5383 loss 2.7406599521636963, acc 18.75\n",
      "iteration 5384 loss 2.8925106525421143, acc 12.5\n",
      "iteration 5385 loss 2.9266257286071777, acc 20.3125\n",
      "iteration 5386 loss 2.7861745357513428, acc 21.875\n",
      "iteration 5387 loss 2.620335340499878, acc 20.3125\n",
      "iteration 5388 loss 2.782593250274658, acc 18.75\n",
      "iteration 5389 loss 2.7686831951141357, acc 28.125\n",
      "iteration 5390 loss 2.615534543991089, acc 20.3125\n",
      "iteration 5391 loss 2.67661190032959, acc 14.0625\n",
      "iteration 5392 loss 2.8170700073242188, acc 9.375\n",
      "iteration 5393 loss 2.4381468296051025, acc 25.0\n",
      "iteration 5394 loss 2.759310245513916, acc 26.5625\n",
      "iteration 5395 loss 2.7729783058166504, acc 18.75\n",
      "iteration 5396 loss 2.6025948524475098, acc 25.0\n",
      "iteration 5397 loss 2.8995819091796875, acc 12.5\n",
      "iteration 5398 loss 2.6326868534088135, acc 28.125\n",
      "iteration 5399 loss 2.7715322971343994, acc 25.0\n",
      "iteration 5400 loss 2.6430466175079346, acc 21.875\n",
      "iteration 5401 loss 2.990527391433716, acc 15.625\n",
      "iteration 5402 loss 2.761334180831909, acc 18.75\n",
      "iteration 5403 loss 2.7928168773651123, acc 25.0\n",
      "iteration 5404 loss 2.563868999481201, acc 17.1875\n",
      "iteration 5405 loss 2.573184013366699, acc 28.125\n",
      "iteration 5406 loss 2.50984263420105, acc 31.25\n",
      "iteration 5407 loss 2.7904319763183594, acc 18.75\n",
      "iteration 5408 loss 2.6975622177124023, acc 20.3125\n",
      "iteration 5409 loss 2.610983371734619, acc 26.5625\n",
      "iteration 5410 loss 2.5593466758728027, acc 21.875\n",
      "iteration 5411 loss 2.817138671875, acc 15.625\n",
      "iteration 5412 loss 2.6190924644470215, acc 26.5625\n",
      "iteration 5413 loss 2.6232216358184814, acc 23.4375\n",
      "iteration 5414 loss 2.5038206577301025, acc 28.125\n",
      "iteration 5415 loss 2.5511837005615234, acc 28.125\n",
      "iteration 5416 loss 2.7607831954956055, acc 20.3125\n",
      "iteration 5417 loss 2.7211897373199463, acc 23.4375\n",
      "iteration 5418 loss 2.827596664428711, acc 17.1875\n",
      "iteration 5419 loss 2.753788471221924, acc 21.875\n",
      "iteration 5420 loss 2.6653802394866943, acc 17.1875\n",
      "iteration 5421 loss 2.811570167541504, acc 7.8125\n",
      "iteration 5422 loss 2.679518461227417, acc 25.0\n",
      "iteration 5423 loss 2.714770555496216, acc 20.3125\n",
      "iteration 5424 loss 2.5825273990631104, acc 23.4375\n",
      "iteration 5425 loss 2.815243721008301, acc 14.0625\n",
      "iteration 5426 loss 2.5996487140655518, acc 23.4375\n",
      "iteration 5427 loss 2.8562135696411133, acc 15.625\n",
      "iteration 5428 loss 2.5954434871673584, acc 29.6875\n",
      "iteration 5429 loss 2.5847342014312744, acc 21.875\n",
      "iteration 5430 loss 2.726641893386841, acc 18.75\n",
      "iteration 5431 loss 2.5575475692749023, acc 31.25\n",
      "iteration 5432 loss 2.567467451095581, acc 21.875\n",
      "iteration 5433 loss 2.90281343460083, acc 18.75\n",
      "iteration 5434 loss 3.0172863006591797, acc 17.1875\n",
      "iteration 5435 loss 2.4940991401672363, acc 25.0\n",
      "iteration 5436 loss 2.564626455307007, acc 29.6875\n",
      "iteration 5437 loss 2.3982067108154297, acc 34.375\n",
      "iteration 5438 loss 2.679105043411255, acc 26.5625\n",
      "iteration 5439 loss 2.6849467754364014, acc 20.3125\n",
      "iteration 5440 loss 2.695651054382324, acc 18.75\n",
      "iteration 5441 loss 2.3831429481506348, acc 23.4375\n",
      "iteration 5442 loss 2.8223133087158203, acc 21.875\n",
      "iteration 5443 loss 2.5835795402526855, acc 21.875\n",
      "iteration 5444 loss 2.598966360092163, acc 26.5625\n",
      "iteration 5445 loss 2.5581209659576416, acc 29.6875\n",
      "iteration 5446 loss 2.774108648300171, acc 18.75\n",
      "iteration 5447 loss 2.5862250328063965, acc 28.125\n",
      "iteration 5448 loss 2.4302399158477783, acc 29.6875\n",
      "iteration 5449 loss 2.8985819816589355, acc 17.1875\n",
      "iteration 5450 loss 2.7473952770233154, acc 18.75\n",
      "iteration 5451 loss 2.6350183486938477, acc 18.75\n",
      "iteration 5452 loss 2.8097126483917236, acc 15.625\n",
      "iteration 5453 loss 2.8247690200805664, acc 20.3125\n",
      "iteration 5454 loss 2.728090286254883, acc 10.9375\n",
      "iteration 5455 loss 2.666929006576538, acc 23.4375\n",
      "iteration 5456 loss 2.8273861408233643, acc 17.1875\n",
      "iteration 5457 loss 2.8056294918060303, acc 17.1875\n",
      "iteration 5458 loss 2.7771003246307373, acc 21.875\n",
      "iteration 5459 loss 2.7524116039276123, acc 21.875\n",
      "iteration 5460 loss 2.6424267292022705, acc 18.75\n",
      "iteration 5461 loss 2.612541913986206, acc 17.1875\n",
      "iteration 5462 loss 2.686439275741577, acc 21.875\n",
      "iteration 5463 loss 2.5757815837860107, acc 31.25\n",
      "iteration 5464 loss 2.7880361080169678, acc 26.5625\n",
      "iteration 5465 loss 2.6977834701538086, acc 17.1875\n",
      "iteration 5466 loss 2.6111624240875244, acc 26.5625\n",
      "iteration 5467 loss 2.5266048908233643, acc 26.5625\n",
      "iteration 5468 loss 2.5690901279449463, acc 28.125\n",
      "iteration 5469 loss 2.7657663822174072, acc 15.625\n",
      "iteration 5470 loss 2.568211317062378, acc 26.5625\n",
      "iteration 5471 loss 2.5727012157440186, acc 26.5625\n",
      "iteration 5472 loss 2.5749971866607666, acc 25.0\n",
      "iteration 5473 loss 2.5389299392700195, acc 26.5625\n",
      "iteration 5474 loss 2.7503156661987305, acc 17.1875\n",
      "iteration 5475 loss 2.7434163093566895, acc 18.75\n",
      "iteration 5476 loss 2.766957998275757, acc 23.4375\n",
      "iteration 5477 loss 2.749314308166504, acc 17.1875\n",
      "iteration 5478 loss 2.6378722190856934, acc 25.0\n",
      "iteration 5479 loss 2.6319265365600586, acc 26.5625\n",
      "iteration 5480 loss 2.958200216293335, acc 20.3125\n",
      "iteration 5481 loss 2.548231363296509, acc 31.25\n",
      "iteration 5482 loss 2.8886301517486572, acc 7.8125\n",
      "iteration 5483 loss 2.6999495029449463, acc 20.3125\n",
      "iteration 5484 loss 2.6780881881713867, acc 26.5625\n",
      "iteration 5485 loss 2.7853031158447266, acc 23.4375\n",
      "iteration 5486 loss 2.6488020420074463, acc 25.0\n",
      "iteration 5487 loss 2.5679259300231934, acc 20.3125\n",
      "iteration 5488 loss 2.465852975845337, acc 25.0\n",
      "iteration 5489 loss 2.827760696411133, acc 17.1875\n",
      "iteration 5490 loss 2.6836352348327637, acc 20.3125\n",
      "iteration 5491 loss 2.786198854446411, acc 14.0625\n",
      "iteration 5492 loss 2.639011859893799, acc 28.125\n",
      "iteration 5493 loss 2.663996934890747, acc 20.3125\n",
      "iteration 5494 loss 2.6052935123443604, acc 23.4375\n",
      "iteration 5495 loss 2.6231517791748047, acc 26.5625\n",
      "iteration 5496 loss 2.7741692066192627, acc 20.3125\n",
      "iteration 5497 loss 2.6994147300720215, acc 28.125\n",
      "iteration 5498 loss 2.4900548458099365, acc 28.125\n",
      "iteration 5499 loss 2.651468276977539, acc 21.875\n",
      "iteration 5500 loss 2.785083770751953, acc 23.4375\n",
      "iteration 5501 loss 2.561370372772217, acc 23.4375\n",
      "iteration 5502 loss 2.6506683826446533, acc 21.875\n",
      "iteration 5503 loss 2.632786750793457, acc 25.0\n",
      "iteration 5504 loss 2.8169105052948, acc 18.75\n",
      "iteration 5505 loss 2.743391513824463, acc 21.875\n",
      "iteration 5506 loss 2.7662172317504883, acc 14.0625\n",
      "iteration 5507 loss 2.5815443992614746, acc 23.4375\n",
      "iteration 5508 loss 2.813680648803711, acc 18.75\n",
      "iteration 5509 loss 2.6458983421325684, acc 20.3125\n",
      "iteration 5510 loss 2.709998607635498, acc 17.1875\n",
      "iteration 5511 loss 2.7494585514068604, acc 18.75\n",
      "iteration 5512 loss 2.617915153503418, acc 26.5625\n",
      "iteration 5513 loss 2.523202419281006, acc 21.875\n",
      "iteration 5514 loss 2.661287784576416, acc 15.625\n",
      "iteration 5515 loss 2.9162256717681885, acc 18.75\n",
      "iteration 5516 loss 2.777033805847168, acc 21.875\n",
      "iteration 5517 loss 2.6308891773223877, acc 21.875\n",
      "iteration 5518 loss 2.709298849105835, acc 17.1875\n",
      "iteration 5519 loss 2.563955783843994, acc 28.125\n",
      "iteration 5520 loss 2.668905735015869, acc 12.5\n",
      "iteration 5521 loss 2.7482733726501465, acc 17.1875\n",
      "iteration 5522 loss 2.714475154876709, acc 17.1875\n",
      "iteration 5523 loss 2.6709163188934326, acc 26.5625\n",
      "iteration 5524 loss 2.7148571014404297, acc 21.875\n",
      "iteration 5525 loss 2.588754177093506, acc 20.3125\n",
      "iteration 5526 loss 2.734701156616211, acc 20.3125\n",
      "iteration 5527 loss 2.7847883701324463, acc 14.0625\n",
      "iteration 5528 loss 2.6967086791992188, acc 26.5625\n",
      "iteration 5529 loss 2.573554515838623, acc 25.0\n",
      "iteration 5530 loss 2.6809115409851074, acc 23.4375\n",
      "iteration 5531 loss 3.0051395893096924, acc 21.875\n",
      "iteration 5532 loss 2.5371508598327637, acc 25.0\n",
      "iteration 5533 loss 2.7555723190307617, acc 23.4375\n",
      "iteration 5534 loss 2.634476900100708, acc 28.125\n",
      "iteration 5535 loss 2.780269145965576, acc 14.0625\n",
      "iteration 5536 loss 2.679706335067749, acc 20.3125\n",
      "iteration 5537 loss 2.765683650970459, acc 17.1875\n",
      "iteration 5538 loss 2.509620189666748, acc 25.0\n",
      "iteration 5539 loss 2.6005971431732178, acc 25.0\n",
      "iteration 5540 loss 2.6743991374969482, acc 29.6875\n",
      "iteration 5541 loss 2.5103161334991455, acc 18.75\n",
      "iteration 5542 loss 2.7032833099365234, acc 25.0\n",
      "iteration 5543 loss 2.6285057067871094, acc 23.4375\n",
      "iteration 5544 loss 2.673701524734497, acc 28.125\n",
      "iteration 5545 loss 2.6991384029388428, acc 25.0\n",
      "iteration 5546 loss 2.735093116760254, acc 18.75\n",
      "iteration 5547 loss 2.6673030853271484, acc 17.1875\n",
      "iteration 5548 loss 2.681304931640625, acc 18.75\n",
      "iteration 5549 loss 2.8025574684143066, acc 18.75\n",
      "iteration 5550 loss 2.656141757965088, acc 18.75\n",
      "iteration 5551 loss 2.72087025642395, acc 23.4375\n",
      "iteration 5552 loss 2.6714227199554443, acc 21.875\n",
      "iteration 5553 loss 2.8028008937835693, acc 12.5\n",
      "iteration 5554 loss 2.736889123916626, acc 15.625\n",
      "iteration 5555 loss 2.6048927307128906, acc 14.0625\n",
      "iteration 5556 loss 2.4456608295440674, acc 26.5625\n",
      "iteration 5557 loss 2.818911552429199, acc 18.75\n",
      "iteration 5558 loss 2.4957480430603027, acc 21.875\n",
      "iteration 5559 loss 2.8447165489196777, acc 25.0\n",
      "iteration 5560 loss 2.604778289794922, acc 21.875\n",
      "iteration 5561 loss 2.9589943885803223, acc 18.75\n",
      "iteration 5562 loss 2.540637969970703, acc 32.8125\n",
      "iteration 5563 loss 2.9035682678222656, acc 23.4375\n",
      "iteration 5564 loss 2.751018524169922, acc 17.1875\n",
      "iteration 5565 loss 2.555198907852173, acc 31.25\n",
      "iteration 5566 loss 2.593745470046997, acc 25.0\n",
      "iteration 5567 loss 2.6294026374816895, acc 26.5625\n",
      "iteration 5568 loss 2.3881113529205322, acc 31.25\n",
      "iteration 5569 loss 2.7716739177703857, acc 23.4375\n",
      "iteration 5570 loss 2.5721075534820557, acc 21.875\n",
      "iteration 5571 loss 2.8623924255371094, acc 15.625\n",
      "iteration 5572 loss 2.667114019393921, acc 29.6875\n",
      "iteration 5573 loss 2.7784018516540527, acc 15.625\n",
      "iteration 5574 loss 2.8308639526367188, acc 17.1875\n",
      "iteration 5575 loss 2.578312873840332, acc 21.875\n",
      "iteration 5576 loss 2.763725757598877, acc 15.625\n",
      "iteration 5577 loss 2.516613721847534, acc 28.125\n",
      "iteration 5578 loss 2.6742641925811768, acc 25.0\n",
      "iteration 5579 loss 2.698303461074829, acc 17.1875\n",
      "iteration 5580 loss 2.7625439167022705, acc 23.4375\n",
      "iteration 5581 loss 2.58499813079834, acc 26.5625\n",
      "iteration 5582 loss 2.6941914558410645, acc 25.0\n",
      "iteration 5583 loss 2.9764163494110107, acc 15.625\n",
      "iteration 5584 loss 2.7002835273742676, acc 20.3125\n",
      "iteration 5585 loss 2.7792882919311523, acc 17.1875\n",
      "iteration 5586 loss 2.8563928604125977, acc 17.1875\n",
      "iteration 5587 loss 2.8184218406677246, acc 23.4375\n",
      "iteration 5588 loss 2.824416399002075, acc 12.5\n",
      "iteration 5589 loss 2.6183149814605713, acc 29.6875\n",
      "iteration 5590 loss 2.7898309230804443, acc 14.0625\n",
      "iteration 5591 loss 2.9181909561157227, acc 18.75\n",
      "iteration 5592 loss 2.688314199447632, acc 20.3125\n",
      "iteration 5593 loss 2.710048198699951, acc 23.4375\n",
      "iteration 5594 loss 2.736145496368408, acc 17.1875\n",
      "iteration 5595 loss 2.6005520820617676, acc 20.3125\n",
      "iteration 5596 loss 2.528160810470581, acc 28.125\n",
      "iteration 5597 loss 2.7301719188690186, acc 20.3125\n",
      "iteration 5598 loss 2.855936050415039, acc 28.125\n",
      "iteration 5599 loss 2.7731785774230957, acc 23.4375\n",
      "iteration 5600 loss 2.7360951900482178, acc 20.3125\n",
      "iteration 5601 loss 2.6550803184509277, acc 28.125\n",
      "iteration 5602 loss 2.847989082336426, acc 17.1875\n",
      "iteration 5603 loss 2.6174890995025635, acc 26.5625\n",
      "iteration 5604 loss 2.6581637859344482, acc 23.4375\n",
      "iteration 5605 loss 2.633864402770996, acc 25.0\n",
      "iteration 5606 loss 2.5233211517333984, acc 20.3125\n",
      "iteration 5607 loss 2.5807149410247803, acc 23.4375\n",
      "iteration 5608 loss 2.6548209190368652, acc 25.0\n",
      "iteration 5609 loss 2.747576951980591, acc 21.875\n",
      "iteration 5610 loss 2.8040432929992676, acc 18.75\n",
      "iteration 5611 loss 2.6857125759124756, acc 15.625\n",
      "iteration 5612 loss 2.7625250816345215, acc 18.75\n",
      "iteration 5613 loss 2.5771164894104004, acc 26.5625\n",
      "iteration 5614 loss 2.780299186706543, acc 12.5\n",
      "iteration 5615 loss 2.626222610473633, acc 26.5625\n",
      "iteration 5616 loss 2.480448007583618, acc 29.6875\n",
      "iteration 5617 loss 2.6219003200531006, acc 25.0\n",
      "iteration 5618 loss 2.6832194328308105, acc 21.875\n",
      "iteration 5619 loss 2.618130683898926, acc 28.125\n",
      "iteration 5620 loss 2.558830499649048, acc 28.125\n",
      "iteration 5621 loss 2.8356940746307373, acc 15.625\n",
      "iteration 5622 loss 2.729672908782959, acc 18.75\n",
      "iteration 5623 loss 2.6031174659729004, acc 18.75\n",
      "iteration 5624 loss 2.695786237716675, acc 15.625\n",
      "iteration 5625 loss 2.8670389652252197, acc 14.0625\n",
      "iteration 5626 loss 2.737642288208008, acc 21.875\n",
      "iteration 5627 loss 2.916470527648926, acc 17.1875\n",
      "iteration 5628 loss 2.7397847175598145, acc 26.5625\n",
      "iteration 5629 loss 2.475551128387451, acc 31.25\n",
      "iteration 5630 loss 2.5530691146850586, acc 25.0\n",
      "iteration 5631 loss 2.942903995513916, acc 10.9375\n",
      "iteration 5632 loss 2.7776882648468018, acc 20.3125\n",
      "iteration 5633 loss 2.7559432983398438, acc 15.625\n",
      "iteration 5634 loss 2.5133752822875977, acc 23.4375\n",
      "iteration 5635 loss 2.7862765789031982, acc 15.625\n",
      "iteration 5636 loss 2.5481276512145996, acc 23.4375\n",
      "iteration 5637 loss 2.8352885246276855, acc 21.875\n",
      "iteration 5638 loss 2.909756898880005, acc 15.625\n",
      "iteration 5639 loss 3.0811703205108643, acc 6.25\n",
      "iteration 5640 loss 2.543097734451294, acc 28.125\n",
      "iteration 5641 loss 2.637645959854126, acc 18.75\n",
      "iteration 5642 loss 2.524754047393799, acc 25.0\n",
      "iteration 5643 loss 2.82725191116333, acc 15.625\n",
      "iteration 5644 loss 2.73178768157959, acc 18.75\n",
      "iteration 5645 loss 2.636935234069824, acc 20.3125\n",
      "iteration 5646 loss 2.4759738445281982, acc 23.4375\n",
      "iteration 5647 loss 2.714571237564087, acc 23.4375\n",
      "iteration 5648 loss 2.741260528564453, acc 15.625\n",
      "iteration 5649 loss 2.7608470916748047, acc 17.1875\n",
      "iteration 5650 loss 2.784679889678955, acc 10.9375\n",
      "iteration 5651 loss 2.590862274169922, acc 21.875\n",
      "iteration 5652 loss 2.487696886062622, acc 21.875\n",
      "iteration 5653 loss 2.609046459197998, acc 20.3125\n",
      "iteration 5654 loss 2.723684072494507, acc 26.5625\n",
      "iteration 5655 loss 2.5011990070343018, acc 28.125\n",
      "iteration 5656 loss 2.611135482788086, acc 28.125\n",
      "iteration 5657 loss 2.8308374881744385, acc 28.125\n",
      "iteration 5658 loss 2.6751718521118164, acc 21.875\n",
      "iteration 5659 loss 2.583740234375, acc 12.5\n",
      "iteration 5660 loss 2.7153618335723877, acc 20.3125\n",
      "iteration 5661 loss 2.770556688308716, acc 20.3125\n",
      "iteration 5662 loss 2.5647926330566406, acc 28.125\n",
      "iteration 5663 loss 2.7003140449523926, acc 25.0\n",
      "iteration 5664 loss 2.5386624336242676, acc 26.5625\n",
      "iteration 5665 loss 2.6018011569976807, acc 21.875\n",
      "iteration 5666 loss 2.793647527694702, acc 23.4375\n",
      "iteration 5667 loss 2.575248956680298, acc 29.6875\n",
      "iteration 5668 loss 2.716195583343506, acc 20.3125\n",
      "iteration 5669 loss 2.5996804237365723, acc 25.0\n",
      "iteration 5670 loss 2.531236410140991, acc 18.75\n",
      "iteration 5671 loss 2.7295069694519043, acc 18.75\n",
      "iteration 5672 loss 2.6578123569488525, acc 23.4375\n",
      "iteration 5673 loss 2.6161417961120605, acc 20.3125\n",
      "iteration 5674 loss 2.6331748962402344, acc 23.4375\n",
      "iteration 5675 loss 2.777838706970215, acc 20.3125\n",
      "iteration 5676 loss 2.5806453227996826, acc 25.0\n",
      "iteration 5677 loss 2.6409642696380615, acc 15.625\n",
      "iteration 5678 loss 2.612680435180664, acc 20.3125\n",
      "iteration 5679 loss 2.718736171722412, acc 23.4375\n",
      "iteration 5680 loss 2.7234086990356445, acc 25.0\n",
      "iteration 5681 loss 2.638265371322632, acc 21.875\n",
      "iteration 5682 loss 2.8053252696990967, acc 20.3125\n",
      "iteration 5683 loss 2.854491949081421, acc 15.625\n",
      "iteration 5684 loss 2.7315821647644043, acc 20.3125\n",
      "iteration 5685 loss 2.6898646354675293, acc 26.5625\n",
      "iteration 5686 loss 2.6273040771484375, acc 21.875\n",
      "iteration 5687 loss 2.7387747764587402, acc 20.3125\n",
      "iteration 5688 loss 2.5955846309661865, acc 28.125\n",
      "iteration 5689 loss 2.9260642528533936, acc 17.1875\n",
      "iteration 5690 loss 2.899876117706299, acc 9.375\n",
      "iteration 5691 loss 2.7360126972198486, acc 6.25\n",
      "iteration 5692 loss 2.8967764377593994, acc 6.25\n",
      "iteration 5693 loss 2.608466148376465, acc 29.6875\n",
      "iteration 5694 loss 2.5782268047332764, acc 20.3125\n",
      "iteration 5695 loss 2.7763891220092773, acc 21.875\n",
      "iteration 5696 loss 2.5174336433410645, acc 26.5625\n",
      "iteration 5697 loss 2.765077829360962, acc 17.1875\n",
      "iteration 5698 loss 2.530280828475952, acc 29.6875\n",
      "iteration 5699 loss 2.6800243854522705, acc 18.75\n",
      "iteration 5700 loss 2.653019666671753, acc 21.875\n",
      "iteration 5701 loss 2.720501184463501, acc 18.75\n",
      "iteration 5702 loss 2.6735339164733887, acc 28.125\n",
      "iteration 5703 loss 2.8115622997283936, acc 14.0625\n",
      "iteration 5704 loss 2.799109697341919, acc 18.75\n",
      "iteration 5705 loss 2.837139129638672, acc 15.625\n",
      "iteration 5706 loss 2.4987266063690186, acc 37.5\n",
      "iteration 5707 loss 2.720426082611084, acc 21.875\n",
      "iteration 5708 loss 2.4212825298309326, acc 29.6875\n",
      "iteration 5709 loss 2.751786708831787, acc 25.0\n",
      "iteration 5710 loss 2.650123119354248, acc 15.625\n",
      "iteration 5711 loss 2.7318944931030273, acc 17.1875\n",
      "iteration 5712 loss 2.8006937503814697, acc 23.4375\n",
      "iteration 5713 loss 2.812594413757324, acc 20.3125\n",
      "iteration 5714 loss 2.84538197517395, acc 12.5\n",
      "iteration 5715 loss 2.756603479385376, acc 26.5625\n",
      "iteration 5716 loss 2.4331021308898926, acc 34.375\n",
      "iteration 5717 loss 2.732118606567383, acc 15.625\n",
      "iteration 5718 loss 2.453443765640259, acc 29.6875\n",
      "iteration 5719 loss 2.7911341190338135, acc 20.3125\n",
      "iteration 5720 loss 2.8454182147979736, acc 17.1875\n",
      "iteration 5721 loss 2.6978065967559814, acc 28.125\n",
      "iteration 5722 loss 2.89520525932312, acc 15.625\n",
      "iteration 5723 loss 2.7011568546295166, acc 18.75\n",
      "iteration 5724 loss 2.8880650997161865, acc 12.5\n",
      "iteration 5725 loss 2.694805860519409, acc 10.9375\n",
      "iteration 5726 loss 2.681696653366089, acc 25.0\n",
      "iteration 5727 loss 2.658928394317627, acc 20.3125\n",
      "iteration 5728 loss 2.546164035797119, acc 28.125\n",
      "iteration 5729 loss 2.8714640140533447, acc 20.3125\n",
      "iteration 5730 loss 2.8935513496398926, acc 14.0625\n",
      "iteration 5731 loss 2.638740062713623, acc 18.75\n",
      "iteration 5732 loss 2.5662155151367188, acc 18.75\n",
      "iteration 5733 loss 2.4142093658447266, acc 31.25\n",
      "iteration 5734 loss 2.466073513031006, acc 21.875\n",
      "iteration 5735 loss 2.9644243717193604, acc 15.625\n",
      "iteration 5736 loss 2.6475937366485596, acc 18.75\n",
      "iteration 5737 loss 2.4994890689849854, acc 31.25\n",
      "iteration 5738 loss 2.8630709648132324, acc 9.375\n",
      "iteration 5739 loss 2.639615535736084, acc 32.8125\n",
      "iteration 5740 loss 2.450198173522949, acc 21.875\n",
      "iteration 5741 loss 2.7582178115844727, acc 10.9375\n",
      "iteration 5742 loss 2.6504671573638916, acc 15.625\n",
      "iteration 5743 loss 2.6797876358032227, acc 17.1875\n",
      "iteration 5744 loss 2.7787680625915527, acc 21.875\n",
      "iteration 5745 loss 2.748731851577759, acc 21.875\n",
      "iteration 5746 loss 2.446378231048584, acc 35.9375\n",
      "iteration 5747 loss 2.771009922027588, acc 20.3125\n",
      "iteration 5748 loss 2.621537685394287, acc 23.4375\n",
      "iteration 5749 loss 2.6163134574890137, acc 25.0\n",
      "iteration 5750 loss 2.7181482315063477, acc 15.625\n",
      "iteration 5751 loss 2.6850335597991943, acc 14.0625\n",
      "iteration 5752 loss 2.660374641418457, acc 18.75\n",
      "iteration 5753 loss 2.7452709674835205, acc 20.3125\n",
      "iteration 5754 loss 2.6317200660705566, acc 23.4375\n",
      "iteration 5755 loss 2.6596715450286865, acc 17.1875\n",
      "iteration 5756 loss 2.737306833267212, acc 20.3125\n",
      "iteration 5757 loss 2.3968517780303955, acc 32.8125\n",
      "iteration 5758 loss 2.8689913749694824, acc 15.625\n",
      "iteration 5759 loss 2.8329660892486572, acc 18.75\n",
      "iteration 5760 loss 2.5563108921051025, acc 21.875\n",
      "iteration 5761 loss 2.732426881790161, acc 25.0\n",
      "iteration 5762 loss 2.7856223583221436, acc 17.1875\n",
      "iteration 5763 loss 2.421308755874634, acc 32.8125\n",
      "iteration 5764 loss 2.554718017578125, acc 23.4375\n",
      "iteration 5765 loss 2.6477644443511963, acc 23.4375\n",
      "iteration 5766 loss 2.5983474254608154, acc 25.0\n",
      "iteration 5767 loss 2.708371877670288, acc 23.4375\n",
      "iteration 5768 loss 2.782686948776245, acc 18.75\n",
      "iteration 5769 loss 2.714434862136841, acc 20.3125\n",
      "iteration 5770 loss 2.89253568649292, acc 15.625\n",
      "iteration 5771 loss 2.4755592346191406, acc 35.9375\n",
      "iteration 5772 loss 2.4918389320373535, acc 26.5625\n",
      "iteration 5773 loss 2.8709311485290527, acc 23.4375\n",
      "iteration 5774 loss 2.584500312805176, acc 21.875\n",
      "iteration 5775 loss 2.715801954269409, acc 26.5625\n",
      "iteration 5776 loss 2.5913891792297363, acc 25.0\n",
      "iteration 5777 loss 2.576754570007324, acc 21.875\n",
      "iteration 5778 loss 2.525033950805664, acc 28.125\n",
      "iteration 5779 loss 2.904334545135498, acc 12.5\n",
      "iteration 5780 loss 2.9645848274230957, acc 12.5\n",
      "iteration 5781 loss 2.6355302333831787, acc 15.625\n",
      "iteration 5782 loss 2.810504198074341, acc 15.625\n",
      "iteration 5783 loss 2.4371426105499268, acc 25.0\n",
      "iteration 5784 loss 2.712502956390381, acc 20.3125\n",
      "iteration 5785 loss 2.627298593521118, acc 20.3125\n",
      "iteration 5786 loss 2.692807912826538, acc 20.3125\n",
      "iteration 5787 loss 2.651326894760132, acc 26.5625\n",
      "iteration 5788 loss 2.6029605865478516, acc 23.4375\n",
      "iteration 5789 loss 2.714020252227783, acc 17.1875\n",
      "iteration 5790 loss 2.766848564147949, acc 12.5\n",
      "iteration 5791 loss 2.663606643676758, acc 12.5\n",
      "iteration 5792 loss 2.6492438316345215, acc 26.5625\n",
      "iteration 5793 loss 2.5685243606567383, acc 23.4375\n",
      "iteration 5794 loss 2.621934175491333, acc 23.4375\n",
      "iteration 5795 loss 2.6942896842956543, acc 21.875\n",
      "iteration 5796 loss 2.5651559829711914, acc 20.3125\n",
      "iteration 5797 loss 2.6796910762786865, acc 20.3125\n",
      "iteration 5798 loss 2.6587159633636475, acc 18.75\n",
      "iteration 5799 loss 2.528381109237671, acc 28.125\n",
      "iteration 5800 loss 2.5018105506896973, acc 25.0\n",
      "iteration 5801 loss 2.609928846359253, acc 23.4375\n",
      "iteration 5802 loss 2.759398937225342, acc 12.5\n",
      "iteration 5803 loss 2.4068500995635986, acc 34.375\n",
      "iteration 5804 loss 2.5415329933166504, acc 25.0\n",
      "iteration 5805 loss 2.655848264694214, acc 15.625\n",
      "iteration 5806 loss 2.5702502727508545, acc 26.5625\n",
      "iteration 5807 loss 2.416102886199951, acc 25.0\n",
      "iteration 5808 loss 2.8212342262268066, acc 18.75\n",
      "iteration 5809 loss 2.858126163482666, acc 18.75\n",
      "iteration 5810 loss 2.685150623321533, acc 15.625\n",
      "iteration 5811 loss 2.802825927734375, acc 21.875\n",
      "iteration 5812 loss 2.804908514022827, acc 17.1875\n",
      "iteration 5813 loss 3.066049814224243, acc 15.625\n",
      "iteration 5814 loss 2.857938051223755, acc 20.3125\n",
      "iteration 5815 loss 2.526341676712036, acc 31.25\n",
      "iteration 5816 loss 2.8678650856018066, acc 15.625\n",
      "iteration 5817 loss 2.5969626903533936, acc 28.125\n",
      "iteration 5818 loss 2.60341215133667, acc 20.3125\n",
      "iteration 5819 loss 2.534743309020996, acc 23.4375\n",
      "iteration 5820 loss 2.541728973388672, acc 28.125\n",
      "iteration 5821 loss 2.7303316593170166, acc 17.1875\n",
      "iteration 5822 loss 2.575622320175171, acc 26.5625\n",
      "iteration 5823 loss 2.587611198425293, acc 25.0\n",
      "iteration 5824 loss 2.7384512424468994, acc 31.25\n",
      "iteration 5825 loss 2.5210366249084473, acc 26.5625\n",
      "iteration 5826 loss 2.512531042098999, acc 26.5625\n",
      "iteration 5827 loss 2.717954397201538, acc 23.4375\n",
      "iteration 5828 loss 2.6238105297088623, acc 25.0\n",
      "iteration 5829 loss 2.6585144996643066, acc 25.0\n",
      "iteration 5830 loss 2.8919341564178467, acc 21.875\n",
      "iteration 5831 loss 2.7081966400146484, acc 17.1875\n",
      "iteration 5832 loss 2.5428993701934814, acc 20.3125\n",
      "iteration 5833 loss 2.628809690475464, acc 25.0\n",
      "iteration 5834 loss 2.619816780090332, acc 25.0\n",
      "iteration 5835 loss 2.522355079650879, acc 25.0\n",
      "iteration 5836 loss 2.5441207885742188, acc 25.0\n",
      "iteration 5837 loss 2.5563390254974365, acc 26.5625\n",
      "iteration 5838 loss 2.649113416671753, acc 25.0\n",
      "iteration 5839 loss 2.708646535873413, acc 20.3125\n",
      "iteration 5840 loss 2.7372868061065674, acc 18.75\n",
      "iteration 5841 loss 2.7914185523986816, acc 18.75\n",
      "iteration 5842 loss 2.8235089778900146, acc 20.3125\n",
      "iteration 5843 loss 2.663775682449341, acc 17.1875\n",
      "iteration 5844 loss 2.636866569519043, acc 26.5625\n",
      "iteration 5845 loss 2.683208703994751, acc 28.125\n",
      "iteration 5846 loss 2.448148012161255, acc 26.5625\n",
      "iteration 5847 loss 2.504984140396118, acc 32.8125\n",
      "iteration 5848 loss 2.7421393394470215, acc 15.625\n",
      "iteration 5849 loss 2.6509063243865967, acc 20.3125\n",
      "iteration 5850 loss 2.6744956970214844, acc 17.1875\n",
      "iteration 5851 loss 2.9775261878967285, acc 18.75\n",
      "iteration 5852 loss 2.6931910514831543, acc 28.125\n",
      "iteration 5853 loss 2.7352359294891357, acc 18.75\n",
      "iteration 5854 loss 2.704350471496582, acc 21.875\n",
      "iteration 5855 loss 2.6336328983306885, acc 28.125\n",
      "iteration 5856 loss 2.689568519592285, acc 18.75\n",
      "iteration 5857 loss 2.69102144241333, acc 15.625\n",
      "iteration 5858 loss 2.6121275424957275, acc 29.6875\n",
      "iteration 5859 loss 2.726499557495117, acc 23.4375\n",
      "iteration 5860 loss 2.7536611557006836, acc 25.0\n",
      "iteration 5861 loss 2.7138445377349854, acc 21.875\n",
      "iteration 5862 loss 2.6049301624298096, acc 23.4375\n",
      "iteration 5863 loss 2.8529584407806396, acc 21.875\n",
      "iteration 5864 loss 2.5344457626342773, acc 25.0\n",
      "iteration 5865 loss 2.6953554153442383, acc 25.0\n",
      "iteration 5866 loss 2.641514301300049, acc 31.25\n",
      "iteration 5867 loss 2.764509677886963, acc 20.3125\n",
      "iteration 5868 loss 2.6707475185394287, acc 18.75\n",
      "iteration 5869 loss 2.915898561477661, acc 15.625\n",
      "iteration 5870 loss 2.7363107204437256, acc 18.75\n",
      "iteration 5871 loss 2.5404179096221924, acc 20.3125\n",
      "iteration 5872 loss 2.758617877960205, acc 18.75\n",
      "iteration 5873 loss 2.3484857082366943, acc 39.0625\n",
      "iteration 5874 loss 2.8194286823272705, acc 20.3125\n",
      "iteration 5875 loss 2.6739838123321533, acc 25.0\n",
      "iteration 5876 loss 2.6833415031433105, acc 21.875\n",
      "iteration 5877 loss 2.636655569076538, acc 23.4375\n",
      "iteration 5878 loss 2.6233983039855957, acc 28.125\n",
      "iteration 5879 loss 2.781768798828125, acc 18.75\n",
      "iteration 5880 loss 2.5806939601898193, acc 18.75\n",
      "iteration 5881 loss 2.4874138832092285, acc 28.125\n",
      "iteration 5882 loss 2.525411605834961, acc 31.25\n",
      "iteration 5883 loss 2.602357864379883, acc 26.5625\n",
      "iteration 5884 loss 2.5541248321533203, acc 20.3125\n",
      "iteration 5885 loss 2.554121255874634, acc 23.4375\n",
      "iteration 5886 loss 2.6775217056274414, acc 29.6875\n",
      "iteration 5887 loss 2.569377899169922, acc 31.25\n",
      "iteration 5888 loss 2.4914159774780273, acc 25.0\n",
      "iteration 5889 loss 2.8502357006073, acc 14.0625\n",
      "iteration 5890 loss 2.7408366203308105, acc 14.0625\n",
      "iteration 5891 loss 2.7308661937713623, acc 18.75\n",
      "iteration 5892 loss 2.51436448097229, acc 18.75\n",
      "iteration 5893 loss 2.744863510131836, acc 25.0\n",
      "iteration 5894 loss 2.509566307067871, acc 28.125\n",
      "iteration 5895 loss 2.776331663131714, acc 23.4375\n",
      "iteration 5896 loss 2.774066686630249, acc 23.4375\n",
      "iteration 5897 loss 2.6824352741241455, acc 20.3125\n",
      "iteration 5898 loss 2.668651580810547, acc 15.625\n",
      "iteration 5899 loss 2.6306488513946533, acc 21.875\n",
      "iteration 5900 loss 2.4951608180999756, acc 28.125\n",
      "iteration 5901 loss 2.770080804824829, acc 17.1875\n",
      "iteration 5902 loss 2.678748607635498, acc 21.875\n",
      "iteration 5903 loss 2.85775089263916, acc 14.0625\n",
      "iteration 5904 loss 2.6722192764282227, acc 26.5625\n",
      "iteration 5905 loss 2.4902079105377197, acc 28.125\n",
      "iteration 5906 loss 2.6840388774871826, acc 21.875\n",
      "iteration 5907 loss 2.5699567794799805, acc 28.125\n",
      "iteration 5908 loss 2.697650194168091, acc 23.4375\n",
      "iteration 5909 loss 2.8733365535736084, acc 17.1875\n",
      "iteration 5910 loss 2.541538715362549, acc 31.25\n",
      "iteration 5911 loss 2.3426802158355713, acc 32.8125\n",
      "iteration 5912 loss 2.4716649055480957, acc 29.6875\n",
      "iteration 5913 loss 2.5901501178741455, acc 23.4375\n",
      "iteration 5914 loss 2.767273426055908, acc 21.875\n",
      "iteration 5915 loss 2.7697441577911377, acc 23.4375\n",
      "iteration 5916 loss 2.668564558029175, acc 18.75\n",
      "iteration 5917 loss 2.772174596786499, acc 21.875\n",
      "iteration 5918 loss 2.4951701164245605, acc 28.125\n",
      "iteration 5919 loss 2.639747381210327, acc 18.75\n",
      "iteration 5920 loss 2.604381799697876, acc 29.6875\n",
      "iteration 5921 loss 2.757443904876709, acc 18.75\n",
      "iteration 5922 loss 2.730074167251587, acc 20.3125\n",
      "iteration 5923 loss 2.7983286380767822, acc 23.4375\n",
      "iteration 5924 loss 2.837897539138794, acc 18.75\n",
      "iteration 5925 loss 2.702300548553467, acc 21.875\n",
      "iteration 5926 loss 2.5540764331817627, acc 21.875\n",
      "iteration 5927 loss 2.719147205352783, acc 17.1875\n",
      "iteration 5928 loss 2.792778730392456, acc 26.5625\n",
      "iteration 5929 loss 2.7482054233551025, acc 17.1875\n",
      "iteration 5930 loss 2.5072829723358154, acc 28.125\n",
      "iteration 5931 loss 2.603945016860962, acc 25.0\n",
      "iteration 5932 loss 2.6638410091400146, acc 23.4375\n",
      "iteration 5933 loss 2.709223747253418, acc 25.0\n",
      "iteration 5934 loss 2.680326223373413, acc 29.6875\n",
      "iteration 5935 loss 2.780212640762329, acc 17.1875\n",
      "iteration 5936 loss 2.7211525440216064, acc 21.875\n",
      "iteration 5937 loss 2.8402271270751953, acc 9.375\n",
      "iteration 5938 loss 2.859773874282837, acc 12.5\n",
      "iteration 5939 loss 2.533889055252075, acc 21.875\n",
      "iteration 5940 loss 2.635836601257324, acc 23.4375\n",
      "iteration 5941 loss 2.57071590423584, acc 20.3125\n",
      "iteration 5942 loss 2.697655439376831, acc 18.75\n",
      "iteration 5943 loss 2.711799144744873, acc 20.3125\n",
      "iteration 5944 loss 2.7839341163635254, acc 18.75\n",
      "iteration 5945 loss 2.6686670780181885, acc 23.4375\n",
      "iteration 5946 loss 2.5797345638275146, acc 26.5625\n",
      "iteration 5947 loss 2.4490320682525635, acc 32.8125\n",
      "iteration 5948 loss 2.9499659538269043, acc 17.1875\n",
      "iteration 5949 loss 2.937433958053589, acc 18.75\n",
      "iteration 5950 loss 2.4422318935394287, acc 25.0\n",
      "iteration 5951 loss 2.755356550216675, acc 10.9375\n",
      "iteration 5952 loss 2.682581901550293, acc 12.5\n",
      "iteration 5953 loss 2.725975275039673, acc 4.6875\n",
      "iteration 5954 loss 2.6161324977874756, acc 20.3125\n",
      "iteration 5955 loss 2.839629650115967, acc 20.3125\n",
      "iteration 5956 loss 2.5173897743225098, acc 29.6875\n",
      "iteration 5957 loss 2.5247397422790527, acc 23.4375\n",
      "iteration 5958 loss 2.731539726257324, acc 12.5\n",
      "iteration 5959 loss 2.6280221939086914, acc 28.125\n",
      "iteration 5960 loss 2.5281386375427246, acc 29.6875\n",
      "iteration 5961 loss 2.6967530250549316, acc 21.875\n",
      "iteration 5962 loss 2.658282995223999, acc 18.75\n",
      "iteration 5963 loss 2.5959551334381104, acc 14.0625\n",
      "iteration 5964 loss 2.940432548522949, acc 9.375\n",
      "iteration 5965 loss 2.5073230266571045, acc 23.4375\n",
      "iteration 5966 loss 2.739933729171753, acc 7.8125\n",
      "iteration 5967 loss 2.7445569038391113, acc 21.875\n",
      "iteration 5968 loss 2.740114450454712, acc 17.1875\n",
      "iteration 5969 loss 2.6793389320373535, acc 28.125\n",
      "iteration 5970 loss 2.6410555839538574, acc 21.875\n",
      "iteration 5971 loss 2.6316990852355957, acc 26.5625\n",
      "iteration 5972 loss 2.6274807453155518, acc 25.0\n",
      "iteration 5973 loss 2.5545380115509033, acc 23.4375\n",
      "iteration 5974 loss 2.6915080547332764, acc 20.3125\n",
      "iteration 5975 loss 2.5325121879577637, acc 29.6875\n",
      "iteration 5976 loss 2.7051756381988525, acc 21.875\n",
      "iteration 5977 loss 2.473876714706421, acc 35.9375\n",
      "iteration 5978 loss 2.564659833908081, acc 17.1875\n",
      "iteration 5979 loss 2.789160966873169, acc 21.875\n",
      "iteration 5980 loss 2.8341050148010254, acc 17.1875\n",
      "iteration 5981 loss 2.7246639728546143, acc 20.3125\n",
      "iteration 5982 loss 2.8761160373687744, acc 15.625\n",
      "iteration 5983 loss 2.611222505569458, acc 25.0\n",
      "iteration 5984 loss 2.612635612487793, acc 31.25\n",
      "iteration 5985 loss 2.5979228019714355, acc 21.875\n",
      "iteration 5986 loss 2.4355602264404297, acc 34.375\n",
      "iteration 5987 loss 2.5232958793640137, acc 25.0\n",
      "iteration 5988 loss 2.8879587650299072, acc 9.375\n",
      "iteration 5989 loss 2.8572824001312256, acc 15.625\n",
      "iteration 5990 loss 2.8600215911865234, acc 18.75\n",
      "iteration 5991 loss 2.759248733520508, acc 17.1875\n",
      "iteration 5992 loss 2.540456533432007, acc 26.5625\n",
      "iteration 5993 loss 2.5137298107147217, acc 34.375\n",
      "iteration 5994 loss 2.8279523849487305, acc 23.4375\n",
      "iteration 5995 loss 2.545442819595337, acc 26.5625\n",
      "iteration 5996 loss 2.808656692504883, acc 10.9375\n",
      "iteration 5997 loss 2.7139294147491455, acc 10.9375\n",
      "iteration 5998 loss 2.6474556922912598, acc 15.625\n",
      "iteration 5999 loss 2.6316919326782227, acc 29.6875\n",
      "iteration 6000 loss 2.851860761642456, acc 15.625\n",
      "iteration 6001 loss 2.7348899841308594, acc 21.875\n",
      "iteration 6002 loss 2.5905895233154297, acc 23.4375\n",
      "iteration 6003 loss 2.367222309112549, acc 37.5\n",
      "iteration 6004 loss 2.929600238800049, acc 10.9375\n",
      "iteration 6005 loss 2.7074227333068848, acc 17.1875\n",
      "iteration 6006 loss 2.6551570892333984, acc 25.0\n",
      "iteration 6007 loss 2.687408447265625, acc 28.125\n",
      "iteration 6008 loss 2.7388105392456055, acc 28.125\n",
      "iteration 6009 loss 2.8830041885375977, acc 14.0625\n",
      "iteration 6010 loss 2.5990183353424072, acc 23.4375\n",
      "iteration 6011 loss 2.4881036281585693, acc 29.6875\n",
      "iteration 6012 loss 2.808295726776123, acc 15.625\n",
      "iteration 6013 loss 2.689046859741211, acc 20.3125\n",
      "iteration 6014 loss 2.4651832580566406, acc 31.25\n",
      "iteration 6015 loss 2.4816079139709473, acc 28.125\n",
      "iteration 6016 loss 2.686556339263916, acc 23.4375\n",
      "iteration 6017 loss 2.7681665420532227, acc 23.4375\n",
      "iteration 6018 loss 2.6951417922973633, acc 26.5625\n",
      "iteration 6019 loss 2.8585519790649414, acc 12.5\n",
      "iteration 6020 loss 2.603935718536377, acc 21.875\n",
      "iteration 6021 loss 2.7991907596588135, acc 21.875\n",
      "iteration 6022 loss 2.4680445194244385, acc 28.125\n",
      "iteration 6023 loss 2.6639091968536377, acc 15.625\n",
      "iteration 6024 loss 2.570267915725708, acc 21.875\n",
      "iteration 6025 loss 2.689784526824951, acc 20.3125\n",
      "iteration 6026 loss 2.725085496902466, acc 17.1875\n",
      "iteration 6027 loss 2.71044921875, acc 20.3125\n",
      "iteration 6028 loss 2.640939235687256, acc 21.875\n",
      "iteration 6029 loss 2.5294981002807617, acc 17.1875\n",
      "iteration 6030 loss 2.6606273651123047, acc 15.625\n",
      "iteration 6031 loss 2.7128775119781494, acc 10.9375\n",
      "iteration 6032 loss 2.8049845695495605, acc 12.5\n",
      "iteration 6033 loss 2.509458303451538, acc 25.0\n",
      "iteration 6034 loss 2.8184142112731934, acc 23.4375\n",
      "iteration 6035 loss 2.6317389011383057, acc 20.3125\n",
      "iteration 6036 loss 2.665952205657959, acc 28.125\n",
      "iteration 6037 loss 2.606370210647583, acc 21.875\n",
      "iteration 6038 loss 2.666778564453125, acc 21.875\n",
      "iteration 6039 loss 2.641698122024536, acc 25.0\n",
      "iteration 6040 loss 2.794175624847412, acc 15.625\n",
      "iteration 6041 loss 2.742870330810547, acc 20.3125\n",
      "iteration 6042 loss 2.8570549488067627, acc 15.625\n",
      "iteration 6043 loss 2.565842628479004, acc 21.875\n",
      "iteration 6044 loss 2.6641268730163574, acc 17.1875\n",
      "iteration 6045 loss 2.700814723968506, acc 21.875\n",
      "iteration 6046 loss 2.7078914642333984, acc 21.875\n",
      "iteration 6047 loss 2.7398147583007812, acc 26.5625\n",
      "iteration 6048 loss 2.6032819747924805, acc 23.4375\n",
      "iteration 6049 loss 2.8481717109680176, acc 25.0\n",
      "iteration 6050 loss 2.921574831008911, acc 15.625\n",
      "iteration 6051 loss 2.6257436275482178, acc 25.0\n",
      "iteration 6052 loss 2.570007801055908, acc 25.0\n",
      "iteration 6053 loss 2.633894681930542, acc 18.75\n",
      "iteration 6054 loss 2.7187044620513916, acc 14.0625\n",
      "iteration 6055 loss 2.544126510620117, acc 25.0\n",
      "iteration 6056 loss 2.5833888053894043, acc 31.25\n",
      "iteration 6057 loss 2.73714280128479, acc 20.3125\n",
      "iteration 6058 loss 2.810637950897217, acc 21.875\n",
      "iteration 6059 loss 2.675278902053833, acc 20.3125\n",
      "iteration 6060 loss 2.6496546268463135, acc 23.4375\n",
      "iteration 6061 loss 2.856809139251709, acc 17.1875\n",
      "iteration 6062 loss 2.679487466812134, acc 18.75\n",
      "iteration 6063 loss 2.6742470264434814, acc 23.4375\n",
      "iteration 6064 loss 2.7123537063598633, acc 17.1875\n",
      "iteration 6065 loss 2.7261502742767334, acc 20.3125\n",
      "iteration 6066 loss 2.963498115539551, acc 20.3125\n",
      "iteration 6067 loss 2.6692492961883545, acc 26.5625\n",
      "iteration 6068 loss 2.352205753326416, acc 29.6875\n",
      "iteration 6069 loss 2.653186321258545, acc 20.3125\n",
      "iteration 6070 loss 2.666419267654419, acc 28.125\n",
      "iteration 6071 loss 2.723024368286133, acc 21.875\n",
      "iteration 6072 loss 2.640810251235962, acc 21.875\n",
      "iteration 6073 loss 2.4942541122436523, acc 21.875\n",
      "iteration 6074 loss 3.0603673458099365, acc 10.9375\n",
      "iteration 6075 loss 2.6065685749053955, acc 28.125\n",
      "iteration 6076 loss 2.7841217517852783, acc 21.875\n",
      "iteration 6077 loss 2.718036413192749, acc 21.875\n",
      "iteration 6078 loss 2.6666178703308105, acc 18.75\n",
      "iteration 6079 loss 2.9405221939086914, acc 20.3125\n",
      "iteration 6080 loss 2.7299246788024902, acc 26.5625\n",
      "iteration 6081 loss 2.6385912895202637, acc 21.875\n",
      "iteration 6082 loss 2.6115167140960693, acc 15.625\n",
      "iteration 6083 loss 2.720128059387207, acc 14.0625\n",
      "iteration 6084 loss 2.555266857147217, acc 18.75\n",
      "iteration 6085 loss 2.8239753246307373, acc 23.4375\n",
      "iteration 6086 loss 2.50691819190979, acc 26.5625\n",
      "iteration 6087 loss 2.5962061882019043, acc 25.0\n",
      "iteration 6088 loss 2.6675162315368652, acc 14.0625\n",
      "iteration 6089 loss 2.5714781284332275, acc 21.875\n",
      "iteration 6090 loss 2.5001697540283203, acc 31.25\n",
      "iteration 6091 loss 2.5850653648376465, acc 17.1875\n",
      "iteration 6092 loss 2.873568058013916, acc 17.1875\n",
      "iteration 6093 loss 2.7376692295074463, acc 15.625\n",
      "iteration 6094 loss 2.689974308013916, acc 17.1875\n",
      "iteration 6095 loss 2.8381946086883545, acc 10.9375\n",
      "iteration 6096 loss 2.705167293548584, acc 17.1875\n",
      "iteration 6097 loss 2.8545308113098145, acc 14.0625\n",
      "iteration 6098 loss 2.8615357875823975, acc 15.625\n",
      "iteration 6099 loss 2.5813984870910645, acc 23.4375\n",
      "iteration 6100 loss 2.672555685043335, acc 21.875\n",
      "iteration 6101 loss 2.5273420810699463, acc 25.0\n",
      "iteration 6102 loss 2.6344385147094727, acc 25.0\n",
      "iteration 6103 loss 2.640368700027466, acc 20.3125\n",
      "iteration 6104 loss 2.8035974502563477, acc 21.875\n",
      "iteration 6105 loss 2.973926067352295, acc 18.75\n",
      "iteration 6106 loss 2.9045891761779785, acc 20.3125\n",
      "iteration 6107 loss 2.76119065284729, acc 23.4375\n",
      "iteration 6108 loss 2.5857672691345215, acc 32.8125\n",
      "iteration 6109 loss 2.6575586795806885, acc 26.5625\n",
      "iteration 6110 loss 2.610163688659668, acc 21.875\n",
      "iteration 6111 loss 2.783128499984741, acc 12.5\n",
      "iteration 6112 loss 2.845085620880127, acc 14.0625\n",
      "iteration 6113 loss 2.7235686779022217, acc 15.625\n",
      "iteration 6114 loss 2.504183530807495, acc 15.625\n",
      "iteration 6115 loss 2.6774542331695557, acc 23.4375\n",
      "iteration 6116 loss 2.6002938747406006, acc 23.4375\n",
      "iteration 6117 loss 2.531437635421753, acc 21.875\n",
      "iteration 6118 loss 2.728487253189087, acc 18.75\n",
      "iteration 6119 loss 2.770604133605957, acc 18.75\n",
      "iteration 6120 loss 2.6654696464538574, acc 21.875\n",
      "iteration 6121 loss 2.604595899581909, acc 26.5625\n",
      "iteration 6122 loss 2.528207302093506, acc 25.0\n",
      "iteration 6123 loss 2.6038172245025635, acc 32.8125\n",
      "iteration 6124 loss 2.696328639984131, acc 18.75\n",
      "iteration 6125 loss 2.952822685241699, acc 18.75\n",
      "iteration 6126 loss 2.6322691440582275, acc 18.75\n",
      "iteration 6127 loss 2.570796012878418, acc 21.875\n",
      "iteration 6128 loss 2.8586835861206055, acc 10.9375\n",
      "iteration 6129 loss 2.5972414016723633, acc 15.625\n",
      "iteration 6130 loss 2.6933863162994385, acc 26.5625\n",
      "iteration 6131 loss 2.751575469970703, acc 17.1875\n",
      "iteration 6132 loss 2.775256395339966, acc 25.0\n",
      "iteration 6133 loss 2.779297351837158, acc 10.9375\n",
      "iteration 6134 loss 2.748534679412842, acc 21.875\n",
      "iteration 6135 loss 2.889873504638672, acc 17.1875\n",
      "iteration 6136 loss 2.757643222808838, acc 21.875\n",
      "iteration 6137 loss 2.732919454574585, acc 18.75\n",
      "iteration 6138 loss 2.870715856552124, acc 12.5\n",
      "iteration 6139 loss 2.6835567951202393, acc 20.3125\n",
      "iteration 6140 loss 2.6854560375213623, acc 20.3125\n",
      "iteration 6141 loss 2.975578784942627, acc 12.5\n",
      "iteration 6142 loss 2.6508586406707764, acc 34.375\n",
      "iteration 6143 loss 2.741292953491211, acc 23.4375\n",
      "iteration 6144 loss 2.7599382400512695, acc 18.75\n",
      "iteration 6145 loss 2.8795673847198486, acc 18.75\n",
      "iteration 6146 loss 2.8224377632141113, acc 18.75\n",
      "iteration 6147 loss 2.3695480823516846, acc 40.625\n",
      "iteration 6148 loss 2.7350010871887207, acc 21.875\n",
      "iteration 6149 loss 2.7748680114746094, acc 15.625\n",
      "iteration 6150 loss 2.6347107887268066, acc 21.875\n",
      "iteration 6151 loss 2.641653060913086, acc 28.125\n",
      "iteration 6152 loss 2.4464876651763916, acc 23.4375\n",
      "iteration 6153 loss 2.833329677581787, acc 7.8125\n",
      "iteration 6154 loss 2.778313636779785, acc 21.875\n",
      "iteration 6155 loss 2.6153359413146973, acc 25.0\n",
      "iteration 6156 loss 2.6179797649383545, acc 23.4375\n",
      "iteration 6157 loss 2.6766016483306885, acc 26.5625\n",
      "iteration 6158 loss 2.5314300060272217, acc 28.125\n",
      "iteration 6159 loss 2.514773368835449, acc 31.25\n",
      "iteration 6160 loss 2.8446085453033447, acc 20.3125\n",
      "iteration 6161 loss 2.6864614486694336, acc 23.4375\n",
      "iteration 6162 loss 2.668182611465454, acc 25.0\n",
      "iteration 6163 loss 2.6102709770202637, acc 23.4375\n",
      "iteration 6164 loss 2.675377607345581, acc 21.875\n",
      "iteration 6165 loss 2.906919240951538, acc 15.625\n",
      "iteration 6166 loss 2.7137339115142822, acc 14.0625\n",
      "iteration 6167 loss 2.672318458557129, acc 21.875\n",
      "iteration 6168 loss 2.6878933906555176, acc 20.3125\n",
      "iteration 6169 loss 2.6210498809814453, acc 31.25\n",
      "iteration 6170 loss 2.610771894454956, acc 26.5625\n",
      "iteration 6171 loss 3.046527624130249, acc 15.625\n",
      "iteration 6172 loss 2.629981756210327, acc 34.375\n",
      "iteration 6173 loss 2.738555908203125, acc 15.625\n",
      "iteration 6174 loss 2.670186996459961, acc 18.75\n",
      "iteration 6175 loss 2.7493014335632324, acc 15.625\n",
      "iteration 6176 loss 2.649298667907715, acc 12.5\n",
      "iteration 6177 loss 2.8866586685180664, acc 17.1875\n",
      "iteration 6178 loss 2.511017322540283, acc 25.0\n",
      "iteration 6179 loss 2.83191180229187, acc 18.75\n",
      "iteration 6180 loss 2.559220314025879, acc 23.4375\n",
      "iteration 6181 loss 2.781215190887451, acc 21.875\n",
      "iteration 6182 loss 2.698747396469116, acc 23.4375\n",
      "iteration 6183 loss 2.623474359512329, acc 21.875\n",
      "iteration 6184 loss 2.62785005569458, acc 25.0\n",
      "iteration 6185 loss 2.7300150394439697, acc 23.4375\n",
      "iteration 6186 loss 2.511965036392212, acc 29.6875\n",
      "iteration 6187 loss 2.5435986518859863, acc 26.5625\n",
      "iteration 6188 loss 2.660346746444702, acc 20.3125\n",
      "iteration 6189 loss 2.540005922317505, acc 31.25\n",
      "iteration 6190 loss 2.560896396636963, acc 20.3125\n",
      "iteration 6191 loss 2.660900831222534, acc 17.1875\n",
      "iteration 6192 loss 2.6618151664733887, acc 20.3125\n",
      "iteration 6193 loss 2.776061773300171, acc 18.75\n",
      "iteration 6194 loss 2.7403581142425537, acc 17.1875\n",
      "iteration 6195 loss 2.590266227722168, acc 15.625\n",
      "iteration 6196 loss 2.826354742050171, acc 10.9375\n",
      "iteration 6197 loss 2.6106486320495605, acc 23.4375\n",
      "iteration 6198 loss 2.6236767768859863, acc 25.0\n",
      "iteration 6199 loss 2.6629526615142822, acc 20.3125\n",
      "iteration 6200 loss 2.732261896133423, acc 18.75\n",
      "iteration 6201 loss 2.7783071994781494, acc 20.3125\n",
      "iteration 6202 loss 2.871694326400757, acc 23.4375\n",
      "iteration 6203 loss 2.7076265811920166, acc 29.6875\n",
      "iteration 6204 loss 2.604602813720703, acc 21.875\n",
      "iteration 6205 loss 2.835616111755371, acc 20.3125\n",
      "iteration 6206 loss 2.5595569610595703, acc 26.5625\n",
      "iteration 6207 loss 2.8084373474121094, acc 17.1875\n",
      "iteration 6208 loss 2.583677291870117, acc 21.875\n",
      "iteration 6209 loss 2.62241268157959, acc 17.1875\n",
      "iteration 6210 loss 2.7817416191101074, acc 21.875\n",
      "iteration 6211 loss 2.7396953105926514, acc 21.875\n",
      "iteration 6212 loss 2.5379576683044434, acc 31.25\n",
      "iteration 6213 loss 2.6303751468658447, acc 26.5625\n",
      "iteration 6214 loss 2.7646920680999756, acc 15.625\n",
      "iteration 6215 loss 2.7822506427764893, acc 18.75\n",
      "iteration 6216 loss 2.5222983360290527, acc 25.0\n",
      "iteration 6217 loss 2.8631997108459473, acc 15.625\n",
      "iteration 6218 loss 2.5282976627349854, acc 25.0\n",
      "iteration 6219 loss 2.7387378215789795, acc 17.1875\n",
      "iteration 6220 loss 2.5620646476745605, acc 20.3125\n",
      "iteration 6221 loss 2.6817104816436768, acc 17.1875\n",
      "iteration 6222 loss 2.612271547317505, acc 28.125\n",
      "iteration 6223 loss 2.510096549987793, acc 28.125\n",
      "iteration 6224 loss 2.709327220916748, acc 25.0\n",
      "iteration 6225 loss 2.665057420730591, acc 25.0\n",
      "iteration 6226 loss 2.8095972537994385, acc 15.625\n",
      "iteration 6227 loss 2.517897129058838, acc 23.4375\n",
      "iteration 6228 loss 2.589033365249634, acc 25.0\n",
      "iteration 6229 loss 2.5473172664642334, acc 26.5625\n",
      "iteration 6230 loss 2.7971649169921875, acc 20.3125\n",
      "iteration 6231 loss 2.8401994705200195, acc 25.0\n",
      "iteration 6232 loss 2.982511520385742, acc 18.75\n",
      "iteration 6233 loss 2.6194424629211426, acc 26.5625\n",
      "iteration 6234 loss 2.7975666522979736, acc 17.1875\n",
      "iteration 6235 loss 2.576385498046875, acc 21.875\n",
      "iteration 6236 loss 2.5330309867858887, acc 28.125\n",
      "iteration 6237 loss 2.65458083152771, acc 18.75\n",
      "iteration 6238 loss 2.6635336875915527, acc 29.6875\n",
      "iteration 6239 loss 2.7448859214782715, acc 23.4375\n",
      "iteration 6240 loss 2.7339820861816406, acc 21.875\n",
      "iteration 6241 loss 2.6978304386138916, acc 25.0\n",
      "iteration 6242 loss 2.5977463722229004, acc 28.125\n",
      "iteration 6243 loss 2.5255839824676514, acc 28.125\n",
      "iteration 6244 loss 2.616872549057007, acc 31.25\n",
      "iteration 6245 loss 2.7849388122558594, acc 23.4375\n",
      "iteration 6246 loss 2.7834107875823975, acc 21.875\n",
      "iteration 6247 loss 2.7332587242126465, acc 20.3125\n",
      "iteration 6248 loss 2.644217014312744, acc 21.875\n",
      "iteration 6249 loss 2.7796742916107178, acc 21.875\n",
      "iteration 6250 loss 2.651364326477051, acc 23.4375\n",
      "iteration 6251 loss 2.6691884994506836, acc 23.4375\n",
      "iteration 6252 loss 2.354842185974121, acc 34.375\n",
      "iteration 6253 loss 2.6935601234436035, acc 28.125\n",
      "iteration 6254 loss 2.7203292846679688, acc 23.4375\n",
      "iteration 6255 loss 2.876124858856201, acc 10.9375\n",
      "iteration 6256 loss 2.7206015586853027, acc 26.5625\n",
      "iteration 6257 loss 2.8549811840057373, acc 18.75\n",
      "iteration 6258 loss 2.4897079467773438, acc 34.375\n",
      "iteration 6259 loss 2.7282466888427734, acc 17.1875\n",
      "iteration 6260 loss 2.781525135040283, acc 25.0\n",
      "iteration 6261 loss 2.806863784790039, acc 18.75\n",
      "iteration 6262 loss 2.816007375717163, acc 9.375\n",
      "iteration 6263 loss 2.711702585220337, acc 14.0625\n",
      "iteration 6264 loss 2.6484291553497314, acc 17.1875\n",
      "iteration 6265 loss 2.817077398300171, acc 18.75\n",
      "iteration 6266 loss 2.528958559036255, acc 29.6875\n",
      "iteration 6267 loss 2.6556949615478516, acc 26.5625\n",
      "iteration 6268 loss 2.8772683143615723, acc 20.3125\n",
      "iteration 6269 loss 2.7404093742370605, acc 21.875\n",
      "iteration 6270 loss 2.6983211040496826, acc 20.3125\n",
      "iteration 6271 loss 2.579643726348877, acc 26.5625\n",
      "iteration 6272 loss 2.672767162322998, acc 23.4375\n",
      "iteration 6273 loss 2.553830623626709, acc 26.5625\n",
      "iteration 6274 loss 2.777054786682129, acc 20.3125\n",
      "iteration 6275 loss 2.585540533065796, acc 25.0\n",
      "iteration 6276 loss 2.8033695220947266, acc 15.625\n",
      "iteration 6277 loss 2.7126834392547607, acc 21.875\n",
      "iteration 6278 loss 2.5667648315429688, acc 14.0625\n",
      "iteration 6279 loss 2.5233256816864014, acc 18.75\n",
      "iteration 6280 loss 2.7177584171295166, acc 15.625\n",
      "iteration 6281 loss 2.5953547954559326, acc 21.875\n",
      "iteration 6282 loss 2.7748513221740723, acc 15.625\n",
      "iteration 6283 loss 2.544584035873413, acc 20.3125\n",
      "iteration 6284 loss 2.6906471252441406, acc 18.75\n",
      "iteration 6285 loss 2.5632033348083496, acc 23.4375\n",
      "iteration 6286 loss 2.8025221824645996, acc 15.625\n",
      "iteration 6287 loss 2.715451955795288, acc 20.3125\n",
      "iteration 6288 loss 2.660151243209839, acc 20.3125\n",
      "iteration 6289 loss 2.6663053035736084, acc 23.4375\n",
      "iteration 6290 loss 2.7780165672302246, acc 14.0625\n",
      "iteration 6291 loss 2.7716622352600098, acc 17.1875\n",
      "iteration 6292 loss 2.607347249984741, acc 23.4375\n",
      "iteration 6293 loss 2.810197353363037, acc 14.0625\n",
      "iteration 6294 loss 2.767841100692749, acc 14.0625\n",
      "iteration 6295 loss 2.719597101211548, acc 9.375\n",
      "iteration 6296 loss 2.503020763397217, acc 21.875\n",
      "iteration 6297 loss 2.7311739921569824, acc 17.1875\n",
      "iteration 6298 loss 2.724964141845703, acc 17.1875\n",
      "iteration 6299 loss 2.5753917694091797, acc 26.5625\n",
      "iteration 6300 loss 2.760746955871582, acc 20.3125\n",
      "iteration 6301 loss 2.794847249984741, acc 20.3125\n",
      "iteration 6302 loss 2.622623920440674, acc 18.75\n",
      "iteration 6303 loss 2.5382184982299805, acc 34.375\n",
      "iteration 6304 loss 2.7682876586914062, acc 17.1875\n",
      "iteration 6305 loss 2.425490140914917, acc 32.8125\n",
      "iteration 6306 loss 2.416217803955078, acc 32.8125\n",
      "iteration 6307 loss 2.730898857116699, acc 23.4375\n",
      "iteration 6308 loss 2.834770441055298, acc 18.75\n",
      "iteration 6309 loss 2.6133532524108887, acc 25.0\n",
      "iteration 6310 loss 2.7831850051879883, acc 20.3125\n",
      "iteration 6311 loss 2.8325514793395996, acc 12.5\n",
      "iteration 6312 loss 2.600262403488159, acc 26.5625\n",
      "iteration 6313 loss 2.66011118888855, acc 15.625\n",
      "iteration 6314 loss 2.5350053310394287, acc 28.125\n",
      "iteration 6315 loss 2.6841251850128174, acc 17.1875\n",
      "iteration 6316 loss 2.6320414543151855, acc 20.3125\n",
      "iteration 6317 loss 2.858884572982788, acc 21.875\n",
      "iteration 6318 loss 2.956683874130249, acc 17.1875\n",
      "iteration 6319 loss 2.919804096221924, acc 18.75\n",
      "iteration 6320 loss 2.658940315246582, acc 29.6875\n",
      "iteration 6321 loss 2.6312496662139893, acc 26.5625\n",
      "iteration 6322 loss 2.716564178466797, acc 21.875\n",
      "iteration 6323 loss 2.6931896209716797, acc 23.4375\n",
      "iteration 6324 loss 2.5615270137786865, acc 17.1875\n",
      "iteration 6325 loss 2.8036701679229736, acc 15.625\n",
      "iteration 6326 loss 2.763322353363037, acc 18.75\n",
      "iteration 6327 loss 2.610308885574341, acc 21.875\n",
      "iteration 6328 loss 2.581688404083252, acc 23.4375\n",
      "iteration 6329 loss 3.0767550468444824, acc 17.1875\n",
      "iteration 6330 loss 2.8146138191223145, acc 14.0625\n",
      "iteration 6331 loss 2.8280837535858154, acc 20.3125\n",
      "iteration 6332 loss 2.945359706878662, acc 10.9375\n",
      "iteration 6333 loss 2.866605043411255, acc 28.125\n",
      "iteration 6334 loss 2.5746006965637207, acc 23.4375\n",
      "iteration 6335 loss 2.7763590812683105, acc 23.4375\n",
      "iteration 6336 loss 2.656738519668579, acc 21.875\n",
      "iteration 6337 loss 2.811708927154541, acc 14.0625\n",
      "iteration 6338 loss 2.748800277709961, acc 26.5625\n",
      "iteration 6339 loss 2.7877795696258545, acc 21.875\n",
      "iteration 6340 loss 2.6419332027435303, acc 23.4375\n",
      "iteration 6341 loss 2.7149109840393066, acc 25.0\n",
      "iteration 6342 loss 2.844602584838867, acc 21.875\n",
      "iteration 6343 loss 2.7153053283691406, acc 23.4375\n",
      "iteration 6344 loss 2.5863540172576904, acc 29.6875\n",
      "iteration 6345 loss 2.8388779163360596, acc 20.3125\n",
      "iteration 6346 loss 2.7156319618225098, acc 21.875\n",
      "iteration 6347 loss 2.538102388381958, acc 20.3125\n",
      "iteration 6348 loss 2.5464391708374023, acc 25.0\n",
      "iteration 6349 loss 2.7683444023132324, acc 21.875\n",
      "iteration 6350 loss 2.6979780197143555, acc 21.875\n",
      "iteration 6351 loss 2.6282551288604736, acc 23.4375\n",
      "iteration 6352 loss 2.802605628967285, acc 20.3125\n",
      "iteration 6353 loss 2.76086163520813, acc 12.5\n",
      "iteration 6354 loss 2.6324703693389893, acc 21.875\n",
      "iteration 6355 loss 2.809009552001953, acc 14.0625\n",
      "iteration 6356 loss 2.7694666385650635, acc 20.3125\n",
      "iteration 6357 loss 2.8197996616363525, acc 17.1875\n",
      "iteration 6358 loss 2.5485575199127197, acc 25.0\n",
      "iteration 6359 loss 2.755932331085205, acc 12.5\n",
      "iteration 6360 loss 2.6876943111419678, acc 18.75\n",
      "iteration 6361 loss 2.7386255264282227, acc 21.875\n",
      "iteration 6362 loss 2.899812936782837, acc 20.3125\n",
      "iteration 6363 loss 2.63822603225708, acc 20.3125\n",
      "iteration 6364 loss 2.386645793914795, acc 31.25\n",
      "iteration 6365 loss 2.3671696186065674, acc 32.8125\n",
      "iteration 6366 loss 2.7930960655212402, acc 14.0625\n",
      "iteration 6367 loss 2.602339267730713, acc 18.75\n",
      "iteration 6368 loss 2.6707048416137695, acc 26.5625\n",
      "iteration 6369 loss 2.5833122730255127, acc 20.3125\n",
      "iteration 6370 loss 2.537886619567871, acc 23.4375\n",
      "iteration 6371 loss 2.4926655292510986, acc 28.125\n",
      "iteration 6372 loss 2.8268980979919434, acc 17.1875\n",
      "iteration 6373 loss 2.7052371501922607, acc 23.4375\n",
      "iteration 6374 loss 2.6423044204711914, acc 23.4375\n",
      "iteration 6375 loss 2.5467703342437744, acc 25.0\n",
      "iteration 6376 loss 2.7121620178222656, acc 23.4375\n",
      "iteration 6377 loss 2.857539653778076, acc 15.625\n",
      "iteration 6378 loss 2.6015403270721436, acc 18.75\n",
      "iteration 6379 loss 2.7895569801330566, acc 20.3125\n",
      "iteration 6380 loss 2.5202038288116455, acc 23.4375\n",
      "iteration 6381 loss 2.906888008117676, acc 14.0625\n",
      "iteration 6382 loss 2.6572704315185547, acc 21.875\n",
      "iteration 6383 loss 2.585805654525757, acc 32.8125\n",
      "iteration 6384 loss 2.7255632877349854, acc 15.625\n",
      "iteration 6385 loss 2.712538480758667, acc 6.25\n",
      "iteration 6386 loss 2.7200019359588623, acc 23.4375\n",
      "iteration 6387 loss 2.8720808029174805, acc 15.625\n",
      "iteration 6388 loss 2.4573614597320557, acc 23.4375\n",
      "iteration 6389 loss 2.8801112174987793, acc 20.3125\n",
      "iteration 6390 loss 2.917921781539917, acc 10.9375\n",
      "iteration 6391 loss 2.6722049713134766, acc 20.3125\n",
      "iteration 6392 loss 2.909116744995117, acc 17.1875\n",
      "iteration 6393 loss 2.6413064002990723, acc 18.75\n",
      "iteration 6394 loss 2.504647731781006, acc 29.6875\n",
      "iteration 6395 loss 2.655452013015747, acc 25.0\n",
      "iteration 6396 loss 2.7143895626068115, acc 21.875\n",
      "iteration 6397 loss 2.771084785461426, acc 17.1875\n",
      "iteration 6398 loss 2.866568088531494, acc 17.1875\n",
      "iteration 6399 loss 2.562901258468628, acc 23.4375\n",
      "iteration 6400 loss 2.660597562789917, acc 20.3125\n",
      "iteration 6401 loss 2.7227466106414795, acc 23.4375\n",
      "iteration 6402 loss 2.689427614212036, acc 23.4375\n",
      "iteration 6403 loss 2.793386220932007, acc 20.3125\n",
      "iteration 6404 loss 2.542603015899658, acc 23.4375\n",
      "iteration 6405 loss 2.82259202003479, acc 23.4375\n",
      "iteration 6406 loss 2.772987127304077, acc 23.4375\n",
      "iteration 6407 loss 2.717416286468506, acc 23.4375\n",
      "iteration 6408 loss 2.738597869873047, acc 20.3125\n",
      "iteration 6409 loss 2.730844259262085, acc 15.625\n",
      "iteration 6410 loss 2.6615140438079834, acc 26.5625\n",
      "iteration 6411 loss 2.704596519470215, acc 15.625\n",
      "iteration 6412 loss 2.697112560272217, acc 25.0\n",
      "iteration 6413 loss 2.6422157287597656, acc 23.4375\n",
      "iteration 6414 loss 2.7139089107513428, acc 26.5625\n",
      "iteration 6415 loss 2.4376583099365234, acc 28.125\n",
      "iteration 6416 loss 2.467085838317871, acc 32.8125\n",
      "iteration 6417 loss 2.6088528633117676, acc 29.6875\n",
      "iteration 6418 loss 2.4908900260925293, acc 26.5625\n",
      "iteration 6419 loss 2.5392842292785645, acc 26.5625\n",
      "iteration 6420 loss 2.90390682220459, acc 10.9375\n",
      "iteration 6421 loss 2.5072803497314453, acc 28.125\n",
      "iteration 6422 loss 2.674652338027954, acc 18.75\n",
      "iteration 6423 loss 2.8817543983459473, acc 15.625\n",
      "iteration 6424 loss 2.8558268547058105, acc 23.4375\n",
      "iteration 6425 loss 2.6055188179016113, acc 21.875\n",
      "iteration 6426 loss 2.7010841369628906, acc 26.5625\n",
      "iteration 6427 loss 2.503473997116089, acc 28.125\n",
      "iteration 6428 loss 2.613191843032837, acc 26.5625\n",
      "iteration 6429 loss 2.6323940753936768, acc 20.3125\n",
      "iteration 6430 loss 2.9897661209106445, acc 15.625\n",
      "iteration 6431 loss 2.9213788509368896, acc 12.5\n",
      "iteration 6432 loss 2.6024136543273926, acc 23.4375\n",
      "iteration 6433 loss 2.7112581729888916, acc 25.0\n",
      "iteration 6434 loss 2.8675551414489746, acc 14.0625\n",
      "iteration 6435 loss 2.612217664718628, acc 23.4375\n",
      "iteration 6436 loss 2.6963412761688232, acc 23.4375\n",
      "iteration 6437 loss 2.7945590019226074, acc 21.875\n",
      "iteration 6438 loss 2.565336227416992, acc 25.0\n",
      "iteration 6439 loss 2.5134010314941406, acc 26.5625\n",
      "iteration 6440 loss 2.6121366024017334, acc 20.3125\n",
      "iteration 6441 loss 2.874689817428589, acc 17.1875\n",
      "iteration 6442 loss 2.7717881202697754, acc 15.625\n",
      "iteration 6443 loss 2.3879337310791016, acc 21.875\n",
      "iteration 6444 loss 2.7791478633880615, acc 17.1875\n",
      "iteration 6445 loss 2.6532845497131348, acc 21.875\n",
      "iteration 6446 loss 2.56685209274292, acc 31.25\n",
      "iteration 6447 loss 2.5503060817718506, acc 25.0\n",
      "iteration 6448 loss 2.4947543144226074, acc 28.125\n",
      "iteration 6449 loss 2.763169050216675, acc 21.875\n",
      "iteration 6450 loss 2.747325897216797, acc 21.875\n",
      "iteration 6451 loss 2.643690347671509, acc 25.0\n",
      "iteration 6452 loss 2.513042688369751, acc 26.5625\n",
      "iteration 6453 loss 2.5269649028778076, acc 29.6875\n",
      "iteration 6454 loss 2.619896173477173, acc 21.875\n",
      "iteration 6455 loss 2.91908597946167, acc 15.625\n",
      "iteration 6456 loss 2.5660598278045654, acc 26.5625\n",
      "iteration 6457 loss 2.5038604736328125, acc 23.4375\n",
      "iteration 6458 loss 2.7022533416748047, acc 23.4375\n",
      "iteration 6459 loss 2.6703383922576904, acc 21.875\n",
      "iteration 6460 loss 2.6245481967926025, acc 31.25\n",
      "iteration 6461 loss 2.7279956340789795, acc 25.0\n",
      "iteration 6462 loss 2.8088295459747314, acc 17.1875\n",
      "iteration 6463 loss 2.7342755794525146, acc 28.125\n",
      "iteration 6464 loss 2.4933927059173584, acc 28.125\n",
      "iteration 6465 loss 2.718409299850464, acc 23.4375\n",
      "iteration 6466 loss 2.672917604446411, acc 25.0\n",
      "iteration 6467 loss 2.6226184368133545, acc 23.4375\n",
      "iteration 6468 loss 2.6219735145568848, acc 26.5625\n",
      "iteration 6469 loss 2.6867756843566895, acc 21.875\n",
      "iteration 6470 loss 2.8234660625457764, acc 23.4375\n",
      "iteration 6471 loss 2.589841365814209, acc 25.0\n",
      "iteration 6472 loss 2.606092929840088, acc 25.0\n",
      "iteration 6473 loss 2.765166759490967, acc 20.3125\n",
      "iteration 6474 loss 2.8110480308532715, acc 18.75\n",
      "iteration 6475 loss 2.718014717102051, acc 23.4375\n",
      "iteration 6476 loss 2.7582826614379883, acc 21.875\n",
      "iteration 6477 loss 2.706418037414551, acc 12.5\n",
      "iteration 6478 loss 2.6519129276275635, acc 21.875\n",
      "iteration 6479 loss 2.8360726833343506, acc 14.0625\n",
      "iteration 6480 loss 2.7429778575897217, acc 15.625\n",
      "iteration 6481 loss 2.6611413955688477, acc 26.5625\n",
      "iteration 6482 loss 2.737614870071411, acc 14.0625\n",
      "iteration 6483 loss 2.6855366230010986, acc 20.3125\n",
      "iteration 6484 loss 2.8039801120758057, acc 12.5\n",
      "iteration 6485 loss 2.5875539779663086, acc 23.4375\n",
      "iteration 6486 loss 2.6166903972625732, acc 26.5625\n",
      "iteration 6487 loss 2.6943511962890625, acc 18.75\n",
      "iteration 6488 loss 2.8048055171966553, acc 18.75\n",
      "iteration 6489 loss 2.6892404556274414, acc 18.75\n",
      "iteration 6490 loss 2.702538013458252, acc 15.625\n",
      "iteration 6491 loss 2.6624631881713867, acc 25.0\n",
      "iteration 6492 loss 2.6706368923187256, acc 25.0\n",
      "iteration 6493 loss 2.7304186820983887, acc 15.625\n",
      "iteration 6494 loss 2.5529754161834717, acc 20.3125\n",
      "iteration 6495 loss 2.352966070175171, acc 25.0\n",
      "iteration 6496 loss 2.5813074111938477, acc 17.1875\n",
      "iteration 6497 loss 2.7491729259490967, acc 20.3125\n",
      "iteration 6498 loss 2.7738475799560547, acc 20.3125\n",
      "iteration 6499 loss 2.819334030151367, acc 6.25\n",
      "iteration 6500 loss 2.740466833114624, acc 23.4375\n",
      "iteration 6501 loss 2.6140098571777344, acc 17.1875\n",
      "iteration 6502 loss 2.79642653465271, acc 14.0625\n",
      "iteration 6503 loss 2.657677412033081, acc 15.625\n",
      "iteration 6504 loss 2.7315680980682373, acc 15.625\n",
      "iteration 6505 loss 2.654728889465332, acc 25.0\n",
      "iteration 6506 loss 2.6985316276550293, acc 26.5625\n",
      "iteration 6507 loss 2.588557481765747, acc 26.5625\n",
      "iteration 6508 loss 2.643491744995117, acc 23.4375\n",
      "iteration 6509 loss 2.810082197189331, acc 15.625\n",
      "iteration 6510 loss 2.648747444152832, acc 21.875\n",
      "iteration 6511 loss 2.6058666706085205, acc 26.5625\n",
      "iteration 6512 loss 2.6937801837921143, acc 25.0\n",
      "iteration 6513 loss 2.6181511878967285, acc 26.5625\n",
      "iteration 6514 loss 2.7547998428344727, acc 18.75\n",
      "iteration 6515 loss 2.6633894443511963, acc 25.0\n",
      "iteration 6516 loss 2.2556164264678955, acc 43.75\n",
      "iteration 6517 loss 2.6121795177459717, acc 21.875\n",
      "iteration 6518 loss 2.5262365341186523, acc 31.25\n",
      "iteration 6519 loss 2.924130439758301, acc 14.0625\n",
      "iteration 6520 loss 2.7755563259124756, acc 17.1875\n",
      "iteration 6521 loss 2.787522077560425, acc 23.4375\n",
      "iteration 6522 loss 2.934429168701172, acc 15.625\n",
      "iteration 6523 loss 2.72292423248291, acc 29.6875\n",
      "iteration 6524 loss 2.6296041011810303, acc 23.4375\n",
      "iteration 6525 loss 2.6249635219573975, acc 18.75\n",
      "iteration 6526 loss 2.5783252716064453, acc 21.875\n",
      "iteration 6527 loss 2.602185010910034, acc 26.5625\n",
      "iteration 6528 loss 2.7212469577789307, acc 25.0\n",
      "iteration 6529 loss 2.6517536640167236, acc 23.4375\n",
      "iteration 6530 loss 2.618511199951172, acc 26.5625\n",
      "iteration 6531 loss 2.673330307006836, acc 26.5625\n",
      "iteration 6532 loss 2.730600118637085, acc 20.3125\n",
      "iteration 6533 loss 2.61057186126709, acc 23.4375\n",
      "iteration 6534 loss 2.5607101917266846, acc 34.375\n",
      "iteration 6535 loss 2.6795222759246826, acc 20.3125\n",
      "iteration 6536 loss 2.6581716537475586, acc 23.4375\n",
      "iteration 6537 loss 2.879329204559326, acc 14.0625\n",
      "iteration 6538 loss 2.6649606227874756, acc 20.3125\n",
      "iteration 6539 loss 2.6724801063537598, acc 21.875\n",
      "iteration 6540 loss 2.5776779651641846, acc 21.875\n",
      "iteration 6541 loss 2.6472973823547363, acc 21.875\n",
      "iteration 6542 loss 2.6982905864715576, acc 23.4375\n",
      "iteration 6543 loss 2.686647891998291, acc 26.5625\n",
      "iteration 6544 loss 2.4528210163116455, acc 29.6875\n",
      "iteration 6545 loss 2.6374881267547607, acc 28.125\n",
      "iteration 6546 loss 2.657743453979492, acc 21.875\n",
      "iteration 6547 loss 2.4511265754699707, acc 26.5625\n",
      "iteration 6548 loss 2.8465073108673096, acc 20.3125\n",
      "iteration 6549 loss 2.888611316680908, acc 15.625\n",
      "iteration 6550 loss 2.79282808303833, acc 14.0625\n",
      "iteration 6551 loss 2.6585333347320557, acc 21.875\n",
      "iteration 6552 loss 2.8787360191345215, acc 21.875\n",
      "iteration 6553 loss 2.5496742725372314, acc 25.0\n",
      "iteration 6554 loss 2.731456995010376, acc 21.875\n",
      "iteration 6555 loss 2.7298197746276855, acc 20.3125\n",
      "iteration 6556 loss 2.645289897918701, acc 25.0\n",
      "iteration 6557 loss 2.6830945014953613, acc 20.3125\n",
      "iteration 6558 loss 2.7969489097595215, acc 23.4375\n",
      "iteration 6559 loss 2.608966827392578, acc 21.875\n",
      "iteration 6560 loss 2.5777816772460938, acc 20.3125\n",
      "iteration 6561 loss 2.6781692504882812, acc 25.0\n",
      "iteration 6562 loss 2.715211868286133, acc 23.4375\n",
      "iteration 6563 loss 2.723236083984375, acc 17.1875\n",
      "iteration 6564 loss 2.6239094734191895, acc 20.3125\n",
      "iteration 6565 loss 2.5090837478637695, acc 28.125\n",
      "iteration 6566 loss 2.585824489593506, acc 26.5625\n",
      "iteration 6567 loss 2.7263975143432617, acc 20.3125\n",
      "iteration 6568 loss 2.5272700786590576, acc 23.4375\n",
      "iteration 6569 loss 2.713773012161255, acc 20.3125\n",
      "iteration 6570 loss 2.526779890060425, acc 21.875\n",
      "iteration 6571 loss 2.682114839553833, acc 28.125\n",
      "iteration 6572 loss 2.5405383110046387, acc 26.5625\n",
      "iteration 6573 loss 2.4973526000976562, acc 25.0\n",
      "iteration 6574 loss 2.7929368019104004, acc 12.5\n",
      "iteration 6575 loss 2.68613862991333, acc 23.4375\n",
      "iteration 6576 loss 2.5660510063171387, acc 26.5625\n",
      "iteration 6577 loss 2.728034496307373, acc 26.5625\n",
      "iteration 6578 loss 2.731374740600586, acc 23.4375\n",
      "iteration 6579 loss 2.524984121322632, acc 39.0625\n",
      "iteration 6580 loss 2.8862040042877197, acc 17.1875\n",
      "iteration 6581 loss 2.574345827102661, acc 25.0\n",
      "iteration 6582 loss 2.6685311794281006, acc 18.75\n",
      "iteration 6583 loss 2.7254292964935303, acc 20.3125\n",
      "iteration 6584 loss 2.8016932010650635, acc 25.0\n",
      "iteration 6585 loss 2.9123387336730957, acc 21.875\n",
      "iteration 6586 loss 2.694476842880249, acc 25.0\n",
      "iteration 6587 loss 2.6570048332214355, acc 17.1875\n",
      "iteration 6588 loss 2.7919816970825195, acc 17.1875\n",
      "iteration 6589 loss 2.8051798343658447, acc 20.3125\n",
      "iteration 6590 loss 2.710036277770996, acc 17.1875\n",
      "iteration 6591 loss 2.864259958267212, acc 18.75\n",
      "iteration 6592 loss 2.6114025115966797, acc 20.3125\n",
      "iteration 6593 loss 2.6699419021606445, acc 28.125\n",
      "iteration 6594 loss 2.7622880935668945, acc 23.4375\n",
      "iteration 6595 loss 2.7847752571105957, acc 15.625\n",
      "iteration 6596 loss 2.6588168144226074, acc 28.125\n",
      "iteration 6597 loss 2.7014174461364746, acc 18.75\n",
      "iteration 6598 loss 2.61775279045105, acc 32.8125\n",
      "iteration 6599 loss 2.599945306777954, acc 23.4375\n",
      "iteration 6600 loss 2.7150847911834717, acc 18.75\n",
      "iteration 6601 loss 2.9420742988586426, acc 14.0625\n",
      "iteration 6602 loss 2.80265212059021, acc 21.875\n",
      "iteration 6603 loss 2.531135082244873, acc 37.5\n",
      "iteration 6604 loss 2.5553905963897705, acc 28.125\n",
      "iteration 6605 loss 2.656865358352661, acc 17.1875\n",
      "iteration 6606 loss 2.5790960788726807, acc 23.4375\n",
      "iteration 6607 loss 2.6031875610351562, acc 25.0\n",
      "iteration 6608 loss 2.7283308506011963, acc 23.4375\n",
      "iteration 6609 loss 2.689192056655884, acc 17.1875\n",
      "iteration 6610 loss 2.847999095916748, acc 15.625\n",
      "iteration 6611 loss 2.7569947242736816, acc 20.3125\n",
      "iteration 6612 loss 2.717919111251831, acc 20.3125\n",
      "iteration 6613 loss 2.83097243309021, acc 21.875\n",
      "iteration 6614 loss 2.576122522354126, acc 28.125\n",
      "iteration 6615 loss 2.7787857055664062, acc 18.75\n",
      "iteration 6616 loss 2.894132375717163, acc 15.625\n",
      "iteration 6617 loss 2.6450421810150146, acc 21.875\n",
      "iteration 6618 loss 2.641274929046631, acc 21.875\n",
      "iteration 6619 loss 2.7024712562561035, acc 18.75\n",
      "iteration 6620 loss 2.6419825553894043, acc 21.875\n",
      "iteration 6621 loss 2.4162960052490234, acc 34.375\n",
      "iteration 6622 loss 2.5048415660858154, acc 28.125\n",
      "iteration 6623 loss 2.787168025970459, acc 15.625\n",
      "iteration 6624 loss 2.622999668121338, acc 28.125\n",
      "iteration 6625 loss 2.482654094696045, acc 23.4375\n",
      "iteration 6626 loss 2.67797589302063, acc 21.875\n",
      "iteration 6627 loss 2.6689953804016113, acc 21.875\n",
      "iteration 6628 loss 2.760552406311035, acc 18.75\n",
      "iteration 6629 loss 2.765993356704712, acc 17.1875\n",
      "iteration 6630 loss 2.5578830242156982, acc 21.875\n",
      "iteration 6631 loss 2.574295997619629, acc 25.0\n",
      "iteration 6632 loss 2.6497445106506348, acc 28.125\n",
      "iteration 6633 loss 2.6947436332702637, acc 21.875\n",
      "iteration 6634 loss 2.610975503921509, acc 28.125\n",
      "iteration 6635 loss 2.4485387802124023, acc 25.0\n",
      "iteration 6636 loss 2.8092103004455566, acc 18.75\n",
      "iteration 6637 loss 2.815976858139038, acc 20.3125\n",
      "iteration 6638 loss 2.778374671936035, acc 15.625\n",
      "iteration 6639 loss 2.8026280403137207, acc 17.1875\n",
      "iteration 6640 loss 2.627272367477417, acc 28.125\n",
      "iteration 6641 loss 2.6538381576538086, acc 21.875\n",
      "iteration 6642 loss 2.56184458732605, acc 29.6875\n",
      "iteration 6643 loss 2.6523261070251465, acc 25.0\n",
      "iteration 6644 loss 2.6601173877716064, acc 20.3125\n",
      "iteration 6645 loss 2.7364821434020996, acc 17.1875\n",
      "iteration 6646 loss 2.6734986305236816, acc 23.4375\n",
      "iteration 6647 loss 2.417903184890747, acc 29.6875\n",
      "iteration 6648 loss 2.6621310710906982, acc 25.0\n",
      "iteration 6649 loss 2.760160207748413, acc 17.1875\n",
      "iteration 6650 loss 2.956782579421997, acc 12.5\n",
      "iteration 6651 loss 2.6646335124969482, acc 20.3125\n",
      "iteration 6652 loss 2.99151873588562, acc 12.5\n",
      "iteration 6653 loss 2.7455902099609375, acc 23.4375\n",
      "iteration 6654 loss 2.7366368770599365, acc 15.625\n",
      "iteration 6655 loss 2.775874137878418, acc 25.0\n",
      "iteration 6656 loss 2.618105173110962, acc 23.4375\n",
      "iteration 6657 loss 2.7085988521575928, acc 29.6875\n",
      "iteration 6658 loss 2.7644717693328857, acc 26.5625\n",
      "iteration 6659 loss 2.941749095916748, acc 17.1875\n",
      "iteration 6660 loss 2.739065170288086, acc 21.875\n",
      "iteration 6661 loss 2.731910228729248, acc 23.4375\n",
      "iteration 6662 loss 2.5829057693481445, acc 26.5625\n",
      "iteration 6663 loss 2.77626371383667, acc 20.3125\n",
      "iteration 6664 loss 2.674818754196167, acc 25.0\n",
      "iteration 6665 loss 2.861889362335205, acc 17.1875\n",
      "iteration 6666 loss 2.585540294647217, acc 25.0\n",
      "iteration 6667 loss 2.6333274841308594, acc 23.4375\n",
      "iteration 6668 loss 2.6170759201049805, acc 29.6875\n",
      "iteration 6669 loss 2.7999255657196045, acc 17.1875\n",
      "iteration 6670 loss 2.674071788787842, acc 28.125\n",
      "iteration 6671 loss 2.8098092079162598, acc 18.75\n",
      "iteration 6672 loss 2.5800039768218994, acc 25.0\n",
      "iteration 6673 loss 2.7828056812286377, acc 15.625\n",
      "iteration 6674 loss 2.8580210208892822, acc 17.1875\n",
      "iteration 6675 loss 2.91817307472229, acc 18.75\n",
      "iteration 6676 loss 2.653373956680298, acc 20.3125\n",
      "iteration 6677 loss 2.632016658782959, acc 21.875\n",
      "iteration 6678 loss 2.692476749420166, acc 25.0\n",
      "iteration 6679 loss 2.713184356689453, acc 18.75\n",
      "iteration 6680 loss 2.7451565265655518, acc 12.5\n",
      "iteration 6681 loss 2.688512086868286, acc 26.5625\n",
      "iteration 6682 loss 2.850654363632202, acc 14.0625\n",
      "iteration 6683 loss 2.853968620300293, acc 18.75\n",
      "iteration 6684 loss 2.644472122192383, acc 20.3125\n",
      "iteration 6685 loss 2.631969690322876, acc 28.125\n",
      "iteration 6686 loss 2.83866024017334, acc 7.8125\n",
      "iteration 6687 loss 2.726121664047241, acc 23.4375\n",
      "iteration 6688 loss 2.8676578998565674, acc 23.4375\n",
      "iteration 6689 loss 2.8351447582244873, acc 20.3125\n",
      "iteration 6690 loss 2.6156952381134033, acc 26.5625\n",
      "iteration 6691 loss 2.5290441513061523, acc 20.3125\n",
      "iteration 6692 loss 2.8159682750701904, acc 17.1875\n",
      "iteration 6693 loss 2.6972897052764893, acc 15.625\n",
      "iteration 6694 loss 2.65374755859375, acc 20.3125\n",
      "iteration 6695 loss 2.6350350379943848, acc 23.4375\n",
      "iteration 6696 loss 2.720884323120117, acc 21.875\n",
      "iteration 6697 loss 2.5695109367370605, acc 26.5625\n",
      "iteration 6698 loss 2.7525575160980225, acc 15.625\n",
      "iteration 6699 loss 2.665860176086426, acc 23.4375\n",
      "iteration 6700 loss 2.854327917098999, acc 18.75\n",
      "iteration 6701 loss 2.5382611751556396, acc 29.6875\n",
      "iteration 6702 loss 2.567399740219116, acc 23.4375\n",
      "iteration 6703 loss 2.32663893699646, acc 32.8125\n",
      "iteration 6704 loss 2.4570980072021484, acc 29.6875\n",
      "iteration 6705 loss 2.591277837753296, acc 29.6875\n",
      "iteration 6706 loss 2.7621607780456543, acc 20.3125\n",
      "iteration 6707 loss 2.511704921722412, acc 28.125\n",
      "iteration 6708 loss 2.7550125122070312, acc 20.3125\n",
      "iteration 6709 loss 2.5717053413391113, acc 21.875\n",
      "iteration 6710 loss 2.87074613571167, acc 14.0625\n",
      "iteration 6711 loss 2.7678985595703125, acc 18.75\n",
      "iteration 6712 loss 2.767559289932251, acc 15.625\n",
      "iteration 6713 loss 2.8288862705230713, acc 23.4375\n",
      "iteration 6714 loss 2.6726455688476562, acc 17.1875\n",
      "iteration 6715 loss 2.803884506225586, acc 20.3125\n",
      "iteration 6716 loss 2.896873712539673, acc 17.1875\n",
      "iteration 6717 loss 2.54885196685791, acc 28.125\n",
      "iteration 6718 loss 2.490069627761841, acc 29.6875\n",
      "iteration 6719 loss 2.8068885803222656, acc 17.1875\n",
      "iteration 6720 loss 2.8704864978790283, acc 31.25\n",
      "iteration 6721 loss 2.664210319519043, acc 14.0625\n",
      "iteration 6722 loss 2.7035655975341797, acc 21.875\n",
      "iteration 6723 loss 2.7395167350769043, acc 21.875\n",
      "iteration 6724 loss 2.482809066772461, acc 31.25\n",
      "iteration 6725 loss 2.7308740615844727, acc 21.875\n",
      "iteration 6726 loss 2.7959561347961426, acc 18.75\n",
      "iteration 6727 loss 2.8030896186828613, acc 21.875\n",
      "iteration 6728 loss 2.848747730255127, acc 15.625\n",
      "iteration 6729 loss 2.7438249588012695, acc 14.0625\n",
      "iteration 6730 loss 2.722480297088623, acc 14.0625\n",
      "iteration 6731 loss 2.834325075149536, acc 20.3125\n",
      "iteration 6732 loss 2.758889675140381, acc 18.75\n",
      "iteration 6733 loss 2.526170492172241, acc 29.6875\n",
      "iteration 6734 loss 2.7972095012664795, acc 23.4375\n",
      "iteration 6735 loss 2.620420217514038, acc 25.0\n",
      "iteration 6736 loss 2.730257987976074, acc 25.0\n",
      "iteration 6737 loss 2.734386920928955, acc 18.75\n",
      "iteration 6738 loss 2.5827789306640625, acc 15.625\n",
      "iteration 6739 loss 2.6274492740631104, acc 25.0\n",
      "iteration 6740 loss 2.543982744216919, acc 28.125\n",
      "iteration 6741 loss 2.452507495880127, acc 29.6875\n",
      "iteration 6742 loss 2.8784327507019043, acc 17.1875\n",
      "iteration 6743 loss 2.733886480331421, acc 21.875\n",
      "iteration 6744 loss 2.8360750675201416, acc 21.875\n",
      "iteration 6745 loss 2.529792547225952, acc 21.875\n",
      "iteration 6746 loss 2.7594871520996094, acc 9.375\n",
      "iteration 6747 loss 2.552185535430908, acc 25.0\n",
      "iteration 6748 loss 2.7268621921539307, acc 18.75\n",
      "iteration 6749 loss 2.7455015182495117, acc 20.3125\n",
      "iteration 6750 loss 2.538137197494507, acc 26.5625\n",
      "iteration 6751 loss 2.7440192699432373, acc 18.75\n",
      "iteration 6752 loss 2.8033831119537354, acc 12.5\n",
      "iteration 6753 loss 2.7742745876312256, acc 25.0\n",
      "iteration 6754 loss 2.711341381072998, acc 25.0\n",
      "iteration 6755 loss 2.5270135402679443, acc 26.5625\n",
      "iteration 6756 loss 2.578230142593384, acc 21.875\n",
      "iteration 6757 loss 2.6175239086151123, acc 17.1875\n",
      "iteration 6758 loss 2.694237232208252, acc 23.4375\n",
      "iteration 6759 loss 2.8283536434173584, acc 21.875\n",
      "iteration 6760 loss 2.6789956092834473, acc 20.3125\n",
      "iteration 6761 loss 2.6809210777282715, acc 21.875\n",
      "iteration 6762 loss 2.6827945709228516, acc 18.75\n",
      "iteration 6763 loss 2.820319175720215, acc 18.75\n",
      "iteration 6764 loss 2.6223299503326416, acc 23.4375\n",
      "iteration 6765 loss 2.6175405979156494, acc 23.4375\n",
      "iteration 6766 loss 2.7225334644317627, acc 17.1875\n",
      "iteration 6767 loss 2.639698028564453, acc 23.4375\n",
      "iteration 6768 loss 2.9166042804718018, acc 17.1875\n",
      "iteration 6769 loss 2.678502082824707, acc 15.625\n",
      "iteration 6770 loss 2.5948967933654785, acc 25.0\n",
      "iteration 6771 loss 2.748450517654419, acc 20.3125\n",
      "iteration 6772 loss 2.6097021102905273, acc 26.5625\n",
      "iteration 6773 loss 2.527576208114624, acc 29.6875\n",
      "iteration 6774 loss 2.7285029888153076, acc 20.3125\n",
      "iteration 6775 loss 2.6888082027435303, acc 18.75\n",
      "iteration 6776 loss 2.6176939010620117, acc 21.875\n",
      "iteration 6777 loss 2.6662468910217285, acc 25.0\n",
      "iteration 6778 loss 2.6284844875335693, acc 25.0\n",
      "iteration 6779 loss 2.580538272857666, acc 26.5625\n",
      "iteration 6780 loss 2.823133707046509, acc 18.75\n",
      "iteration 6781 loss 2.7751145362854004, acc 21.875\n",
      "iteration 6782 loss 2.5849382877349854, acc 29.6875\n",
      "iteration 6783 loss 2.6428372859954834, acc 18.75\n",
      "iteration 6784 loss 2.8438405990600586, acc 15.625\n",
      "iteration 6785 loss 2.5847585201263428, acc 23.4375\n",
      "iteration 6786 loss 2.706589460372925, acc 15.625\n",
      "iteration 6787 loss 2.6376452445983887, acc 23.4375\n",
      "iteration 6788 loss 2.5705807209014893, acc 23.4375\n",
      "iteration 6789 loss 2.73366117477417, acc 15.625\n",
      "iteration 6790 loss 2.7681407928466797, acc 17.1875\n",
      "iteration 6791 loss 2.7131459712982178, acc 18.75\n",
      "iteration 6792 loss 2.6079540252685547, acc 21.875\n",
      "iteration 6793 loss 2.7327566146850586, acc 21.875\n",
      "iteration 6794 loss 2.6089067459106445, acc 20.3125\n",
      "iteration 6795 loss 2.8018603324890137, acc 21.875\n",
      "iteration 6796 loss 2.809175491333008, acc 18.75\n",
      "iteration 6797 loss 2.7994728088378906, acc 21.875\n",
      "iteration 6798 loss 2.780626058578491, acc 25.0\n",
      "iteration 6799 loss 2.9492716789245605, acc 17.1875\n",
      "iteration 6800 loss 2.710693359375, acc 21.875\n",
      "iteration 6801 loss 2.7778537273406982, acc 20.3125\n",
      "iteration 6802 loss 2.9094882011413574, acc 15.625\n",
      "iteration 6803 loss 2.813832998275757, acc 23.4375\n",
      "iteration 6804 loss 2.6408936977386475, acc 21.875\n",
      "iteration 6805 loss 2.901174545288086, acc 17.1875\n",
      "iteration 6806 loss 2.675828695297241, acc 23.4375\n",
      "iteration 6807 loss 2.7245280742645264, acc 14.0625\n",
      "iteration 6808 loss 2.7665345668792725, acc 14.0625\n",
      "iteration 6809 loss 2.622201442718506, acc 14.0625\n",
      "iteration 6810 loss 2.7145376205444336, acc 17.1875\n",
      "iteration 6811 loss 2.586064100265503, acc 21.875\n",
      "iteration 6812 loss 2.591750383377075, acc 32.8125\n",
      "iteration 6813 loss 2.753830909729004, acc 17.1875\n",
      "iteration 6814 loss 2.8461921215057373, acc 14.0625\n",
      "iteration 6815 loss 2.74169659614563, acc 14.0625\n",
      "iteration 6816 loss 2.523730516433716, acc 20.3125\n",
      "iteration 6817 loss 2.8492181301116943, acc 18.75\n",
      "iteration 6818 loss 2.579996109008789, acc 23.4375\n",
      "iteration 6819 loss 2.732900381088257, acc 20.3125\n",
      "iteration 6820 loss 2.631206512451172, acc 20.3125\n",
      "iteration 6821 loss 2.780834197998047, acc 21.875\n",
      "iteration 6822 loss 2.5580320358276367, acc 17.1875\n",
      "iteration 6823 loss 2.6877965927124023, acc 21.875\n",
      "iteration 6824 loss 2.9299979209899902, acc 18.75\n",
      "iteration 6825 loss 2.620150327682495, acc 25.0\n",
      "iteration 6826 loss 2.691762924194336, acc 26.5625\n",
      "iteration 6827 loss 2.7338669300079346, acc 14.0625\n",
      "iteration 6828 loss 2.5176804065704346, acc 21.875\n",
      "iteration 6829 loss 2.7370874881744385, acc 18.75\n",
      "iteration 6830 loss 2.657658576965332, acc 20.3125\n",
      "iteration 6831 loss 2.6409685611724854, acc 18.75\n",
      "iteration 6832 loss 2.7627058029174805, acc 29.6875\n",
      "iteration 6833 loss 2.6320974826812744, acc 28.125\n",
      "iteration 6834 loss 2.8110511302948, acc 10.9375\n",
      "iteration 6835 loss 2.891521453857422, acc 12.5\n",
      "iteration 6836 loss 2.8028934001922607, acc 17.1875\n",
      "iteration 6837 loss 2.7100138664245605, acc 18.75\n",
      "iteration 6838 loss 2.9101037979125977, acc 15.625\n",
      "iteration 6839 loss 2.5541162490844727, acc 26.5625\n",
      "iteration 6840 loss 2.6905014514923096, acc 17.1875\n",
      "iteration 6841 loss 2.4925034046173096, acc 35.9375\n",
      "iteration 6842 loss 2.656589984893799, acc 18.75\n",
      "iteration 6843 loss 2.733909845352173, acc 15.625\n",
      "iteration 6844 loss 2.895873785018921, acc 14.0625\n",
      "iteration 6845 loss 2.8036231994628906, acc 10.9375\n",
      "iteration 6846 loss 2.7738053798675537, acc 14.0625\n",
      "iteration 6847 loss 2.672374725341797, acc 25.0\n",
      "iteration 6848 loss 2.778245210647583, acc 20.3125\n",
      "iteration 6849 loss 2.7688071727752686, acc 18.75\n",
      "iteration 6850 loss 2.7781457901000977, acc 14.0625\n",
      "iteration 6851 loss 2.565828323364258, acc 25.0\n",
      "iteration 6852 loss 2.6426854133605957, acc 17.1875\n",
      "iteration 6853 loss 2.6069960594177246, acc 25.0\n",
      "iteration 6854 loss 2.7037134170532227, acc 26.5625\n",
      "iteration 6855 loss 2.5742576122283936, acc 26.5625\n",
      "iteration 6856 loss 2.69897723197937, acc 25.0\n",
      "iteration 6857 loss 2.702498435974121, acc 26.5625\n",
      "iteration 6858 loss 2.8266003131866455, acc 18.75\n",
      "iteration 6859 loss 2.853492021560669, acc 15.625\n",
      "iteration 6860 loss 2.6345298290252686, acc 26.5625\n",
      "iteration 6861 loss 2.5442278385162354, acc 17.1875\n",
      "iteration 6862 loss 2.651876211166382, acc 17.1875\n",
      "iteration 6863 loss 2.643524646759033, acc 20.3125\n",
      "iteration 6864 loss 2.7900142669677734, acc 17.1875\n",
      "iteration 6865 loss 2.649263381958008, acc 31.25\n",
      "iteration 6866 loss 2.9633729457855225, acc 7.8125\n",
      "iteration 6867 loss 2.6542608737945557, acc 21.875\n",
      "iteration 6868 loss 2.6342310905456543, acc 28.125\n",
      "iteration 6869 loss 2.6150124073028564, acc 21.875\n",
      "iteration 6870 loss 2.5967397689819336, acc 23.4375\n",
      "iteration 6871 loss 2.7163889408111572, acc 25.0\n",
      "iteration 6872 loss 2.758629322052002, acc 18.75\n",
      "iteration 6873 loss 2.5813093185424805, acc 25.0\n",
      "iteration 6874 loss 2.6857430934906006, acc 18.75\n",
      "iteration 6875 loss 2.881944417953491, acc 14.0625\n",
      "iteration 6876 loss 2.5461957454681396, acc 28.125\n",
      "iteration 6877 loss 2.7783586978912354, acc 15.625\n",
      "iteration 6878 loss 2.5482122898101807, acc 25.0\n",
      "iteration 6879 loss 2.6682119369506836, acc 18.75\n",
      "iteration 6880 loss 2.765244960784912, acc 25.0\n",
      "iteration 6881 loss 2.849024772644043, acc 15.625\n",
      "iteration 6882 loss 2.92454195022583, acc 10.9375\n",
      "iteration 6883 loss 2.721801519393921, acc 21.875\n",
      "iteration 6884 loss 2.895728588104248, acc 15.625\n",
      "iteration 6885 loss 2.8236846923828125, acc 10.9375\n",
      "iteration 6886 loss 2.7698049545288086, acc 17.1875\n",
      "iteration 6887 loss 2.668071746826172, acc 20.3125\n",
      "iteration 6888 loss 2.7032580375671387, acc 15.625\n",
      "iteration 6889 loss 2.6315927505493164, acc 26.5625\n",
      "iteration 6890 loss 2.7449285984039307, acc 20.3125\n",
      "iteration 6891 loss 2.710186243057251, acc 18.75\n",
      "iteration 6892 loss 2.5242295265197754, acc 21.875\n",
      "iteration 6893 loss 2.7260923385620117, acc 20.3125\n",
      "iteration 6894 loss 2.6516475677490234, acc 21.875\n",
      "iteration 6895 loss 2.663670778274536, acc 31.25\n",
      "iteration 6896 loss 2.405311107635498, acc 29.6875\n",
      "iteration 6897 loss 2.5801820755004883, acc 23.4375\n",
      "iteration 6898 loss 2.550194025039673, acc 25.0\n",
      "iteration 6899 loss 2.51448130607605, acc 23.4375\n",
      "iteration 6900 loss 2.613696336746216, acc 25.0\n",
      "iteration 6901 loss 2.674328327178955, acc 23.4375\n",
      "iteration 6902 loss 2.7691781520843506, acc 21.875\n",
      "iteration 6903 loss 2.59283709526062, acc 23.4375\n",
      "iteration 6904 loss 2.733750581741333, acc 17.1875\n",
      "iteration 6905 loss 2.6770591735839844, acc 26.5625\n",
      "iteration 6906 loss 2.5940582752227783, acc 25.0\n",
      "iteration 6907 loss 2.6984949111938477, acc 25.0\n",
      "iteration 6908 loss 2.7141406536102295, acc 25.0\n",
      "iteration 6909 loss 2.958855628967285, acc 15.625\n",
      "iteration 6910 loss 2.636073589324951, acc 21.875\n",
      "iteration 6911 loss 2.51413893699646, acc 25.0\n",
      "iteration 6912 loss 2.6397182941436768, acc 26.5625\n",
      "iteration 6913 loss 2.638942003250122, acc 25.0\n",
      "iteration 6914 loss 2.744422197341919, acc 20.3125\n",
      "iteration 6915 loss 2.815286159515381, acc 15.625\n",
      "iteration 6916 loss 2.6553215980529785, acc 25.0\n",
      "iteration 6917 loss 2.7900593280792236, acc 17.1875\n",
      "iteration 6918 loss 2.911027193069458, acc 14.0625\n",
      "iteration 6919 loss 2.828127861022949, acc 17.1875\n",
      "iteration 6920 loss 2.6988821029663086, acc 28.125\n",
      "iteration 6921 loss 2.8937199115753174, acc 17.1875\n",
      "iteration 6922 loss 2.5020508766174316, acc 25.0\n",
      "iteration 6923 loss 2.6577460765838623, acc 20.3125\n",
      "iteration 6924 loss 2.611370325088501, acc 20.3125\n",
      "iteration 6925 loss 2.786101818084717, acc 21.875\n",
      "iteration 6926 loss 2.637979507446289, acc 23.4375\n",
      "iteration 6927 loss 2.593595504760742, acc 25.0\n",
      "iteration 6928 loss 2.8534722328186035, acc 20.3125\n",
      "iteration 6929 loss 2.7598483562469482, acc 15.625\n",
      "iteration 6930 loss 2.7357287406921387, acc 31.25\n",
      "iteration 6931 loss 2.5941290855407715, acc 21.875\n",
      "iteration 6932 loss 2.699761390686035, acc 26.5625\n",
      "iteration 6933 loss 2.690894365310669, acc 21.875\n",
      "iteration 6934 loss 2.8089089393615723, acc 15.625\n",
      "iteration 6935 loss 2.530613899230957, acc 32.8125\n",
      "iteration 6936 loss 2.8092470169067383, acc 10.9375\n",
      "iteration 6937 loss 2.6633763313293457, acc 25.0\n",
      "iteration 6938 loss 2.7894535064697266, acc 20.3125\n",
      "iteration 6939 loss 2.6614136695861816, acc 21.875\n",
      "iteration 6940 loss 2.613713026046753, acc 25.0\n",
      "iteration 6941 loss 2.8288073539733887, acc 14.0625\n",
      "iteration 6942 loss 2.6334619522094727, acc 25.0\n",
      "iteration 6943 loss 2.767899751663208, acc 15.625\n",
      "iteration 6944 loss 2.8160719871520996, acc 20.3125\n",
      "iteration 6945 loss 2.5551443099975586, acc 23.4375\n",
      "iteration 6946 loss 2.6889829635620117, acc 28.125\n",
      "iteration 6947 loss 2.583265781402588, acc 20.3125\n",
      "iteration 6948 loss 2.704774856567383, acc 28.125\n",
      "iteration 6949 loss 2.733124017715454, acc 26.5625\n",
      "iteration 6950 loss 2.5814263820648193, acc 28.125\n",
      "iteration 6951 loss 2.7139229774475098, acc 18.75\n",
      "iteration 6952 loss 2.702792167663574, acc 20.3125\n",
      "iteration 6953 loss 2.610826015472412, acc 20.3125\n",
      "iteration 6954 loss 2.77640700340271, acc 15.625\n",
      "iteration 6955 loss 2.662071466445923, acc 17.1875\n",
      "iteration 6956 loss 2.6969544887542725, acc 20.3125\n",
      "iteration 6957 loss 2.564244031906128, acc 21.875\n",
      "iteration 6958 loss 2.648491144180298, acc 21.875\n",
      "iteration 6959 loss 2.6691389083862305, acc 26.5625\n",
      "iteration 6960 loss 2.879348039627075, acc 12.5\n",
      "iteration 6961 loss 2.781121015548706, acc 21.875\n",
      "iteration 6962 loss 2.6341564655303955, acc 21.875\n",
      "iteration 6963 loss 2.439239025115967, acc 31.25\n",
      "iteration 6964 loss 2.6484766006469727, acc 21.875\n",
      "iteration 6965 loss 2.516744375228882, acc 26.5625\n",
      "iteration 6966 loss 2.595501661300659, acc 25.0\n",
      "iteration 6967 loss 2.810495138168335, acc 17.1875\n",
      "iteration 6968 loss 2.610121011734009, acc 29.6875\n",
      "iteration 6969 loss 2.895857810974121, acc 20.3125\n",
      "iteration 6970 loss 2.742405414581299, acc 17.1875\n",
      "iteration 6971 loss 2.7769687175750732, acc 21.875\n",
      "iteration 6972 loss 2.7329180240631104, acc 23.4375\n",
      "iteration 6973 loss 2.859571933746338, acc 15.625\n",
      "iteration 6974 loss 2.7613439559936523, acc 26.5625\n",
      "iteration 6975 loss 2.673816680908203, acc 28.125\n",
      "iteration 6976 loss 2.584425210952759, acc 29.6875\n",
      "iteration 6977 loss 2.898982524871826, acc 10.9375\n",
      "iteration 6978 loss 2.615314483642578, acc 25.0\n",
      "iteration 6979 loss 2.70564603805542, acc 17.1875\n",
      "iteration 6980 loss 2.653262138366699, acc 21.875\n",
      "iteration 6981 loss 2.5188286304473877, acc 28.125\n",
      "iteration 6982 loss 2.756333112716675, acc 25.0\n",
      "iteration 6983 loss 2.559027910232544, acc 26.5625\n",
      "iteration 6984 loss 2.6685240268707275, acc 23.4375\n",
      "iteration 6985 loss 2.731264352798462, acc 28.125\n",
      "iteration 6986 loss 2.640594959259033, acc 21.875\n",
      "iteration 6987 loss 2.8104255199432373, acc 20.3125\n",
      "iteration 6988 loss 2.519662618637085, acc 18.75\n",
      "iteration 6989 loss 2.4954795837402344, acc 25.0\n",
      "iteration 6990 loss 2.704594850540161, acc 23.4375\n",
      "iteration 6991 loss 2.6893420219421387, acc 18.75\n",
      "iteration 6992 loss 2.9056472778320312, acc 20.3125\n",
      "iteration 6993 loss 2.6187639236450195, acc 21.875\n",
      "iteration 6994 loss 2.499892234802246, acc 31.25\n",
      "iteration 6995 loss 2.797976016998291, acc 20.3125\n",
      "iteration 6996 loss 2.6186838150024414, acc 17.1875\n",
      "iteration 6997 loss 2.6845664978027344, acc 28.125\n",
      "iteration 6998 loss 2.7326645851135254, acc 17.1875\n",
      "iteration 6999 loss 2.612335443496704, acc 29.6875\n",
      "iteration 7000 loss 2.5287201404571533, acc 25.0\n",
      "iteration 7001 loss 2.8040099143981934, acc 14.0625\n",
      "iteration 7002 loss 2.743633985519409, acc 23.4375\n",
      "iteration 7003 loss 2.7326292991638184, acc 25.0\n",
      "iteration 7004 loss 2.7602834701538086, acc 12.5\n",
      "iteration 7005 loss 2.756664752960205, acc 23.4375\n",
      "iteration 7006 loss 2.883258104324341, acc 20.3125\n",
      "iteration 7007 loss 2.688283681869507, acc 20.3125\n",
      "iteration 7008 loss 2.880845069885254, acc 14.0625\n",
      "iteration 7009 loss 2.549259662628174, acc 23.4375\n",
      "iteration 7010 loss 2.4373855590820312, acc 31.25\n",
      "iteration 7011 loss 2.6942226886749268, acc 23.4375\n",
      "iteration 7012 loss 2.721501350402832, acc 29.6875\n",
      "iteration 7013 loss 2.575122594833374, acc 28.125\n",
      "iteration 7014 loss 2.785822629928589, acc 20.3125\n",
      "iteration 7015 loss 2.571505069732666, acc 32.8125\n",
      "iteration 7016 loss 2.7186920642852783, acc 18.75\n",
      "iteration 7017 loss 2.7983808517456055, acc 12.5\n",
      "iteration 7018 loss 2.700392484664917, acc 21.875\n",
      "iteration 7019 loss 2.678873062133789, acc 31.25\n",
      "iteration 7020 loss 2.8463618755340576, acc 14.0625\n",
      "iteration 7021 loss 2.7280476093292236, acc 21.875\n",
      "iteration 7022 loss 2.7947564125061035, acc 21.875\n",
      "iteration 7023 loss 2.5193779468536377, acc 25.0\n",
      "iteration 7024 loss 3.0400919914245605, acc 12.5\n",
      "iteration 7025 loss 2.7016897201538086, acc 17.1875\n",
      "iteration 7026 loss 2.7384116649627686, acc 25.0\n",
      "iteration 7027 loss 2.849525213241577, acc 12.5\n",
      "iteration 7028 loss 2.676682710647583, acc 26.5625\n",
      "iteration 7029 loss 2.599067211151123, acc 26.5625\n",
      "iteration 7030 loss 2.5348286628723145, acc 25.0\n",
      "iteration 7031 loss 2.871258020401001, acc 17.1875\n",
      "iteration 7032 loss 2.587322950363159, acc 25.0\n",
      "iteration 7033 loss 2.595160484313965, acc 26.5625\n",
      "iteration 7034 loss 2.7113442420959473, acc 18.75\n",
      "iteration 7035 loss 2.6008496284484863, acc 26.5625\n",
      "iteration 7036 loss 2.656433582305908, acc 25.0\n",
      "iteration 7037 loss 2.62811279296875, acc 26.5625\n",
      "iteration 7038 loss 2.704406499862671, acc 21.875\n",
      "iteration 7039 loss 2.8100240230560303, acc 18.75\n",
      "iteration 7040 loss 2.6000354290008545, acc 18.75\n",
      "iteration 7041 loss 2.675313949584961, acc 23.4375\n",
      "iteration 7042 loss 2.803165912628174, acc 18.75\n",
      "iteration 7043 loss 2.679863929748535, acc 23.4375\n",
      "iteration 7044 loss 2.7085185050964355, acc 18.75\n",
      "iteration 7045 loss 2.712367296218872, acc 21.875\n",
      "iteration 7046 loss 2.5682945251464844, acc 28.125\n",
      "iteration 7047 loss 3.0139217376708984, acc 7.8125\n",
      "iteration 7048 loss 2.740845203399658, acc 23.4375\n",
      "iteration 7049 loss 2.6946966648101807, acc 18.75\n",
      "iteration 7050 loss 2.6300368309020996, acc 20.3125\n",
      "iteration 7051 loss 2.6115362644195557, acc 34.375\n",
      "iteration 7052 loss 2.6876611709594727, acc 20.3125\n",
      "iteration 7053 loss 2.7159197330474854, acc 18.75\n",
      "iteration 7054 loss 2.552906036376953, acc 28.125\n",
      "iteration 7055 loss 2.701788902282715, acc 20.3125\n",
      "iteration 7056 loss 2.750941038131714, acc 20.3125\n",
      "iteration 7057 loss 2.7263636589050293, acc 14.0625\n",
      "iteration 7058 loss 2.550239324569702, acc 25.0\n",
      "iteration 7059 loss 2.8407886028289795, acc 18.75\n",
      "iteration 7060 loss 2.60481858253479, acc 25.0\n",
      "iteration 7061 loss 2.7884769439697266, acc 14.0625\n",
      "iteration 7062 loss 2.792304277420044, acc 21.875\n",
      "iteration 7063 loss 2.787121534347534, acc 23.4375\n",
      "iteration 7064 loss 2.7481679916381836, acc 15.625\n",
      "iteration 7065 loss 2.919003963470459, acc 14.0625\n",
      "iteration 7066 loss 2.8371119499206543, acc 15.625\n",
      "iteration 7067 loss 2.693641424179077, acc 14.0625\n",
      "iteration 7068 loss 2.766068458557129, acc 10.9375\n",
      "iteration 7069 loss 2.784648895263672, acc 18.75\n",
      "iteration 7070 loss 2.785890817642212, acc 17.1875\n",
      "iteration 7071 loss 2.7130825519561768, acc 25.0\n",
      "iteration 7072 loss 2.6813812255859375, acc 21.875\n",
      "iteration 7073 loss 2.77553653717041, acc 28.125\n",
      "iteration 7074 loss 2.484537124633789, acc 20.3125\n",
      "iteration 7075 loss 2.482067346572876, acc 28.125\n",
      "iteration 7076 loss 2.6304771900177, acc 26.5625\n",
      "iteration 7077 loss 2.7634294033050537, acc 14.0625\n",
      "iteration 7078 loss 2.89493465423584, acc 23.4375\n",
      "iteration 7079 loss 2.7299273014068604, acc 21.875\n",
      "iteration 7080 loss 2.7067782878875732, acc 20.3125\n",
      "iteration 7081 loss 2.6543736457824707, acc 20.3125\n",
      "iteration 7082 loss 2.547006607055664, acc 23.4375\n",
      "iteration 7083 loss 2.6357994079589844, acc 29.6875\n",
      "iteration 7084 loss 2.8116655349731445, acc 20.3125\n",
      "iteration 7085 loss 2.5795838832855225, acc 25.0\n",
      "iteration 7086 loss 2.50962495803833, acc 21.875\n",
      "iteration 7087 loss 2.8988635540008545, acc 17.1875\n",
      "iteration 7088 loss 2.8377132415771484, acc 14.0625\n",
      "iteration 7089 loss 2.6890835762023926, acc 17.1875\n",
      "iteration 7090 loss 2.8045713901519775, acc 12.5\n",
      "iteration 7091 loss 2.600147008895874, acc 17.1875\n",
      "iteration 7092 loss 2.4741508960723877, acc 29.6875\n",
      "iteration 7093 loss 2.789576768875122, acc 26.5625\n",
      "iteration 7094 loss 2.6030237674713135, acc 21.875\n",
      "iteration 7095 loss 2.7288007736206055, acc 18.75\n",
      "iteration 7096 loss 2.5071468353271484, acc 34.375\n",
      "iteration 7097 loss 2.7102773189544678, acc 20.3125\n",
      "iteration 7098 loss 2.7028908729553223, acc 26.5625\n",
      "iteration 7099 loss 2.675894021987915, acc 23.4375\n",
      "iteration 7100 loss 2.649184226989746, acc 23.4375\n",
      "iteration 7101 loss 2.7525928020477295, acc 23.4375\n",
      "iteration 7102 loss 2.584163188934326, acc 21.875\n",
      "iteration 7103 loss 2.7071123123168945, acc 28.125\n",
      "iteration 7104 loss 2.5198214054107666, acc 25.0\n",
      "iteration 7105 loss 2.9143624305725098, acc 15.625\n",
      "iteration 7106 loss 2.765915632247925, acc 23.4375\n",
      "iteration 7107 loss 2.61189866065979, acc 23.4375\n",
      "iteration 7108 loss 2.4432947635650635, acc 29.6875\n",
      "iteration 7109 loss 2.5937390327453613, acc 21.875\n",
      "iteration 7110 loss 2.876333475112915, acc 15.625\n",
      "iteration 7111 loss 2.7626805305480957, acc 10.9375\n",
      "iteration 7112 loss 2.5821261405944824, acc 17.1875\n",
      "iteration 7113 loss 2.649779796600342, acc 23.4375\n",
      "iteration 7114 loss 2.4085211753845215, acc 26.5625\n",
      "iteration 7115 loss 2.458329439163208, acc 26.5625\n",
      "iteration 7116 loss 2.707078456878662, acc 17.1875\n",
      "iteration 7117 loss 2.7366816997528076, acc 18.75\n",
      "iteration 7118 loss 2.702388286590576, acc 20.3125\n",
      "iteration 7119 loss 2.6461429595947266, acc 20.3125\n",
      "iteration 7120 loss 2.6266584396362305, acc 26.5625\n",
      "iteration 7121 loss 2.655811309814453, acc 14.0625\n",
      "iteration 7122 loss 2.547511339187622, acc 23.4375\n",
      "iteration 7123 loss 2.4003307819366455, acc 26.5625\n",
      "iteration 7124 loss 2.866499900817871, acc 15.625\n",
      "iteration 7125 loss 2.613846778869629, acc 31.25\n",
      "iteration 7126 loss 2.764331102371216, acc 25.0\n",
      "iteration 7127 loss 2.8263535499572754, acc 23.4375\n",
      "iteration 7128 loss 2.601193428039551, acc 14.0625\n",
      "iteration 7129 loss 2.6056935787200928, acc 23.4375\n",
      "iteration 7130 loss 2.6839540004730225, acc 25.0\n",
      "iteration 7131 loss 2.6907074451446533, acc 21.875\n",
      "iteration 7132 loss 2.616316318511963, acc 25.0\n",
      "iteration 7133 loss 2.5079760551452637, acc 29.6875\n",
      "iteration 7134 loss 2.4678478240966797, acc 31.25\n",
      "iteration 7135 loss 2.8338558673858643, acc 20.3125\n",
      "iteration 7136 loss 2.7049479484558105, acc 21.875\n",
      "iteration 7137 loss 2.6878974437713623, acc 28.125\n",
      "iteration 7138 loss 2.5660829544067383, acc 18.75\n",
      "iteration 7139 loss 2.886263132095337, acc 17.1875\n",
      "iteration 7140 loss 2.618746280670166, acc 25.0\n",
      "iteration 7141 loss 2.760885000228882, acc 17.1875\n",
      "iteration 7142 loss 2.753070116043091, acc 17.1875\n",
      "iteration 7143 loss 2.671579122543335, acc 17.1875\n",
      "iteration 7144 loss 2.857548236846924, acc 14.0625\n",
      "iteration 7145 loss 2.6649293899536133, acc 12.5\n",
      "iteration 7146 loss 2.6534061431884766, acc 20.3125\n",
      "iteration 7147 loss 2.7755446434020996, acc 15.625\n",
      "iteration 7148 loss 2.630863666534424, acc 23.4375\n",
      "iteration 7149 loss 2.751042366027832, acc 23.4375\n",
      "iteration 7150 loss 2.7210803031921387, acc 23.4375\n",
      "iteration 7151 loss 2.6817517280578613, acc 18.75\n",
      "iteration 7152 loss 2.8175172805786133, acc 14.0625\n",
      "iteration 7153 loss 2.803405523300171, acc 14.0625\n",
      "iteration 7154 loss 2.7480528354644775, acc 18.75\n",
      "iteration 7155 loss 2.682978630065918, acc 29.6875\n",
      "iteration 7156 loss 2.7041075229644775, acc 20.3125\n",
      "iteration 7157 loss 2.755992889404297, acc 21.875\n",
      "iteration 7158 loss 2.683732748031616, acc 23.4375\n",
      "iteration 7159 loss 2.7147700786590576, acc 14.0625\n",
      "iteration 7160 loss 2.7423810958862305, acc 25.0\n",
      "iteration 7161 loss 2.874006509780884, acc 17.1875\n",
      "iteration 7162 loss 2.661015748977661, acc 32.8125\n",
      "iteration 7163 loss 2.53652286529541, acc 26.5625\n",
      "iteration 7164 loss 2.640941619873047, acc 18.75\n",
      "iteration 7165 loss 2.4804956912994385, acc 29.6875\n",
      "iteration 7166 loss 2.686896562576294, acc 17.1875\n",
      "iteration 7167 loss 2.731775999069214, acc 21.875\n",
      "iteration 7168 loss 2.6493687629699707, acc 20.3125\n",
      "iteration 7169 loss 2.6938552856445312, acc 20.3125\n",
      "iteration 7170 loss 2.7644054889678955, acc 15.625\n",
      "iteration 7171 loss 2.7281723022460938, acc 21.875\n",
      "iteration 7172 loss 2.7944815158843994, acc 14.0625\n",
      "iteration 7173 loss 2.6225626468658447, acc 20.3125\n",
      "iteration 7174 loss 2.7257027626037598, acc 20.3125\n",
      "iteration 7175 loss 2.7188024520874023, acc 26.5625\n",
      "iteration 7176 loss 2.6913270950317383, acc 18.75\n",
      "iteration 7177 loss 2.7637603282928467, acc 20.3125\n",
      "iteration 7178 loss 2.869640827178955, acc 17.1875\n",
      "iteration 7179 loss 2.5430259704589844, acc 28.125\n",
      "iteration 7180 loss 2.629105806350708, acc 25.0\n",
      "iteration 7181 loss 2.437502861022949, acc 32.8125\n",
      "iteration 7182 loss 2.698532819747925, acc 26.5625\n",
      "iteration 7183 loss 2.693256378173828, acc 18.75\n",
      "iteration 7184 loss 2.960348606109619, acc 17.1875\n",
      "iteration 7185 loss 2.515489339828491, acc 25.0\n",
      "iteration 7186 loss 2.676298141479492, acc 18.75\n",
      "iteration 7187 loss 2.6125283241271973, acc 20.3125\n",
      "iteration 7188 loss 2.7606987953186035, acc 10.9375\n",
      "iteration 7189 loss 2.783644914627075, acc 15.625\n",
      "iteration 7190 loss 2.493621587753296, acc 26.5625\n",
      "iteration 7191 loss 2.4175777435302734, acc 31.25\n",
      "iteration 7192 loss 2.4442315101623535, acc 26.5625\n",
      "iteration 7193 loss 2.381805896759033, acc 34.375\n",
      "iteration 7194 loss 2.6916534900665283, acc 12.5\n",
      "iteration 7195 loss 2.6963114738464355, acc 23.4375\n",
      "iteration 7196 loss 2.4810221195220947, acc 32.8125\n",
      "iteration 7197 loss 2.636923313140869, acc 25.0\n",
      "iteration 7198 loss 3.0597293376922607, acc 10.9375\n",
      "iteration 7199 loss 2.712766170501709, acc 25.0\n",
      "iteration 7200 loss 2.6855385303497314, acc 23.4375\n",
      "iteration 7201 loss 2.6272919178009033, acc 15.625\n",
      "iteration 7202 loss 2.5264251232147217, acc 21.875\n",
      "iteration 7203 loss 2.6296164989471436, acc 32.8125\n",
      "iteration 7204 loss 2.851454973220825, acc 17.1875\n",
      "iteration 7205 loss 2.6388752460479736, acc 25.0\n",
      "iteration 7206 loss 2.7799317836761475, acc 18.75\n",
      "iteration 7207 loss 2.717254161834717, acc 26.5625\n",
      "iteration 7208 loss 2.4705049991607666, acc 31.25\n",
      "iteration 7209 loss 2.7108941078186035, acc 21.875\n",
      "iteration 7210 loss 2.944998264312744, acc 15.625\n",
      "iteration 7211 loss 2.708160400390625, acc 18.75\n",
      "iteration 7212 loss 2.800020217895508, acc 17.1875\n",
      "iteration 7213 loss 2.566413164138794, acc 21.875\n",
      "iteration 7214 loss 2.6177375316619873, acc 21.875\n",
      "iteration 7215 loss 2.6269147396087646, acc 21.875\n",
      "iteration 7216 loss 2.630767822265625, acc 25.0\n",
      "iteration 7217 loss 2.6801300048828125, acc 26.5625\n",
      "iteration 7218 loss 2.710930347442627, acc 21.875\n",
      "iteration 7219 loss 2.6025383472442627, acc 25.0\n",
      "iteration 7220 loss 2.6080150604248047, acc 26.5625\n",
      "iteration 7221 loss 2.433941125869751, acc 28.125\n",
      "iteration 7222 loss 2.8632590770721436, acc 15.625\n",
      "iteration 7223 loss 2.523331880569458, acc 25.0\n",
      "iteration 7224 loss 2.691892623901367, acc 23.4375\n",
      "iteration 7225 loss 2.5927793979644775, acc 23.4375\n",
      "iteration 7226 loss 2.858638048171997, acc 20.3125\n",
      "iteration 7227 loss 2.5947213172912598, acc 26.5625\n",
      "iteration 7228 loss 2.8781023025512695, acc 18.75\n",
      "iteration 7229 loss 2.701277494430542, acc 20.3125\n",
      "iteration 7230 loss 2.5252387523651123, acc 23.4375\n",
      "iteration 7231 loss 2.788219928741455, acc 25.0\n",
      "iteration 7232 loss 2.7612993717193604, acc 17.1875\n",
      "iteration 7233 loss 2.6411731243133545, acc 28.125\n",
      "iteration 7234 loss 2.6859652996063232, acc 25.0\n",
      "iteration 7235 loss 2.7276275157928467, acc 25.0\n",
      "iteration 7236 loss 2.8180649280548096, acc 20.3125\n",
      "iteration 7237 loss 2.7591981887817383, acc 21.875\n",
      "iteration 7238 loss 2.49033784866333, acc 31.25\n",
      "iteration 7239 loss 2.6145741939544678, acc 21.875\n",
      "iteration 7240 loss 2.8227667808532715, acc 20.3125\n",
      "iteration 7241 loss 2.7737717628479004, acc 21.875\n",
      "iteration 7242 loss 2.642786741256714, acc 26.5625\n",
      "iteration 7243 loss 2.555490255355835, acc 21.875\n",
      "iteration 7244 loss 2.55033016204834, acc 31.25\n",
      "iteration 7245 loss 2.836477279663086, acc 18.75\n",
      "iteration 7246 loss 2.604102849960327, acc 23.4375\n",
      "iteration 7247 loss 2.4489543437957764, acc 29.6875\n",
      "iteration 7248 loss 2.864443778991699, acc 20.3125\n",
      "iteration 7249 loss 2.546818971633911, acc 26.5625\n",
      "iteration 7250 loss 2.7222461700439453, acc 20.3125\n",
      "iteration 7251 loss 2.5446994304656982, acc 20.3125\n",
      "iteration 7252 loss 2.677938938140869, acc 20.3125\n",
      "iteration 7253 loss 2.6376354694366455, acc 29.6875\n",
      "iteration 7254 loss 2.809055805206299, acc 21.875\n",
      "iteration 7255 loss 2.422909736633301, acc 21.875\n",
      "iteration 7256 loss 2.5319271087646484, acc 32.8125\n",
      "iteration 7257 loss 2.768397331237793, acc 18.75\n",
      "iteration 7258 loss 2.675142765045166, acc 10.9375\n",
      "iteration 7259 loss 2.8661837577819824, acc 21.875\n",
      "iteration 7260 loss 2.5854289531707764, acc 21.875\n",
      "iteration 7261 loss 2.529019832611084, acc 21.875\n",
      "iteration 7262 loss 2.5184741020202637, acc 29.6875\n",
      "iteration 7263 loss 2.4463870525360107, acc 29.6875\n",
      "iteration 7264 loss 2.9608654975891113, acc 14.0625\n",
      "iteration 7265 loss 2.7286136150360107, acc 20.3125\n",
      "iteration 7266 loss 2.8588829040527344, acc 14.0625\n",
      "iteration 7267 loss 2.8425488471984863, acc 14.0625\n",
      "iteration 7268 loss 2.6722710132598877, acc 20.3125\n",
      "iteration 7269 loss 2.810108184814453, acc 21.875\n",
      "iteration 7270 loss 2.7096569538116455, acc 23.4375\n",
      "iteration 7271 loss 2.695002317428589, acc 29.6875\n",
      "iteration 7272 loss 2.6776132583618164, acc 21.875\n",
      "iteration 7273 loss 2.7686808109283447, acc 14.0625\n",
      "iteration 7274 loss 2.5809082984924316, acc 26.5625\n",
      "iteration 7275 loss 2.879805564880371, acc 15.625\n",
      "iteration 7276 loss 2.607724666595459, acc 23.4375\n",
      "iteration 7277 loss 3.04231333732605, acc 12.5\n",
      "iteration 7278 loss 2.687525510787964, acc 20.3125\n",
      "iteration 7279 loss 2.746807813644409, acc 21.875\n",
      "iteration 7280 loss 2.60553240776062, acc 31.25\n",
      "iteration 7281 loss 2.704254150390625, acc 26.5625\n",
      "iteration 7282 loss 2.927406072616577, acc 14.0625\n",
      "iteration 7283 loss 2.7190945148468018, acc 14.0625\n",
      "iteration 7284 loss 2.4751992225646973, acc 23.4375\n",
      "iteration 7285 loss 2.480621099472046, acc 31.25\n",
      "iteration 7286 loss 2.911064863204956, acc 18.75\n",
      "iteration 7287 loss 2.728503465652466, acc 18.75\n",
      "iteration 7288 loss 2.8101141452789307, acc 10.9375\n",
      "iteration 7289 loss 2.982713222503662, acc 21.875\n",
      "iteration 7290 loss 2.8477768898010254, acc 18.75\n",
      "iteration 7291 loss 2.601659059524536, acc 10.9375\n",
      "iteration 7292 loss 2.661691665649414, acc 20.3125\n",
      "iteration 7293 loss 2.6712865829467773, acc 15.625\n",
      "iteration 7294 loss 2.5833845138549805, acc 25.0\n",
      "iteration 7295 loss 2.630869150161743, acc 26.5625\n",
      "iteration 7296 loss 2.874742269515991, acc 12.5\n",
      "iteration 7297 loss 2.939544916152954, acc 17.1875\n",
      "iteration 7298 loss 2.5393543243408203, acc 25.0\n",
      "iteration 7299 loss 2.616800308227539, acc 26.5625\n",
      "iteration 7300 loss 2.719717264175415, acc 10.9375\n",
      "iteration 7301 loss 2.7302143573760986, acc 20.3125\n",
      "iteration 7302 loss 2.5125601291656494, acc 25.0\n",
      "iteration 7303 loss 2.7185418605804443, acc 15.625\n",
      "iteration 7304 loss 2.735243082046509, acc 18.75\n",
      "iteration 7305 loss 2.534475326538086, acc 26.5625\n",
      "iteration 7306 loss 2.6322803497314453, acc 25.0\n",
      "iteration 7307 loss 2.6558516025543213, acc 17.1875\n",
      "iteration 7308 loss 2.7217962741851807, acc 20.3125\n",
      "iteration 7309 loss 2.778038501739502, acc 21.875\n",
      "iteration 7310 loss 2.7556867599487305, acc 21.875\n",
      "iteration 7311 loss 2.741878032684326, acc 21.875\n",
      "iteration 7312 loss 2.778266429901123, acc 9.375\n",
      "iteration 7313 loss 2.558349132537842, acc 29.6875\n",
      "iteration 7314 loss 2.576796293258667, acc 21.875\n",
      "iteration 7315 loss 2.8827707767486572, acc 14.0625\n",
      "iteration 7316 loss 2.6057512760162354, acc 25.0\n",
      "iteration 7317 loss 2.8275442123413086, acc 18.75\n",
      "iteration 7318 loss 2.896104574203491, acc 14.0625\n",
      "iteration 7319 loss 2.9166059494018555, acc 23.4375\n",
      "iteration 7320 loss 2.60904860496521, acc 21.875\n",
      "iteration 7321 loss 2.6773364543914795, acc 15.625\n",
      "iteration 7322 loss 2.648336172103882, acc 26.5625\n",
      "iteration 7323 loss 2.84623122215271, acc 21.875\n",
      "iteration 7324 loss 2.507753849029541, acc 25.0\n",
      "iteration 7325 loss 2.4241394996643066, acc 34.375\n",
      "iteration 7326 loss 2.6764719486236572, acc 26.5625\n",
      "iteration 7327 loss 2.7941677570343018, acc 15.625\n",
      "iteration 7328 loss 2.6091668605804443, acc 21.875\n",
      "iteration 7329 loss 2.753911018371582, acc 17.1875\n",
      "iteration 7330 loss 2.805126190185547, acc 18.75\n",
      "iteration 7331 loss 2.721188545227051, acc 23.4375\n",
      "iteration 7332 loss 2.6227970123291016, acc 21.875\n",
      "iteration 7333 loss 2.567518472671509, acc 26.5625\n",
      "iteration 7334 loss 2.6384403705596924, acc 23.4375\n",
      "iteration 7335 loss 2.7255852222442627, acc 17.1875\n",
      "iteration 7336 loss 2.6151833534240723, acc 18.75\n",
      "iteration 7337 loss 2.6200997829437256, acc 31.25\n",
      "iteration 7338 loss 2.637894868850708, acc 20.3125\n",
      "iteration 7339 loss 2.663416862487793, acc 21.875\n",
      "iteration 7340 loss 2.584623336791992, acc 23.4375\n",
      "iteration 7341 loss 2.66388201713562, acc 20.3125\n",
      "iteration 7342 loss 2.775226354598999, acc 18.75\n",
      "iteration 7343 loss 2.5989291667938232, acc 25.0\n",
      "iteration 7344 loss 2.745082378387451, acc 18.75\n",
      "iteration 7345 loss 2.4402222633361816, acc 32.8125\n",
      "iteration 7346 loss 2.4045956134796143, acc 31.25\n",
      "iteration 7347 loss 2.730903387069702, acc 23.4375\n",
      "iteration 7348 loss 2.775655746459961, acc 18.75\n",
      "iteration 7349 loss 2.674955368041992, acc 20.3125\n",
      "iteration 7350 loss 2.5893447399139404, acc 25.0\n",
      "iteration 7351 loss 2.7899224758148193, acc 15.625\n",
      "iteration 7352 loss 2.8323066234588623, acc 14.0625\n",
      "iteration 7353 loss 2.6051366329193115, acc 26.5625\n",
      "iteration 7354 loss 2.5239460468292236, acc 25.0\n",
      "iteration 7355 loss 2.864835500717163, acc 15.625\n",
      "iteration 7356 loss 2.587714195251465, acc 29.6875\n",
      "iteration 7357 loss 2.6475331783294678, acc 25.0\n",
      "iteration 7358 loss 2.4623658657073975, acc 31.25\n",
      "iteration 7359 loss 2.6119227409362793, acc 25.0\n",
      "iteration 7360 loss 2.5987696647644043, acc 26.5625\n",
      "iteration 7361 loss 2.6668753623962402, acc 26.5625\n",
      "iteration 7362 loss 2.4903523921966553, acc 28.125\n",
      "iteration 7363 loss 2.5633323192596436, acc 20.3125\n",
      "iteration 7364 loss 2.5093584060668945, acc 26.5625\n",
      "iteration 7365 loss 2.79105544090271, acc 23.4375\n",
      "iteration 7366 loss 2.6637792587280273, acc 18.75\n",
      "iteration 7367 loss 2.716789484024048, acc 18.75\n",
      "iteration 7368 loss 2.687659978866577, acc 20.3125\n",
      "iteration 7369 loss 2.467599391937256, acc 32.8125\n",
      "iteration 7370 loss 2.5870471000671387, acc 20.3125\n",
      "iteration 7371 loss 2.6455869674682617, acc 21.875\n",
      "iteration 7372 loss 2.8301661014556885, acc 9.375\n",
      "iteration 7373 loss 2.8820114135742188, acc 15.625\n",
      "iteration 7374 loss 2.7527477741241455, acc 20.3125\n",
      "iteration 7375 loss 2.6341400146484375, acc 28.125\n",
      "iteration 7376 loss 2.4459996223449707, acc 28.125\n",
      "iteration 7377 loss 2.560114860534668, acc 18.75\n",
      "iteration 7378 loss 2.776350259780884, acc 25.0\n",
      "iteration 7379 loss 2.6500120162963867, acc 20.3125\n",
      "iteration 7380 loss 2.5245325565338135, acc 29.6875\n",
      "iteration 7381 loss 2.85440731048584, acc 25.0\n",
      "iteration 7382 loss 2.8143129348754883, acc 18.75\n",
      "iteration 7383 loss 2.6334164142608643, acc 21.875\n",
      "iteration 7384 loss 2.7382779121398926, acc 14.0625\n",
      "iteration 7385 loss 2.70337176322937, acc 14.0625\n",
      "iteration 7386 loss 2.6370437145233154, acc 20.3125\n",
      "iteration 7387 loss 2.548100471496582, acc 15.625\n",
      "iteration 7388 loss 2.7693188190460205, acc 15.625\n",
      "iteration 7389 loss 2.646131753921509, acc 21.875\n",
      "iteration 7390 loss 2.5491364002227783, acc 25.0\n",
      "iteration 7391 loss 2.653125047683716, acc 18.75\n",
      "iteration 7392 loss 2.471580743789673, acc 32.8125\n",
      "iteration 7393 loss 2.6755192279815674, acc 20.3125\n",
      "iteration 7394 loss 2.5968685150146484, acc 25.0\n",
      "iteration 7395 loss 2.7291324138641357, acc 25.0\n",
      "iteration 7396 loss 2.5586955547332764, acc 23.4375\n",
      "iteration 7397 loss 2.607041120529175, acc 28.125\n",
      "iteration 7398 loss 2.779468536376953, acc 17.1875\n",
      "iteration 7399 loss 2.9245827198028564, acc 14.0625\n",
      "iteration 7400 loss 2.7952632904052734, acc 17.1875\n",
      "iteration 7401 loss 2.6841957569122314, acc 18.75\n",
      "iteration 7402 loss 2.5404984951019287, acc 29.6875\n",
      "iteration 7403 loss 2.6792163848876953, acc 28.125\n",
      "iteration 7404 loss 2.65484356880188, acc 20.3125\n",
      "iteration 7405 loss 2.6252150535583496, acc 20.3125\n",
      "iteration 7406 loss 2.5969998836517334, acc 28.125\n",
      "iteration 7407 loss 3.043304443359375, acc 12.5\n",
      "iteration 7408 loss 2.3901314735412598, acc 35.9375\n",
      "iteration 7409 loss 2.639907121658325, acc 25.0\n",
      "iteration 7410 loss 2.8482110500335693, acc 18.75\n",
      "iteration 7411 loss 2.7222752571105957, acc 20.3125\n",
      "iteration 7412 loss 2.5623672008514404, acc 26.5625\n",
      "iteration 7413 loss 2.6846940517425537, acc 20.3125\n",
      "iteration 7414 loss 2.615973472595215, acc 26.5625\n",
      "iteration 7415 loss 2.6580939292907715, acc 28.125\n",
      "iteration 7416 loss 2.8525781631469727, acc 23.4375\n",
      "iteration 7417 loss 2.9524385929107666, acc 12.5\n",
      "iteration 7418 loss 2.8352160453796387, acc 18.75\n",
      "iteration 7419 loss 2.6793229579925537, acc 17.1875\n",
      "iteration 7420 loss 2.6306629180908203, acc 18.75\n",
      "iteration 7421 loss 2.9178624153137207, acc 12.5\n",
      "iteration 7422 loss 2.626378297805786, acc 12.5\n",
      "iteration 7423 loss 2.696730852127075, acc 18.75\n",
      "iteration 7424 loss 2.59618878364563, acc 23.4375\n",
      "iteration 7425 loss 2.772878885269165, acc 12.5\n",
      "iteration 7426 loss 2.649101972579956, acc 28.125\n",
      "iteration 7427 loss 2.691608190536499, acc 21.875\n",
      "iteration 7428 loss 2.5735507011413574, acc 23.4375\n",
      "iteration 7429 loss 2.57315993309021, acc 20.3125\n",
      "iteration 7430 loss 2.4979095458984375, acc 28.125\n",
      "iteration 7431 loss 2.7189745903015137, acc 28.125\n",
      "iteration 7432 loss 2.5707011222839355, acc 26.5625\n",
      "iteration 7433 loss 2.2998242378234863, acc 37.5\n",
      "iteration 7434 loss 3.0024890899658203, acc 20.3125\n",
      "iteration 7435 loss 2.707003116607666, acc 31.25\n",
      "iteration 7436 loss 2.6661429405212402, acc 18.75\n",
      "iteration 7437 loss 2.8506014347076416, acc 14.0625\n",
      "iteration 7438 loss 2.5846946239471436, acc 26.5625\n",
      "iteration 7439 loss 2.4871013164520264, acc 21.875\n",
      "iteration 7440 loss 2.730973482131958, acc 25.0\n",
      "iteration 7441 loss 2.6212217807769775, acc 25.0\n",
      "iteration 7442 loss 2.9160866737365723, acc 18.75\n",
      "iteration 7443 loss 2.6624972820281982, acc 31.25\n",
      "iteration 7444 loss 2.725982189178467, acc 18.75\n",
      "iteration 7445 loss 2.645890235900879, acc 28.125\n",
      "iteration 7446 loss 2.775797128677368, acc 14.0625\n",
      "iteration 7447 loss 2.4702537059783936, acc 26.5625\n",
      "iteration 7448 loss 2.6618096828460693, acc 21.875\n",
      "iteration 7449 loss 2.6782538890838623, acc 20.3125\n",
      "iteration 7450 loss 2.694791316986084, acc 23.4375\n",
      "iteration 7451 loss 2.7294139862060547, acc 17.1875\n",
      "iteration 7452 loss 2.6819589138031006, acc 25.0\n",
      "iteration 7453 loss 2.8363001346588135, acc 12.5\n",
      "iteration 7454 loss 2.673582077026367, acc 26.5625\n",
      "iteration 7455 loss 2.6469268798828125, acc 18.75\n",
      "iteration 7456 loss 2.641746997833252, acc 20.3125\n",
      "iteration 7457 loss 2.6867330074310303, acc 20.3125\n",
      "iteration 7458 loss 2.5906143188476562, acc 23.4375\n",
      "iteration 7459 loss 2.790724039077759, acc 15.625\n",
      "iteration 7460 loss 2.6777007579803467, acc 28.125\n",
      "iteration 7461 loss 2.700336217880249, acc 25.0\n",
      "iteration 7462 loss 2.519378900527954, acc 23.4375\n",
      "iteration 7463 loss 2.837191581726074, acc 15.625\n",
      "iteration 7464 loss 2.819133758544922, acc 20.3125\n",
      "iteration 7465 loss 2.8357856273651123, acc 15.625\n",
      "iteration 7466 loss 2.6606881618499756, acc 21.875\n",
      "iteration 7467 loss 2.725879430770874, acc 25.0\n",
      "iteration 7468 loss 2.64241623878479, acc 25.0\n",
      "iteration 7469 loss 2.9178738594055176, acc 18.75\n",
      "iteration 7470 loss 2.662537097930908, acc 17.1875\n",
      "iteration 7471 loss 2.603039503097534, acc 29.6875\n",
      "iteration 7472 loss 2.6021623611450195, acc 20.3125\n",
      "iteration 7473 loss 2.67807674407959, acc 20.3125\n",
      "iteration 7474 loss 2.7363853454589844, acc 21.875\n",
      "iteration 7475 loss 2.7137107849121094, acc 29.6875\n",
      "iteration 7476 loss 2.529808759689331, acc 20.3125\n",
      "iteration 7477 loss 2.7596747875213623, acc 12.5\n",
      "iteration 7478 loss 2.6645171642303467, acc 28.125\n",
      "iteration 7479 loss 2.604553461074829, acc 21.875\n",
      "iteration 7480 loss 2.6449997425079346, acc 20.3125\n",
      "iteration 7481 loss 2.8293001651763916, acc 18.75\n",
      "iteration 7482 loss 2.7880733013153076, acc 21.875\n",
      "iteration 7483 loss 2.6939237117767334, acc 20.3125\n",
      "iteration 7484 loss 2.763052225112915, acc 18.75\n",
      "iteration 7485 loss 2.667715549468994, acc 26.5625\n",
      "iteration 7486 loss 2.7311859130859375, acc 18.75\n",
      "iteration 7487 loss 2.765765905380249, acc 23.4375\n",
      "iteration 7488 loss 2.68393874168396, acc 18.75\n",
      "iteration 7489 loss 2.6513519287109375, acc 28.125\n",
      "iteration 7490 loss 2.8294894695281982, acc 17.1875\n",
      "iteration 7491 loss 2.8173787593841553, acc 26.5625\n",
      "iteration 7492 loss 2.6778042316436768, acc 20.3125\n",
      "iteration 7493 loss 2.6766300201416016, acc 21.875\n",
      "iteration 7494 loss 2.570822238922119, acc 23.4375\n",
      "iteration 7495 loss 2.6109299659729004, acc 26.5625\n",
      "iteration 7496 loss 2.650500774383545, acc 29.6875\n",
      "iteration 7497 loss 2.832263231277466, acc 17.1875\n",
      "iteration 7498 loss 2.705111265182495, acc 25.0\n",
      "iteration 7499 loss 2.657670497894287, acc 17.1875\n",
      "iteration 7500 loss 2.664496898651123, acc 20.3125\n",
      "iteration 7501 loss 2.687556743621826, acc 17.1875\n",
      "iteration 7502 loss 2.727874994277954, acc 18.75\n",
      "iteration 7503 loss 2.780660629272461, acc 23.4375\n",
      "iteration 7504 loss 2.3754475116729736, acc 32.8125\n",
      "iteration 7505 loss 2.913465738296509, acc 20.3125\n",
      "iteration 7506 loss 2.6111762523651123, acc 25.0\n",
      "iteration 7507 loss 2.6943955421447754, acc 28.125\n",
      "iteration 7508 loss 2.490391254425049, acc 34.375\n",
      "iteration 7509 loss 2.555040121078491, acc 26.5625\n",
      "iteration 7510 loss 2.7544312477111816, acc 25.0\n",
      "iteration 7511 loss 2.713494062423706, acc 21.875\n",
      "iteration 7512 loss 2.741179943084717, acc 25.0\n",
      "iteration 7513 loss 2.7746570110321045, acc 23.4375\n",
      "iteration 7514 loss 2.903968572616577, acc 18.75\n",
      "iteration 7515 loss 2.624413251876831, acc 28.125\n",
      "iteration 7516 loss 2.742764472961426, acc 25.0\n",
      "iteration 7517 loss 2.879776954650879, acc 17.1875\n",
      "iteration 7518 loss 2.7426400184631348, acc 20.3125\n",
      "iteration 7519 loss 2.5430710315704346, acc 29.6875\n",
      "iteration 7520 loss 2.7533745765686035, acc 25.0\n",
      "iteration 7521 loss 2.6449496746063232, acc 18.75\n",
      "iteration 7522 loss 2.606100082397461, acc 31.25\n",
      "iteration 7523 loss 2.975656747817993, acc 18.75\n",
      "iteration 7524 loss 2.782170295715332, acc 20.3125\n",
      "iteration 7525 loss 2.6468875408172607, acc 21.875\n",
      "iteration 7526 loss 2.6821889877319336, acc 20.3125\n",
      "iteration 7527 loss 2.689035177230835, acc 25.0\n",
      "iteration 7528 loss 2.822262763977051, acc 17.1875\n",
      "iteration 7529 loss 2.6296072006225586, acc 29.6875\n",
      "iteration 7530 loss 2.7422118186950684, acc 18.75\n",
      "iteration 7531 loss 2.7160918712615967, acc 17.1875\n",
      "iteration 7532 loss 2.7620151042938232, acc 12.5\n",
      "iteration 7533 loss 2.76430606842041, acc 23.4375\n",
      "iteration 7534 loss 2.536316394805908, acc 29.6875\n",
      "iteration 7535 loss 2.629737615585327, acc 26.5625\n",
      "iteration 7536 loss 2.711561918258667, acc 17.1875\n",
      "iteration 7537 loss 2.807288885116577, acc 20.3125\n",
      "iteration 7538 loss 2.5173630714416504, acc 31.25\n",
      "iteration 7539 loss 2.7015461921691895, acc 18.75\n",
      "iteration 7540 loss 2.639002561569214, acc 26.5625\n",
      "iteration 7541 loss 2.7924511432647705, acc 14.0625\n",
      "iteration 7542 loss 2.772338628768921, acc 15.625\n",
      "iteration 7543 loss 2.798527956008911, acc 18.75\n",
      "iteration 7544 loss 2.5518646240234375, acc 29.6875\n",
      "iteration 7545 loss 2.685312271118164, acc 23.4375\n",
      "iteration 7546 loss 2.8841731548309326, acc 10.9375\n",
      "iteration 7547 loss 2.710714340209961, acc 21.875\n",
      "iteration 7548 loss 2.7023062705993652, acc 21.875\n",
      "iteration 7549 loss 2.832164764404297, acc 17.1875\n",
      "iteration 7550 loss 2.7685887813568115, acc 21.875\n",
      "iteration 7551 loss 2.8168230056762695, acc 14.0625\n",
      "iteration 7552 loss 2.8624589443206787, acc 17.1875\n",
      "iteration 7553 loss 2.8586738109588623, acc 21.875\n",
      "iteration 7554 loss 2.8079612255096436, acc 20.3125\n",
      "iteration 7555 loss 2.8614115715026855, acc 17.1875\n",
      "iteration 7556 loss 2.5071640014648438, acc 23.4375\n",
      "iteration 7557 loss 2.8672404289245605, acc 15.625\n",
      "iteration 7558 loss 2.6826000213623047, acc 20.3125\n",
      "iteration 7559 loss 2.7421889305114746, acc 14.0625\n",
      "iteration 7560 loss 2.6697142124176025, acc 23.4375\n",
      "iteration 7561 loss 2.603260040283203, acc 29.6875\n",
      "iteration 7562 loss 2.5697412490844727, acc 23.4375\n",
      "iteration 7563 loss 2.6930248737335205, acc 21.875\n",
      "iteration 7564 loss 2.536118268966675, acc 18.75\n",
      "iteration 7565 loss 2.891425848007202, acc 17.1875\n",
      "iteration 7566 loss 2.6837003231048584, acc 20.3125\n",
      "iteration 7567 loss 2.6434459686279297, acc 18.75\n",
      "iteration 7568 loss 2.66437029838562, acc 20.3125\n",
      "iteration 7569 loss 2.8023669719696045, acc 18.75\n",
      "iteration 7570 loss 2.547947883605957, acc 28.125\n",
      "iteration 7571 loss 2.521974802017212, acc 31.25\n",
      "iteration 7572 loss 2.892770767211914, acc 14.0625\n",
      "iteration 7573 loss 2.787494659423828, acc 15.625\n",
      "iteration 7574 loss 2.6458938121795654, acc 21.875\n",
      "iteration 7575 loss 2.6959426403045654, acc 28.125\n",
      "iteration 7576 loss 2.5762386322021484, acc 28.125\n",
      "iteration 7577 loss 2.882713794708252, acc 10.9375\n",
      "iteration 7578 loss 2.575652599334717, acc 25.0\n",
      "iteration 7579 loss 2.5925636291503906, acc 26.5625\n",
      "iteration 7580 loss 2.3329899311065674, acc 31.25\n",
      "iteration 7581 loss 2.692303419113159, acc 17.1875\n",
      "iteration 7582 loss 2.6419968605041504, acc 23.4375\n",
      "iteration 7583 loss 2.5519814491271973, acc 21.875\n",
      "iteration 7584 loss 2.507274866104126, acc 32.8125\n",
      "iteration 7585 loss 2.4932613372802734, acc 21.875\n",
      "iteration 7586 loss 2.804234504699707, acc 14.0625\n",
      "iteration 7587 loss 2.744272470474243, acc 21.875\n",
      "iteration 7588 loss 2.576079845428467, acc 23.4375\n",
      "iteration 7589 loss 2.772226572036743, acc 23.4375\n",
      "iteration 7590 loss 2.7199177742004395, acc 20.3125\n",
      "iteration 7591 loss 2.534449577331543, acc 25.0\n",
      "iteration 7592 loss 2.6335933208465576, acc 26.5625\n",
      "iteration 7593 loss 2.6783037185668945, acc 23.4375\n",
      "iteration 7594 loss 2.841492176055908, acc 9.375\n",
      "iteration 7595 loss 2.7186436653137207, acc 25.0\n",
      "iteration 7596 loss 2.6935198307037354, acc 15.625\n",
      "iteration 7597 loss 2.8808023929595947, acc 17.1875\n",
      "iteration 7598 loss 2.76605224609375, acc 23.4375\n",
      "iteration 7599 loss 2.617804765701294, acc 31.25\n",
      "iteration 7600 loss 2.7824790477752686, acc 15.625\n",
      "iteration 7601 loss 2.8920581340789795, acc 10.9375\n",
      "iteration 7602 loss 2.8488547801971436, acc 12.5\n",
      "iteration 7603 loss 2.665753126144409, acc 20.3125\n",
      "iteration 7604 loss 2.585719108581543, acc 28.125\n",
      "iteration 7605 loss 2.6465835571289062, acc 25.0\n",
      "iteration 7606 loss 2.7508742809295654, acc 20.3125\n",
      "iteration 7607 loss 2.718695878982544, acc 20.3125\n",
      "iteration 7608 loss 2.4313411712646484, acc 29.6875\n",
      "iteration 7609 loss 2.8535900115966797, acc 10.9375\n",
      "iteration 7610 loss 2.522069215774536, acc 32.8125\n",
      "iteration 7611 loss 2.9174532890319824, acc 14.0625\n",
      "iteration 7612 loss 2.8072187900543213, acc 18.75\n",
      "iteration 7613 loss 2.5650343894958496, acc 29.6875\n",
      "iteration 7614 loss 2.580861806869507, acc 28.125\n",
      "iteration 7615 loss 2.84090518951416, acc 25.0\n",
      "iteration 7616 loss 2.5191280841827393, acc 23.4375\n",
      "iteration 7617 loss 2.538597822189331, acc 26.5625\n",
      "iteration 7618 loss 2.5837175846099854, acc 28.125\n",
      "iteration 7619 loss 2.8216919898986816, acc 18.75\n",
      "iteration 7620 loss 2.5643162727355957, acc 21.875\n",
      "iteration 7621 loss 2.4544458389282227, acc 26.5625\n",
      "iteration 7622 loss 2.979330062866211, acc 10.9375\n",
      "iteration 7623 loss 2.6404383182525635, acc 25.0\n",
      "iteration 7624 loss 2.6039552688598633, acc 26.5625\n",
      "iteration 7625 loss 2.696700096130371, acc 17.1875\n",
      "iteration 7626 loss 2.497559070587158, acc 37.5\n",
      "iteration 7627 loss 2.904672145843506, acc 20.3125\n",
      "iteration 7628 loss 2.581085443496704, acc 18.75\n",
      "iteration 7629 loss 2.78906512260437, acc 18.75\n",
      "iteration 7630 loss 2.4349899291992188, acc 32.8125\n",
      "iteration 7631 loss 2.9165420532226562, acc 14.0625\n",
      "iteration 7632 loss 2.4851715564727783, acc 26.5625\n",
      "iteration 7633 loss 2.6102612018585205, acc 21.875\n",
      "iteration 7634 loss 2.8216168880462646, acc 26.5625\n",
      "iteration 7635 loss 2.7822813987731934, acc 15.625\n",
      "iteration 7636 loss 2.7215020656585693, acc 21.875\n",
      "iteration 7637 loss 2.655015707015991, acc 21.875\n",
      "iteration 7638 loss 2.580763578414917, acc 31.25\n",
      "iteration 7639 loss 2.59649395942688, acc 26.5625\n",
      "iteration 7640 loss 2.703744649887085, acc 20.3125\n",
      "iteration 7641 loss 2.878706455230713, acc 18.75\n",
      "iteration 7642 loss 2.660205125808716, acc 25.0\n",
      "iteration 7643 loss 2.6398491859436035, acc 28.125\n",
      "iteration 7644 loss 2.6698575019836426, acc 28.125\n",
      "iteration 7645 loss 2.7660813331604004, acc 26.5625\n",
      "iteration 7646 loss 2.6700000762939453, acc 26.5625\n",
      "iteration 7647 loss 2.620682954788208, acc 26.5625\n",
      "iteration 7648 loss 2.7039284706115723, acc 21.875\n",
      "iteration 7649 loss 2.633291482925415, acc 26.5625\n",
      "iteration 7650 loss 2.7161900997161865, acc 21.875\n",
      "iteration 7651 loss 2.7298331260681152, acc 26.5625\n",
      "iteration 7652 loss 2.5020225048065186, acc 37.5\n",
      "iteration 7653 loss 2.7517290115356445, acc 21.875\n",
      "iteration 7654 loss 2.802556276321411, acc 20.3125\n",
      "iteration 7655 loss 2.685885190963745, acc 21.875\n",
      "iteration 7656 loss 2.7777507305145264, acc 20.3125\n",
      "iteration 7657 loss 2.650958299636841, acc 21.875\n",
      "iteration 7658 loss 2.635775566101074, acc 18.75\n",
      "iteration 7659 loss 2.884558916091919, acc 15.625\n",
      "iteration 7660 loss 2.7436330318450928, acc 15.625\n",
      "iteration 7661 loss 2.4716105461120605, acc 31.25\n",
      "iteration 7662 loss 2.694594144821167, acc 28.125\n",
      "iteration 7663 loss 2.6583800315856934, acc 21.875\n",
      "iteration 7664 loss 2.664844274520874, acc 21.875\n",
      "iteration 7665 loss 2.4109890460968018, acc 26.5625\n",
      "iteration 7666 loss 2.589890718460083, acc 31.25\n",
      "iteration 7667 loss 2.578749895095825, acc 21.875\n",
      "iteration 7668 loss 2.7055816650390625, acc 28.125\n",
      "iteration 7669 loss 2.920757532119751, acc 15.625\n",
      "iteration 7670 loss 2.5979814529418945, acc 28.125\n",
      "iteration 7671 loss 2.6754984855651855, acc 25.0\n",
      "iteration 7672 loss 2.806647539138794, acc 21.875\n",
      "iteration 7673 loss 2.706733226776123, acc 31.25\n",
      "iteration 7674 loss 2.5168118476867676, acc 29.6875\n",
      "iteration 7675 loss 2.4720468521118164, acc 34.375\n",
      "iteration 7676 loss 2.7801976203918457, acc 17.1875\n",
      "iteration 7677 loss 2.6593990325927734, acc 29.6875\n",
      "iteration 7678 loss 2.6250836849212646, acc 25.0\n",
      "iteration 7679 loss 2.6455554962158203, acc 25.0\n",
      "iteration 7680 loss 2.512256622314453, acc 26.5625\n",
      "iteration 7681 loss 2.743640184402466, acc 18.75\n",
      "iteration 7682 loss 2.5890467166900635, acc 23.4375\n",
      "iteration 7683 loss 2.9215681552886963, acc 9.375\n",
      "iteration 7684 loss 2.57525372505188, acc 18.75\n",
      "iteration 7685 loss 2.818587064743042, acc 25.0\n",
      "iteration 7686 loss 2.791982650756836, acc 21.875\n",
      "iteration 7687 loss 2.7747373580932617, acc 17.1875\n",
      "iteration 7688 loss 2.852886438369751, acc 17.1875\n",
      "iteration 7689 loss 2.590919256210327, acc 25.0\n",
      "iteration 7690 loss 2.726123809814453, acc 15.625\n",
      "iteration 7691 loss 2.658282518386841, acc 21.875\n",
      "iteration 7692 loss 2.974273204803467, acc 10.9375\n",
      "iteration 7693 loss 2.718350648880005, acc 25.0\n",
      "iteration 7694 loss 2.7554216384887695, acc 21.875\n",
      "iteration 7695 loss 2.599789619445801, acc 20.3125\n",
      "iteration 7696 loss 2.5471322536468506, acc 29.6875\n",
      "iteration 7697 loss 2.686306953430176, acc 18.75\n",
      "iteration 7698 loss 2.7814762592315674, acc 23.4375\n",
      "iteration 7699 loss 2.6139285564422607, acc 20.3125\n",
      "iteration 7700 loss 2.636404275894165, acc 17.1875\n",
      "iteration 7701 loss 2.668400287628174, acc 21.875\n",
      "iteration 7702 loss 2.7797963619232178, acc 20.3125\n",
      "iteration 7703 loss 2.755631923675537, acc 17.1875\n",
      "iteration 7704 loss 2.806424856185913, acc 20.3125\n",
      "iteration 7705 loss 2.57568359375, acc 26.5625\n",
      "iteration 7706 loss 2.735213041305542, acc 20.3125\n",
      "iteration 7707 loss 2.600827693939209, acc 23.4375\n",
      "iteration 7708 loss 2.69665789604187, acc 17.1875\n",
      "iteration 7709 loss 2.813784122467041, acc 21.875\n",
      "iteration 7710 loss 2.577155590057373, acc 29.6875\n",
      "iteration 7711 loss 2.711439609527588, acc 23.4375\n",
      "iteration 7712 loss 2.5999703407287598, acc 23.4375\n",
      "iteration 7713 loss 2.5910775661468506, acc 21.875\n",
      "iteration 7714 loss 2.745598793029785, acc 26.5625\n",
      "iteration 7715 loss 2.7928006649017334, acc 14.0625\n",
      "iteration 7716 loss 2.6664469242095947, acc 23.4375\n",
      "iteration 7717 loss 2.647777795791626, acc 26.5625\n",
      "iteration 7718 loss 2.6557414531707764, acc 23.4375\n",
      "iteration 7719 loss 2.8380329608917236, acc 20.3125\n",
      "iteration 7720 loss 2.7428648471832275, acc 17.1875\n",
      "iteration 7721 loss 2.74949049949646, acc 21.875\n",
      "iteration 7722 loss 2.8125691413879395, acc 17.1875\n",
      "iteration 7723 loss 2.618431329727173, acc 23.4375\n",
      "iteration 7724 loss 2.5972800254821777, acc 23.4375\n",
      "iteration 7725 loss 2.8695929050445557, acc 15.625\n",
      "iteration 7726 loss 2.397205352783203, acc 39.0625\n",
      "iteration 7727 loss 2.724418878555298, acc 15.625\n",
      "iteration 7728 loss 2.6352620124816895, acc 23.4375\n",
      "iteration 7729 loss 3.0465054512023926, acc 18.75\n",
      "iteration 7730 loss 2.8758769035339355, acc 20.3125\n",
      "iteration 7731 loss 2.788349151611328, acc 21.875\n",
      "iteration 7732 loss 2.5015552043914795, acc 26.5625\n",
      "iteration 7733 loss 2.623129367828369, acc 21.875\n",
      "iteration 7734 loss 2.6929986476898193, acc 15.625\n",
      "iteration 7735 loss 2.736618757247925, acc 20.3125\n",
      "iteration 7736 loss 2.692089557647705, acc 21.875\n",
      "iteration 7737 loss 2.796626091003418, acc 15.625\n",
      "iteration 7738 loss 2.6460776329040527, acc 18.75\n",
      "iteration 7739 loss 2.6132614612579346, acc 20.3125\n",
      "iteration 7740 loss 2.7570793628692627, acc 15.625\n",
      "iteration 7741 loss 2.847407817840576, acc 23.4375\n",
      "iteration 7742 loss 2.669684648513794, acc 15.625\n",
      "iteration 7743 loss 2.754728317260742, acc 21.875\n",
      "iteration 7744 loss 2.8800830841064453, acc 17.1875\n",
      "iteration 7745 loss 2.5649991035461426, acc 25.0\n",
      "iteration 7746 loss 2.6515536308288574, acc 29.6875\n",
      "iteration 7747 loss 2.5292489528656006, acc 29.6875\n",
      "iteration 7748 loss 2.742753028869629, acc 18.75\n",
      "iteration 7749 loss 2.7166619300842285, acc 18.75\n",
      "iteration 7750 loss 2.6113579273223877, acc 23.4375\n",
      "iteration 7751 loss 2.7272961139678955, acc 17.1875\n",
      "iteration 7752 loss 2.699256181716919, acc 21.875\n",
      "iteration 7753 loss 2.789119243621826, acc 18.75\n",
      "iteration 7754 loss 2.6837522983551025, acc 21.875\n",
      "iteration 7755 loss 2.7617526054382324, acc 17.1875\n",
      "iteration 7756 loss 2.678473949432373, acc 25.0\n",
      "iteration 7757 loss 2.4734082221984863, acc 28.125\n",
      "iteration 7758 loss 2.645857334136963, acc 20.3125\n",
      "iteration 7759 loss 2.7448666095733643, acc 21.875\n",
      "iteration 7760 loss 2.617856979370117, acc 23.4375\n",
      "iteration 7761 loss 2.801302433013916, acc 15.625\n",
      "iteration 7762 loss 2.733938694000244, acc 15.625\n",
      "iteration 7763 loss 2.752500295639038, acc 10.9375\n",
      "iteration 7764 loss 2.5058953762054443, acc 26.5625\n",
      "iteration 7765 loss 2.6397345066070557, acc 29.6875\n",
      "iteration 7766 loss 2.836291790008545, acc 20.3125\n",
      "iteration 7767 loss 2.7056655883789062, acc 18.75\n",
      "iteration 7768 loss 2.720195770263672, acc 21.875\n",
      "iteration 7769 loss 2.7414700984954834, acc 18.75\n",
      "iteration 7770 loss 2.684706211090088, acc 18.75\n",
      "iteration 7771 loss 2.535430431365967, acc 35.9375\n",
      "iteration 7772 loss 2.7968313694000244, acc 20.3125\n",
      "iteration 7773 loss 2.794907331466675, acc 20.3125\n",
      "iteration 7774 loss 2.5487759113311768, acc 28.125\n",
      "iteration 7775 loss 2.7906365394592285, acc 23.4375\n",
      "iteration 7776 loss 2.5757081508636475, acc 18.75\n",
      "iteration 7777 loss 2.670164108276367, acc 12.5\n",
      "iteration 7778 loss 2.8686256408691406, acc 25.0\n",
      "iteration 7779 loss 2.8411648273468018, acc 15.625\n",
      "iteration 7780 loss 2.8751275539398193, acc 18.75\n",
      "iteration 7781 loss 2.6838316917419434, acc 25.0\n",
      "iteration 7782 loss 2.695344924926758, acc 26.5625\n",
      "iteration 7783 loss 2.741770029067993, acc 23.4375\n",
      "iteration 7784 loss 2.716888666152954, acc 18.75\n",
      "iteration 7785 loss 2.771346092224121, acc 23.4375\n",
      "iteration 7786 loss 2.544550657272339, acc 23.4375\n",
      "iteration 7787 loss 2.7406747341156006, acc 17.1875\n",
      "iteration 7788 loss 2.750159740447998, acc 25.0\n",
      "iteration 7789 loss 2.5730466842651367, acc 25.0\n",
      "iteration 7790 loss 2.7367141246795654, acc 21.875\n",
      "iteration 7791 loss 2.606818675994873, acc 25.0\n",
      "iteration 7792 loss 2.715630292892456, acc 21.875\n",
      "iteration 7793 loss 2.86492919921875, acc 18.75\n",
      "iteration 7794 loss 2.7677180767059326, acc 17.1875\n",
      "iteration 7795 loss 2.649542808532715, acc 20.3125\n",
      "iteration 7796 loss 2.707746744155884, acc 14.0625\n",
      "iteration 7797 loss 2.6398260593414307, acc 20.3125\n",
      "iteration 7798 loss 2.589529514312744, acc 23.4375\n",
      "iteration 7799 loss 2.759387254714966, acc 17.1875\n",
      "iteration 7800 loss 3.0053651332855225, acc 12.5\n",
      "iteration 7801 loss 2.7380659580230713, acc 23.4375\n",
      "iteration 7802 loss 2.7388174533843994, acc 25.0\n",
      "iteration 7803 loss 2.6977555751800537, acc 15.625\n",
      "iteration 7804 loss 2.60123872756958, acc 28.125\n",
      "iteration 7805 loss 2.604515552520752, acc 25.0\n",
      "iteration 7806 loss 2.646721839904785, acc 18.75\n",
      "iteration 7807 loss 2.9098336696624756, acc 20.3125\n",
      "iteration 7808 loss 2.654219627380371, acc 25.0\n",
      "iteration 7809 loss 2.5951573848724365, acc 21.875\n",
      "iteration 7810 loss 2.7391774654388428, acc 29.6875\n",
      "iteration 7811 loss 2.6530730724334717, acc 17.1875\n",
      "iteration 7812 loss 2.491961717605591, acc 23.4375\n",
      "iteration 7813 loss 2.5755364894866943, acc 21.875\n",
      "iteration 7814 loss 2.908310890197754, acc 15.625\n",
      "iteration 7815 loss 2.649319887161255, acc 28.125\n",
      "iteration 7816 loss 2.3895750045776367, acc 29.6875\n",
      "iteration 7817 loss 2.8684675693511963, acc 9.375\n",
      "iteration 7818 loss 2.8354759216308594, acc 17.1875\n",
      "iteration 7819 loss 2.492473840713501, acc 35.9375\n",
      "iteration 7820 loss 2.6405246257781982, acc 18.75\n",
      "iteration 7821 loss 2.9879422187805176, acc 15.625\n",
      "iteration 7822 loss 2.8686747550964355, acc 21.875\n",
      "iteration 7823 loss 2.8229992389678955, acc 17.1875\n",
      "iteration 7824 loss 2.6246485710144043, acc 25.0\n",
      "iteration 7825 loss 2.589682102203369, acc 21.875\n",
      "iteration 7826 loss 2.71116042137146, acc 26.5625\n",
      "iteration 7827 loss 2.92608904838562, acc 14.0625\n",
      "iteration 7828 loss 2.630513906478882, acc 21.875\n",
      "iteration 7829 loss 2.5654680728912354, acc 28.125\n",
      "iteration 7830 loss 2.938049793243408, acc 6.25\n",
      "iteration 7831 loss 2.470407009124756, acc 32.8125\n",
      "iteration 7832 loss 2.6701791286468506, acc 29.6875\n",
      "iteration 7833 loss 2.5375680923461914, acc 28.125\n",
      "iteration 7834 loss 2.6332287788391113, acc 21.875\n",
      "iteration 7835 loss 2.6722371578216553, acc 18.75\n",
      "iteration 7836 loss 2.5251526832580566, acc 25.0\n",
      "iteration 7837 loss 2.6608612537384033, acc 26.5625\n",
      "iteration 7838 loss 2.662106990814209, acc 18.75\n",
      "iteration 7839 loss 2.797410726547241, acc 17.1875\n",
      "iteration 7840 loss 2.835620880126953, acc 25.0\n",
      "iteration 7841 loss 2.6104767322540283, acc 17.1875\n",
      "iteration 7842 loss 2.6862754821777344, acc 10.9375\n",
      "iteration 7843 loss 2.5775418281555176, acc 18.75\n",
      "iteration 7844 loss 3.139566659927368, acc 7.8125\n",
      "iteration 7845 loss 2.7076494693756104, acc 20.3125\n",
      "iteration 7846 loss 2.676710605621338, acc 21.875\n",
      "iteration 7847 loss 2.5064847469329834, acc 29.6875\n",
      "iteration 7848 loss 2.766388416290283, acc 18.75\n",
      "iteration 7849 loss 2.5595414638519287, acc 29.6875\n",
      "iteration 7850 loss 2.630086898803711, acc 25.0\n",
      "iteration 7851 loss 2.651214361190796, acc 18.75\n",
      "iteration 7852 loss 2.528075695037842, acc 31.25\n",
      "iteration 7853 loss 2.7297751903533936, acc 29.6875\n",
      "iteration 7854 loss 2.7309820652008057, acc 23.4375\n",
      "iteration 7855 loss 2.697819948196411, acc 20.3125\n",
      "iteration 7856 loss 2.6966922283172607, acc 17.1875\n",
      "iteration 7857 loss 2.9181392192840576, acc 9.375\n",
      "iteration 7858 loss 2.8258776664733887, acc 14.0625\n",
      "iteration 7859 loss 2.624985694885254, acc 25.0\n",
      "iteration 7860 loss 2.7891130447387695, acc 26.5625\n",
      "iteration 7861 loss 2.8555049896240234, acc 15.625\n",
      "iteration 7862 loss 2.5708041191101074, acc 25.0\n",
      "iteration 7863 loss 2.7251675128936768, acc 9.375\n",
      "iteration 7864 loss 2.723206043243408, acc 25.0\n",
      "iteration 7865 loss 2.3990213871002197, acc 32.8125\n",
      "iteration 7866 loss 2.655886173248291, acc 23.4375\n",
      "iteration 7867 loss 2.7380785942077637, acc 12.5\n",
      "iteration 7868 loss 2.697803258895874, acc 15.625\n",
      "iteration 7869 loss 2.5294599533081055, acc 21.875\n",
      "iteration 7870 loss 2.8127505779266357, acc 23.4375\n",
      "iteration 7871 loss 2.679837226867676, acc 20.3125\n",
      "iteration 7872 loss 2.5252816677093506, acc 28.125\n",
      "iteration 7873 loss 2.6544601917266846, acc 20.3125\n",
      "iteration 7874 loss 2.597034454345703, acc 29.6875\n",
      "iteration 7875 loss 2.6743831634521484, acc 20.3125\n",
      "iteration 7876 loss 2.790525436401367, acc 17.1875\n",
      "iteration 7877 loss 2.7862985134124756, acc 18.75\n",
      "iteration 7878 loss 2.5743651390075684, acc 23.4375\n",
      "iteration 7879 loss 2.640209913253784, acc 23.4375\n",
      "iteration 7880 loss 2.428236484527588, acc 26.5625\n",
      "iteration 7881 loss 2.7202072143554688, acc 18.75\n",
      "iteration 7882 loss 2.584440231323242, acc 29.6875\n",
      "iteration 7883 loss 2.5261342525482178, acc 23.4375\n",
      "iteration 7884 loss 2.587594509124756, acc 20.3125\n",
      "iteration 7885 loss 2.6277990341186523, acc 23.4375\n",
      "iteration 7886 loss 2.771106004714966, acc 17.1875\n",
      "iteration 7887 loss 2.539283514022827, acc 29.6875\n",
      "iteration 7888 loss 2.699104070663452, acc 20.3125\n",
      "iteration 7889 loss 2.7340469360351562, acc 20.3125\n",
      "iteration 7890 loss 2.905323028564453, acc 10.9375\n",
      "iteration 7891 loss 2.7087202072143555, acc 21.875\n",
      "iteration 7892 loss 2.577434539794922, acc 26.5625\n",
      "iteration 7893 loss 2.6647579669952393, acc 21.875\n",
      "iteration 7894 loss 2.688971519470215, acc 26.5625\n",
      "iteration 7895 loss 2.7551231384277344, acc 18.75\n",
      "iteration 7896 loss 2.740769386291504, acc 14.0625\n",
      "iteration 7897 loss 2.685694932937622, acc 17.1875\n",
      "iteration 7898 loss 2.4900851249694824, acc 23.4375\n",
      "iteration 7899 loss 2.6647307872772217, acc 15.625\n",
      "iteration 7900 loss 2.665656089782715, acc 26.5625\n",
      "iteration 7901 loss 2.589892625808716, acc 23.4375\n",
      "iteration 7902 loss 2.7330658435821533, acc 20.3125\n",
      "iteration 7903 loss 2.847993850708008, acc 20.3125\n",
      "iteration 7904 loss 2.7376725673675537, acc 23.4375\n",
      "iteration 7905 loss 2.611786365509033, acc 28.125\n",
      "iteration 7906 loss 2.798557996749878, acc 23.4375\n",
      "iteration 7907 loss 2.6890037059783936, acc 17.1875\n",
      "iteration 7908 loss 2.671278238296509, acc 23.4375\n",
      "iteration 7909 loss 2.651629686355591, acc 20.3125\n",
      "iteration 7910 loss 2.749070405960083, acc 21.875\n",
      "iteration 7911 loss 2.6632184982299805, acc 32.8125\n",
      "iteration 7912 loss 2.6045916080474854, acc 23.4375\n",
      "iteration 7913 loss 2.7535548210144043, acc 9.375\n",
      "iteration 7914 loss 2.6418654918670654, acc 21.875\n",
      "iteration 7915 loss 2.7998859882354736, acc 20.3125\n",
      "iteration 7916 loss 2.855023145675659, acc 9.375\n",
      "iteration 7917 loss 2.7602553367614746, acc 20.3125\n",
      "iteration 7918 loss 2.579204797744751, acc 21.875\n",
      "iteration 7919 loss 2.663975477218628, acc 20.3125\n",
      "iteration 7920 loss 2.5636038780212402, acc 23.4375\n",
      "iteration 7921 loss 2.6919708251953125, acc 21.875\n",
      "iteration 7922 loss 2.532992124557495, acc 18.75\n",
      "iteration 7923 loss 2.650611162185669, acc 26.5625\n",
      "iteration 7924 loss 2.6857080459594727, acc 25.0\n",
      "iteration 7925 loss 2.726893186569214, acc 21.875\n",
      "iteration 7926 loss 2.5063083171844482, acc 28.125\n",
      "iteration 7927 loss 2.539766311645508, acc 28.125\n",
      "iteration 7928 loss 2.666351318359375, acc 23.4375\n",
      "iteration 7929 loss 2.714489698410034, acc 21.875\n",
      "iteration 7930 loss 2.752683162689209, acc 17.1875\n",
      "iteration 7931 loss 2.859497547149658, acc 12.5\n",
      "iteration 7932 loss 2.642427682876587, acc 21.875\n",
      "iteration 7933 loss 2.6961700916290283, acc 18.75\n",
      "iteration 7934 loss 2.7440783977508545, acc 23.4375\n",
      "iteration 7935 loss 2.676231622695923, acc 21.875\n",
      "iteration 7936 loss 2.592872142791748, acc 21.875\n",
      "iteration 7937 loss 2.7068169116973877, acc 18.75\n",
      "iteration 7938 loss 2.632692337036133, acc 17.1875\n",
      "iteration 7939 loss 2.635409116744995, acc 26.5625\n",
      "iteration 7940 loss 2.672667980194092, acc 23.4375\n",
      "iteration 7941 loss 2.7820963859558105, acc 25.0\n",
      "iteration 7942 loss 2.688997745513916, acc 21.875\n",
      "iteration 7943 loss 2.470768451690674, acc 31.25\n",
      "iteration 7944 loss 2.4855270385742188, acc 26.5625\n",
      "iteration 7945 loss 2.8045389652252197, acc 21.875\n",
      "iteration 7946 loss 2.648221731185913, acc 18.75\n",
      "iteration 7947 loss 2.5308964252471924, acc 29.6875\n",
      "iteration 7948 loss 2.587923049926758, acc 25.0\n",
      "iteration 7949 loss 2.5092172622680664, acc 29.6875\n",
      "iteration 7950 loss 2.864173412322998, acc 15.625\n",
      "iteration 7951 loss 2.5935354232788086, acc 23.4375\n",
      "iteration 7952 loss 2.822514057159424, acc 17.1875\n",
      "iteration 7953 loss 2.6500816345214844, acc 14.0625\n",
      "iteration 7954 loss 2.7153842449188232, acc 20.3125\n",
      "iteration 7955 loss 2.844165325164795, acc 17.1875\n",
      "iteration 7956 loss 2.8496086597442627, acc 18.75\n",
      "iteration 7957 loss 2.950279712677002, acc 20.3125\n",
      "iteration 7958 loss 2.8392527103424072, acc 17.1875\n",
      "iteration 7959 loss 2.671999454498291, acc 17.1875\n",
      "iteration 7960 loss 2.6631886959075928, acc 21.875\n",
      "iteration 7961 loss 2.746490478515625, acc 23.4375\n",
      "iteration 7962 loss 2.6829519271850586, acc 17.1875\n",
      "iteration 7963 loss 2.790925979614258, acc 21.875\n",
      "iteration 7964 loss 2.4275286197662354, acc 32.8125\n",
      "iteration 7965 loss 2.5048019886016846, acc 21.875\n",
      "iteration 7966 loss 2.870781660079956, acc 18.75\n",
      "iteration 7967 loss 2.6439998149871826, acc 28.125\n",
      "iteration 7968 loss 2.5781731605529785, acc 18.75\n",
      "iteration 7969 loss 2.726182222366333, acc 17.1875\n",
      "iteration 7970 loss 2.8477542400360107, acc 15.625\n",
      "iteration 7971 loss 2.7755088806152344, acc 18.75\n",
      "iteration 7972 loss 2.6858222484588623, acc 28.125\n",
      "iteration 7973 loss 2.6757752895355225, acc 21.875\n",
      "iteration 7974 loss 2.874213218688965, acc 18.75\n",
      "iteration 7975 loss 2.6062164306640625, acc 25.0\n",
      "iteration 7976 loss 2.657258987426758, acc 18.75\n",
      "iteration 7977 loss 2.6718084812164307, acc 15.625\n",
      "iteration 7978 loss 2.7906906604766846, acc 20.3125\n",
      "iteration 7979 loss 2.575122833251953, acc 21.875\n",
      "iteration 7980 loss 2.753462076187134, acc 12.5\n",
      "iteration 7981 loss 2.7721455097198486, acc 18.75\n",
      "iteration 7982 loss 2.489450216293335, acc 26.5625\n",
      "iteration 7983 loss 2.7745728492736816, acc 20.3125\n",
      "iteration 7984 loss 2.609112501144409, acc 18.75\n",
      "iteration 7985 loss 2.544020175933838, acc 26.5625\n",
      "iteration 7986 loss 2.814832925796509, acc 15.625\n",
      "iteration 7987 loss 2.784604072570801, acc 15.625\n",
      "iteration 7988 loss 2.6258788108825684, acc 26.5625\n",
      "iteration 7989 loss 2.7071027755737305, acc 15.625\n",
      "iteration 7990 loss 2.843414545059204, acc 10.9375\n",
      "iteration 7991 loss 2.6926286220550537, acc 17.1875\n",
      "iteration 7992 loss 2.4626965522766113, acc 29.6875\n",
      "iteration 7993 loss 2.3700878620147705, acc 31.25\n",
      "iteration 7994 loss 2.6308302879333496, acc 20.3125\n",
      "iteration 7995 loss 2.6939220428466797, acc 17.1875\n",
      "iteration 7996 loss 2.541682004928589, acc 20.3125\n",
      "iteration 7997 loss 2.55719256401062, acc 21.875\n",
      "iteration 7998 loss 2.796375274658203, acc 15.625\n",
      "iteration 7999 loss 2.691009283065796, acc 25.0\n",
      "iteration 8000 loss 2.746507167816162, acc 29.6875\n",
      "iteration 8001 loss 2.458524227142334, acc 29.6875\n",
      "iteration 8002 loss 2.498702049255371, acc 34.375\n",
      "iteration 8003 loss 2.9129390716552734, acc 17.1875\n",
      "iteration 8004 loss 2.8563215732574463, acc 20.3125\n",
      "iteration 8005 loss 2.732612133026123, acc 18.75\n",
      "iteration 8006 loss 2.6336159706115723, acc 26.5625\n",
      "iteration 8007 loss 2.687128782272339, acc 12.5\n",
      "iteration 8008 loss 2.690779685974121, acc 23.4375\n",
      "iteration 8009 loss 2.789921760559082, acc 12.5\n",
      "iteration 8010 loss 2.6476147174835205, acc 29.6875\n",
      "iteration 8011 loss 2.8041348457336426, acc 17.1875\n",
      "iteration 8012 loss 2.625138759613037, acc 20.3125\n",
      "iteration 8013 loss 2.6173996925354004, acc 23.4375\n",
      "iteration 8014 loss 2.742795944213867, acc 20.3125\n",
      "iteration 8015 loss 2.6147069931030273, acc 25.0\n",
      "iteration 8016 loss 2.7734293937683105, acc 20.3125\n",
      "iteration 8017 loss 2.608365058898926, acc 23.4375\n",
      "iteration 8018 loss 2.5477824211120605, acc 25.0\n",
      "iteration 8019 loss 2.697265625, acc 21.875\n",
      "iteration 8020 loss 2.5930161476135254, acc 29.6875\n",
      "iteration 8021 loss 2.6228299140930176, acc 26.5625\n",
      "iteration 8022 loss 2.4577279090881348, acc 29.6875\n",
      "iteration 8023 loss 2.684969186782837, acc 26.5625\n",
      "iteration 8024 loss 2.508573055267334, acc 23.4375\n",
      "iteration 8025 loss 2.840921401977539, acc 15.625\n",
      "iteration 8026 loss 2.8233084678649902, acc 12.5\n",
      "iteration 8027 loss 2.6797637939453125, acc 18.75\n",
      "iteration 8028 loss 2.631598949432373, acc 15.625\n",
      "iteration 8029 loss 2.999640703201294, acc 10.9375\n",
      "iteration 8030 loss 2.6075522899627686, acc 23.4375\n",
      "iteration 8031 loss 2.7356996536254883, acc 20.3125\n",
      "iteration 8032 loss 2.6685168743133545, acc 26.5625\n",
      "iteration 8033 loss 2.551290273666382, acc 26.5625\n",
      "iteration 8034 loss 2.5982022285461426, acc 21.875\n",
      "iteration 8035 loss 2.548109531402588, acc 29.6875\n",
      "iteration 8036 loss 2.686673402786255, acc 23.4375\n",
      "iteration 8037 loss 2.7318570613861084, acc 21.875\n",
      "iteration 8038 loss 2.588000535964966, acc 12.5\n",
      "iteration 8039 loss 2.677665948867798, acc 23.4375\n",
      "iteration 8040 loss 2.775216579437256, acc 21.875\n",
      "iteration 8041 loss 2.613142728805542, acc 28.125\n",
      "iteration 8042 loss 2.623782157897949, acc 21.875\n",
      "iteration 8043 loss 2.745404005050659, acc 21.875\n",
      "iteration 8044 loss 2.53401255607605, acc 31.25\n",
      "iteration 8045 loss 2.766472578048706, acc 20.3125\n",
      "iteration 8046 loss 2.610793113708496, acc 26.5625\n",
      "iteration 8047 loss 2.7346975803375244, acc 17.1875\n",
      "iteration 8048 loss 2.410939931869507, acc 35.9375\n",
      "iteration 8049 loss 2.7825872898101807, acc 15.625\n",
      "iteration 8050 loss 2.644340991973877, acc 32.8125\n",
      "iteration 8051 loss 2.4198901653289795, acc 35.9375\n",
      "iteration 8052 loss 2.9326319694519043, acc 20.3125\n",
      "iteration 8053 loss 2.6195907592773438, acc 23.4375\n",
      "iteration 8054 loss 2.4986233711242676, acc 28.125\n",
      "iteration 8055 loss 2.770503044128418, acc 15.625\n",
      "iteration 8056 loss 2.957247257232666, acc 23.4375\n",
      "iteration 8057 loss 2.7312662601470947, acc 25.0\n",
      "iteration 8058 loss 2.8346240520477295, acc 17.1875\n",
      "iteration 8059 loss 2.5486984252929688, acc 25.0\n",
      "iteration 8060 loss 2.7247471809387207, acc 21.875\n",
      "iteration 8061 loss 2.773233652114868, acc 20.3125\n",
      "iteration 8062 loss 2.654850959777832, acc 20.3125\n",
      "iteration 8063 loss 2.5628182888031006, acc 18.75\n",
      "iteration 8064 loss 2.6936452388763428, acc 23.4375\n",
      "iteration 8065 loss 2.711199998855591, acc 18.75\n",
      "iteration 8066 loss 2.6203055381774902, acc 23.4375\n",
      "iteration 8067 loss 2.7270171642303467, acc 21.875\n",
      "iteration 8068 loss 2.6823904514312744, acc 15.625\n",
      "iteration 8069 loss 2.8737974166870117, acc 21.875\n",
      "iteration 8070 loss 2.6978375911712646, acc 21.875\n",
      "iteration 8071 loss 2.5833487510681152, acc 25.0\n",
      "iteration 8072 loss 2.687983274459839, acc 15.625\n",
      "iteration 8073 loss 2.696712017059326, acc 25.0\n",
      "iteration 8074 loss 2.5770788192749023, acc 17.1875\n",
      "iteration 8075 loss 2.7688746452331543, acc 18.75\n",
      "iteration 8076 loss 3.035985231399536, acc 12.5\n",
      "iteration 8077 loss 2.6201534271240234, acc 25.0\n",
      "iteration 8078 loss 2.558382987976074, acc 21.875\n",
      "iteration 8079 loss 2.731074333190918, acc 26.5625\n",
      "iteration 8080 loss 2.8547449111938477, acc 20.3125\n",
      "iteration 8081 loss 2.8202340602874756, acc 21.875\n",
      "iteration 8082 loss 2.6141035556793213, acc 23.4375\n",
      "iteration 8083 loss 2.6455934047698975, acc 23.4375\n",
      "iteration 8084 loss 2.583730459213257, acc 20.3125\n",
      "iteration 8085 loss 2.613151788711548, acc 17.1875\n",
      "iteration 8086 loss 2.759531259536743, acc 17.1875\n",
      "iteration 8087 loss 2.6646220684051514, acc 25.0\n",
      "iteration 8088 loss 2.7477457523345947, acc 23.4375\n",
      "iteration 8089 loss 2.6847822666168213, acc 28.125\n",
      "iteration 8090 loss 2.888101100921631, acc 18.75\n",
      "iteration 8091 loss 2.7577733993530273, acc 21.875\n",
      "iteration 8092 loss 2.6125199794769287, acc 31.25\n",
      "iteration 8093 loss 2.8204262256622314, acc 20.3125\n",
      "iteration 8094 loss 2.7867801189422607, acc 9.375\n",
      "iteration 8095 loss 2.992664337158203, acc 17.1875\n",
      "iteration 8096 loss 2.6817216873168945, acc 23.4375\n",
      "iteration 8097 loss 2.590691089630127, acc 28.125\n",
      "iteration 8098 loss 2.530245542526245, acc 31.25\n",
      "iteration 8099 loss 2.8970248699188232, acc 14.0625\n",
      "iteration 8100 loss 2.631834030151367, acc 26.5625\n",
      "iteration 8101 loss 2.8396003246307373, acc 15.625\n",
      "iteration 8102 loss 2.7305586338043213, acc 17.1875\n",
      "iteration 8103 loss 2.7732596397399902, acc 21.875\n",
      "iteration 8104 loss 2.6751790046691895, acc 25.0\n",
      "iteration 8105 loss 2.549513339996338, acc 25.0\n",
      "iteration 8106 loss 2.8897087574005127, acc 10.9375\n",
      "iteration 8107 loss 2.8257274627685547, acc 14.0625\n",
      "iteration 8108 loss 2.8002498149871826, acc 17.1875\n",
      "iteration 8109 loss 2.9013772010803223, acc 14.0625\n",
      "iteration 8110 loss 2.6943490505218506, acc 20.3125\n",
      "iteration 8111 loss 2.584216356277466, acc 25.0\n",
      "iteration 8112 loss 2.6350057125091553, acc 20.3125\n",
      "iteration 8113 loss 2.8444225788116455, acc 15.625\n",
      "iteration 8114 loss 2.670112133026123, acc 21.875\n",
      "iteration 8115 loss 2.7496838569641113, acc 26.5625\n",
      "iteration 8116 loss 2.852393388748169, acc 18.75\n",
      "iteration 8117 loss 2.567763090133667, acc 28.125\n",
      "iteration 8118 loss 2.6981053352355957, acc 17.1875\n",
      "iteration 8119 loss 2.6605920791625977, acc 18.75\n",
      "iteration 8120 loss 2.747889518737793, acc 18.75\n",
      "iteration 8121 loss 2.436957597732544, acc 32.8125\n",
      "iteration 8122 loss 2.6229727268218994, acc 23.4375\n",
      "iteration 8123 loss 3.069920778274536, acc 14.0625\n",
      "iteration 8124 loss 2.558471918106079, acc 28.125\n",
      "iteration 8125 loss 2.5850930213928223, acc 26.5625\n",
      "iteration 8126 loss 2.808473825454712, acc 23.4375\n",
      "iteration 8127 loss 2.7895922660827637, acc 20.3125\n",
      "iteration 8128 loss 2.6224350929260254, acc 17.1875\n",
      "iteration 8129 loss 2.722229242324829, acc 25.0\n",
      "iteration 8130 loss 2.7552261352539062, acc 18.75\n",
      "iteration 8131 loss 2.8224897384643555, acc 14.0625\n",
      "iteration 8132 loss 2.674948215484619, acc 20.3125\n",
      "iteration 8133 loss 2.6341817378997803, acc 25.0\n",
      "iteration 8134 loss 2.659691333770752, acc 25.0\n",
      "iteration 8135 loss 2.4669132232666016, acc 31.25\n",
      "iteration 8136 loss 2.5510830879211426, acc 28.125\n",
      "iteration 8137 loss 2.7875888347625732, acc 23.4375\n",
      "iteration 8138 loss 2.7765040397644043, acc 20.3125\n",
      "iteration 8139 loss 2.7996997833251953, acc 23.4375\n",
      "iteration 8140 loss 2.790407180786133, acc 18.75\n",
      "iteration 8141 loss 2.6398446559906006, acc 20.3125\n",
      "iteration 8142 loss 2.6606292724609375, acc 23.4375\n",
      "iteration 8143 loss 2.52262020111084, acc 26.5625\n",
      "iteration 8144 loss 2.5838382244110107, acc 26.5625\n",
      "iteration 8145 loss 2.827514410018921, acc 17.1875\n",
      "iteration 8146 loss 2.839738368988037, acc 14.0625\n",
      "iteration 8147 loss 2.6276395320892334, acc 25.0\n",
      "iteration 8148 loss 2.555159330368042, acc 23.4375\n",
      "iteration 8149 loss 2.7100424766540527, acc 17.1875\n",
      "iteration 8150 loss 2.578042984008789, acc 26.5625\n",
      "iteration 8151 loss 2.9159960746765137, acc 18.75\n",
      "iteration 8152 loss 2.68825626373291, acc 14.0625\n",
      "iteration 8153 loss 2.619889974594116, acc 29.6875\n",
      "iteration 8154 loss 2.7603461742401123, acc 20.3125\n",
      "iteration 8155 loss 2.7990663051605225, acc 20.3125\n",
      "iteration 8156 loss 2.7055678367614746, acc 14.0625\n",
      "iteration 8157 loss 2.613298177719116, acc 29.6875\n",
      "iteration 8158 loss 2.688324213027954, acc 23.4375\n",
      "iteration 8159 loss 2.6643826961517334, acc 25.0\n",
      "iteration 8160 loss 2.562603712081909, acc 25.0\n",
      "iteration 8161 loss 2.6002819538116455, acc 28.125\n",
      "iteration 8162 loss 2.6572396755218506, acc 26.5625\n",
      "iteration 8163 loss 2.7108187675476074, acc 14.0625\n",
      "iteration 8164 loss 2.8186607360839844, acc 23.4375\n",
      "iteration 8165 loss 2.66920804977417, acc 23.4375\n",
      "iteration 8166 loss 2.5665993690490723, acc 32.8125\n",
      "iteration 8167 loss 2.634298086166382, acc 25.0\n",
      "iteration 8168 loss 2.7023682594299316, acc 17.1875\n",
      "iteration 8169 loss 2.8056490421295166, acc 17.1875\n",
      "iteration 8170 loss 2.724893569946289, acc 18.75\n",
      "iteration 8171 loss 2.722837209701538, acc 14.0625\n",
      "iteration 8172 loss 2.6152613162994385, acc 20.3125\n",
      "iteration 8173 loss 2.6221776008605957, acc 23.4375\n",
      "iteration 8174 loss 2.726806640625, acc 17.1875\n",
      "iteration 8175 loss 2.8261072635650635, acc 23.4375\n",
      "iteration 8176 loss 2.8671317100524902, acc 15.625\n",
      "iteration 8177 loss 2.7240121364593506, acc 18.75\n",
      "iteration 8178 loss 2.416567802429199, acc 28.125\n",
      "iteration 8179 loss 2.6536314487457275, acc 20.3125\n",
      "iteration 8180 loss 2.634340524673462, acc 17.1875\n",
      "iteration 8181 loss 2.7379794120788574, acc 20.3125\n",
      "iteration 8182 loss 2.5170845985412598, acc 21.875\n",
      "iteration 8183 loss 2.8101158142089844, acc 10.9375\n",
      "iteration 8184 loss 2.855012893676758, acc 23.4375\n",
      "iteration 8185 loss 2.751955270767212, acc 23.4375\n",
      "iteration 8186 loss 2.826206922531128, acc 15.625\n",
      "iteration 8187 loss 2.7254862785339355, acc 15.625\n",
      "iteration 8188 loss 2.6280148029327393, acc 12.5\n",
      "iteration 8189 loss 2.5582592487335205, acc 26.5625\n",
      "iteration 8190 loss 2.640671491622925, acc 18.75\n",
      "iteration 8191 loss 2.615922689437866, acc 26.5625\n",
      "iteration 8192 loss 2.5836730003356934, acc 28.125\n",
      "iteration 8193 loss 2.650343418121338, acc 21.875\n",
      "iteration 8194 loss 2.612977981567383, acc 32.8125\n",
      "iteration 8195 loss 2.612182140350342, acc 28.125\n",
      "iteration 8196 loss 2.5730161666870117, acc 25.0\n",
      "iteration 8197 loss 2.539292335510254, acc 23.4375\n",
      "iteration 8198 loss 2.755258321762085, acc 23.4375\n",
      "iteration 8199 loss 2.8450229167938232, acc 20.3125\n",
      "iteration 8200 loss 2.6924920082092285, acc 20.3125\n",
      "iteration 8201 loss 2.4929680824279785, acc 18.75\n",
      "iteration 8202 loss 2.5727314949035645, acc 25.0\n",
      "iteration 8203 loss 2.3864684104919434, acc 35.9375\n",
      "iteration 8204 loss 2.528973340988159, acc 26.5625\n",
      "iteration 8205 loss 2.769116163253784, acc 21.875\n",
      "iteration 8206 loss 2.7924587726593018, acc 12.5\n",
      "iteration 8207 loss 2.6714928150177, acc 25.0\n",
      "iteration 8208 loss 2.83345627784729, acc 17.1875\n",
      "iteration 8209 loss 2.581807851791382, acc 25.0\n",
      "iteration 8210 loss 2.6776673793792725, acc 21.875\n",
      "iteration 8211 loss 2.642287492752075, acc 26.5625\n",
      "iteration 8212 loss 2.7511038780212402, acc 17.1875\n",
      "iteration 8213 loss 2.8075509071350098, acc 18.75\n",
      "iteration 8214 loss 2.602595806121826, acc 15.625\n",
      "iteration 8215 loss 2.686817169189453, acc 28.125\n",
      "iteration 8216 loss 2.8607914447784424, acc 12.5\n",
      "iteration 8217 loss 2.5965161323547363, acc 28.125\n",
      "iteration 8218 loss 2.70371150970459, acc 21.875\n",
      "iteration 8219 loss 2.7371537685394287, acc 25.0\n",
      "iteration 8220 loss 2.575056314468384, acc 23.4375\n",
      "iteration 8221 loss 2.703608512878418, acc 25.0\n",
      "iteration 8222 loss 2.7765097618103027, acc 14.0625\n",
      "iteration 8223 loss 2.8414485454559326, acc 12.5\n",
      "iteration 8224 loss 2.6169991493225098, acc 23.4375\n",
      "iteration 8225 loss 2.924952745437622, acc 12.5\n",
      "iteration 8226 loss 2.570202112197876, acc 29.6875\n",
      "iteration 8227 loss 2.605469226837158, acc 28.125\n",
      "iteration 8228 loss 2.8092174530029297, acc 14.0625\n",
      "iteration 8229 loss 2.676257371902466, acc 20.3125\n",
      "iteration 8230 loss 2.6900904178619385, acc 20.3125\n",
      "iteration 8231 loss 2.6215741634368896, acc 17.1875\n",
      "iteration 8232 loss 2.5793516635894775, acc 26.5625\n",
      "iteration 8233 loss 2.481205701828003, acc 23.4375\n",
      "iteration 8234 loss 2.646700859069824, acc 23.4375\n",
      "iteration 8235 loss 2.956284999847412, acc 10.9375\n",
      "iteration 8236 loss 2.758826732635498, acc 20.3125\n",
      "iteration 8237 loss 2.5073037147521973, acc 34.375\n",
      "iteration 8238 loss 2.8857994079589844, acc 15.625\n",
      "iteration 8239 loss 2.4860808849334717, acc 29.6875\n",
      "iteration 8240 loss 2.6673974990844727, acc 20.3125\n",
      "iteration 8241 loss 2.6237244606018066, acc 26.5625\n",
      "iteration 8242 loss 2.8076751232147217, acc 15.625\n",
      "iteration 8243 loss 2.635646104812622, acc 18.75\n",
      "iteration 8244 loss 2.892057418823242, acc 12.5\n",
      "iteration 8245 loss 2.827488899230957, acc 17.1875\n",
      "iteration 8246 loss 2.779963254928589, acc 12.5\n",
      "iteration 8247 loss 2.6856465339660645, acc 17.1875\n",
      "iteration 8248 loss 2.490312337875366, acc 25.0\n",
      "iteration 8249 loss 2.601626396179199, acc 21.875\n",
      "iteration 8250 loss 2.5488576889038086, acc 25.0\n",
      "iteration 8251 loss 2.5007858276367188, acc 31.25\n",
      "iteration 8252 loss 2.680102825164795, acc 18.75\n",
      "iteration 8253 loss 2.8884241580963135, acc 18.75\n",
      "iteration 8254 loss 2.7657203674316406, acc 15.625\n",
      "iteration 8255 loss 2.5402815341949463, acc 26.5625\n",
      "iteration 8256 loss 2.579326868057251, acc 21.875\n",
      "iteration 8257 loss 2.5352730751037598, acc 25.0\n",
      "iteration 8258 loss 2.6033108234405518, acc 23.4375\n",
      "iteration 8259 loss 2.7436413764953613, acc 18.75\n",
      "iteration 8260 loss 2.7883970737457275, acc 14.0625\n",
      "iteration 8261 loss 2.9519474506378174, acc 9.375\n",
      "iteration 8262 loss 2.687237024307251, acc 21.875\n",
      "iteration 8263 loss 2.624091148376465, acc 29.6875\n",
      "iteration 8264 loss 2.6610982418060303, acc 25.0\n",
      "iteration 8265 loss 2.666869878768921, acc 25.0\n",
      "iteration 8266 loss 2.583134889602661, acc 18.75\n",
      "iteration 8267 loss 2.7468130588531494, acc 26.5625\n",
      "iteration 8268 loss 2.634491443634033, acc 21.875\n",
      "iteration 8269 loss 2.589937448501587, acc 29.6875\n",
      "iteration 8270 loss 2.590759515762329, acc 29.6875\n",
      "iteration 8271 loss 2.89203143119812, acc 7.8125\n",
      "iteration 8272 loss 2.446826696395874, acc 29.6875\n",
      "iteration 8273 loss 2.6976675987243652, acc 15.625\n",
      "iteration 8274 loss 2.582831382751465, acc 17.1875\n",
      "iteration 8275 loss 2.9199235439300537, acc 12.5\n",
      "iteration 8276 loss 2.5695548057556152, acc 28.125\n",
      "iteration 8277 loss 2.515561580657959, acc 23.4375\n",
      "iteration 8278 loss 2.8121891021728516, acc 18.75\n",
      "iteration 8279 loss 2.6578903198242188, acc 25.0\n",
      "iteration 8280 loss 2.8134729862213135, acc 12.5\n",
      "iteration 8281 loss 2.937747001647949, acc 17.1875\n",
      "iteration 8282 loss 2.584656000137329, acc 21.875\n",
      "iteration 8283 loss 2.5174427032470703, acc 31.25\n",
      "iteration 8284 loss 2.58901309967041, acc 20.3125\n",
      "iteration 8285 loss 2.681837320327759, acc 21.875\n",
      "iteration 8286 loss 2.6531333923339844, acc 25.0\n",
      "iteration 8287 loss 2.870697498321533, acc 20.3125\n",
      "iteration 8288 loss 2.347320318222046, acc 31.25\n",
      "iteration 8289 loss 2.6396942138671875, acc 12.5\n",
      "iteration 8290 loss 2.4982552528381348, acc 23.4375\n",
      "iteration 8291 loss 2.5336854457855225, acc 21.875\n",
      "iteration 8292 loss 2.6643059253692627, acc 17.1875\n",
      "iteration 8293 loss 2.7465338706970215, acc 23.4375\n",
      "iteration 8294 loss 2.5191471576690674, acc 25.0\n",
      "iteration 8295 loss 2.6930158138275146, acc 23.4375\n",
      "iteration 8296 loss 2.700888156890869, acc 25.0\n",
      "iteration 8297 loss 2.7128608226776123, acc 15.625\n",
      "iteration 8298 loss 2.6045520305633545, acc 23.4375\n",
      "iteration 8299 loss 2.6707234382629395, acc 23.4375\n",
      "iteration 8300 loss 2.613098621368408, acc 29.6875\n",
      "iteration 8301 loss 2.490431070327759, acc 28.125\n",
      "iteration 8302 loss 2.827329158782959, acc 21.875\n",
      "iteration 8303 loss 2.955141544342041, acc 17.1875\n",
      "iteration 8304 loss 2.843644380569458, acc 23.4375\n",
      "iteration 8305 loss 2.6800694465637207, acc 21.875\n",
      "iteration 8306 loss 2.5502822399139404, acc 25.0\n",
      "iteration 8307 loss 2.6227967739105225, acc 26.5625\n",
      "iteration 8308 loss 2.6376798152923584, acc 20.3125\n",
      "iteration 8309 loss 2.6036155223846436, acc 25.0\n",
      "iteration 8310 loss 2.692202568054199, acc 25.0\n",
      "iteration 8311 loss 2.7766411304473877, acc 26.5625\n",
      "iteration 8312 loss 2.5270063877105713, acc 26.5625\n",
      "iteration 8313 loss 2.690155506134033, acc 21.875\n",
      "iteration 8314 loss 2.7247631549835205, acc 17.1875\n",
      "iteration 8315 loss 2.5510284900665283, acc 26.5625\n",
      "iteration 8316 loss 2.63364315032959, acc 25.0\n",
      "iteration 8317 loss 2.5342795848846436, acc 29.6875\n",
      "iteration 8318 loss 2.6680514812469482, acc 17.1875\n",
      "iteration 8319 loss 2.702691078186035, acc 21.875\n",
      "iteration 8320 loss 2.942913770675659, acc 20.3125\n",
      "iteration 8321 loss 2.884204387664795, acc 12.5\n",
      "iteration 8322 loss 2.8914289474487305, acc 18.75\n",
      "iteration 8323 loss 2.6015615463256836, acc 21.875\n",
      "iteration 8324 loss 2.670516014099121, acc 17.1875\n",
      "iteration 8325 loss 2.5707221031188965, acc 28.125\n",
      "iteration 8326 loss 2.7110846042633057, acc 15.625\n",
      "iteration 8327 loss 2.866269826889038, acc 15.625\n",
      "iteration 8328 loss 2.5599324703216553, acc 18.75\n",
      "iteration 8329 loss 2.7420852184295654, acc 18.75\n",
      "iteration 8330 loss 2.523159980773926, acc 31.25\n",
      "iteration 8331 loss 2.601205825805664, acc 25.0\n",
      "iteration 8332 loss 2.539194345474243, acc 28.125\n",
      "iteration 8333 loss 2.625826120376587, acc 29.6875\n",
      "iteration 8334 loss 2.767305850982666, acc 15.625\n",
      "iteration 8335 loss 2.681349277496338, acc 23.4375\n",
      "iteration 8336 loss 2.7714850902557373, acc 20.3125\n",
      "iteration 8337 loss 2.903585195541382, acc 14.0625\n",
      "iteration 8338 loss 2.783129930496216, acc 14.0625\n",
      "iteration 8339 loss 2.868067979812622, acc 15.625\n",
      "iteration 8340 loss 2.5097649097442627, acc 34.375\n",
      "iteration 8341 loss 2.62165904045105, acc 23.4375\n",
      "iteration 8342 loss 2.517179489135742, acc 25.0\n",
      "iteration 8343 loss 2.748368501663208, acc 29.6875\n",
      "iteration 8344 loss 2.6257243156433105, acc 15.625\n",
      "iteration 8345 loss 2.8045361042022705, acc 20.3125\n",
      "iteration 8346 loss 2.480652332305908, acc 23.4375\n",
      "iteration 8347 loss 2.7341010570526123, acc 17.1875\n",
      "iteration 8348 loss 2.6341359615325928, acc 14.0625\n",
      "iteration 8349 loss 2.7672011852264404, acc 14.0625\n",
      "iteration 8350 loss 2.6440951824188232, acc 15.625\n",
      "iteration 8351 loss 2.819286823272705, acc 12.5\n",
      "iteration 8352 loss 2.8633992671966553, acc 17.1875\n",
      "iteration 8353 loss 2.512178421020508, acc 25.0\n",
      "iteration 8354 loss 2.5814318656921387, acc 25.0\n",
      "iteration 8355 loss 2.6571857929229736, acc 15.625\n",
      "iteration 8356 loss 2.5943174362182617, acc 23.4375\n",
      "iteration 8357 loss 2.576357126235962, acc 34.375\n",
      "iteration 8358 loss 2.539663791656494, acc 29.6875\n",
      "iteration 8359 loss 2.760948896408081, acc 21.875\n",
      "iteration 8360 loss 2.6594669818878174, acc 17.1875\n",
      "iteration 8361 loss 2.685007095336914, acc 20.3125\n",
      "iteration 8362 loss 2.5822808742523193, acc 15.625\n",
      "iteration 8363 loss 2.658191442489624, acc 25.0\n",
      "iteration 8364 loss 2.600846529006958, acc 25.0\n",
      "iteration 8365 loss 2.7646923065185547, acc 18.75\n",
      "iteration 8366 loss 2.6690011024475098, acc 21.875\n",
      "iteration 8367 loss 2.6187326908111572, acc 20.3125\n",
      "iteration 8368 loss 2.62496280670166, acc 20.3125\n",
      "iteration 8369 loss 2.5164244174957275, acc 28.125\n",
      "iteration 8370 loss 2.656416654586792, acc 21.875\n",
      "iteration 8371 loss 2.769756555557251, acc 17.1875\n",
      "iteration 8372 loss 2.6474881172180176, acc 10.9375\n",
      "iteration 8373 loss 2.8707807064056396, acc 21.875\n",
      "iteration 8374 loss 2.578348159790039, acc 20.3125\n",
      "iteration 8375 loss 2.7500407695770264, acc 17.1875\n",
      "iteration 8376 loss 2.7155492305755615, acc 20.3125\n",
      "iteration 8377 loss 2.6528537273406982, acc 23.4375\n",
      "iteration 8378 loss 2.698387861251831, acc 18.75\n",
      "iteration 8379 loss 2.8074300289154053, acc 17.1875\n",
      "iteration 8380 loss 2.4086687564849854, acc 29.6875\n",
      "iteration 8381 loss 2.7299304008483887, acc 20.3125\n",
      "iteration 8382 loss 2.618086099624634, acc 28.125\n",
      "iteration 8383 loss 2.893817186355591, acc 23.4375\n",
      "iteration 8384 loss 2.823685884475708, acc 21.875\n",
      "iteration 8385 loss 2.6576740741729736, acc 23.4375\n",
      "iteration 8386 loss 2.528623104095459, acc 26.5625\n",
      "iteration 8387 loss 2.6440682411193848, acc 18.75\n",
      "iteration 8388 loss 2.662173271179199, acc 28.125\n",
      "iteration 8389 loss 2.623231887817383, acc 23.4375\n",
      "iteration 8390 loss 2.6939480304718018, acc 20.3125\n",
      "iteration 8391 loss 2.6494176387786865, acc 21.875\n",
      "iteration 8392 loss 2.470930576324463, acc 28.125\n",
      "iteration 8393 loss 2.4917027950286865, acc 31.25\n",
      "iteration 8394 loss 2.6745569705963135, acc 25.0\n",
      "iteration 8395 loss 2.993180990219116, acc 17.1875\n",
      "iteration 8396 loss 2.5822913646698, acc 26.5625\n",
      "iteration 8397 loss 2.5681612491607666, acc 25.0\n",
      "iteration 8398 loss 2.9417755603790283, acc 9.375\n",
      "iteration 8399 loss 2.489689826965332, acc 28.125\n",
      "iteration 8400 loss 2.9880850315093994, acc 14.0625\n",
      "iteration 8401 loss 2.5614004135131836, acc 23.4375\n",
      "iteration 8402 loss 2.664494276046753, acc 25.0\n",
      "iteration 8403 loss 2.6250550746917725, acc 21.875\n",
      "iteration 8404 loss 2.629286527633667, acc 23.4375\n",
      "iteration 8405 loss 2.902905225753784, acc 15.625\n",
      "iteration 8406 loss 2.7775306701660156, acc 12.5\n",
      "iteration 8407 loss 2.588000535964966, acc 25.0\n",
      "iteration 8408 loss 2.682204246520996, acc 18.75\n",
      "iteration 8409 loss 2.4092395305633545, acc 28.125\n",
      "iteration 8410 loss 2.722677707672119, acc 17.1875\n",
      "iteration 8411 loss 2.7909798622131348, acc 15.625\n",
      "iteration 8412 loss 2.770447254180908, acc 23.4375\n",
      "iteration 8413 loss 3.0195043087005615, acc 18.75\n",
      "iteration 8414 loss 2.7392890453338623, acc 23.4375\n",
      "iteration 8415 loss 2.6797056198120117, acc 21.875\n",
      "iteration 8416 loss 2.5301294326782227, acc 34.375\n",
      "iteration 8417 loss 2.873289108276367, acc 12.5\n",
      "iteration 8418 loss 2.6505208015441895, acc 23.4375\n",
      "iteration 8419 loss 2.842728614807129, acc 14.0625\n",
      "iteration 8420 loss 2.7858190536499023, acc 17.1875\n",
      "iteration 8421 loss 2.4244747161865234, acc 29.6875\n",
      "iteration 8422 loss 2.6564698219299316, acc 17.1875\n",
      "iteration 8423 loss 2.634605646133423, acc 18.75\n",
      "iteration 8424 loss 2.8012492656707764, acc 18.75\n",
      "iteration 8425 loss 2.6793148517608643, acc 25.0\n",
      "iteration 8426 loss 2.9930315017700195, acc 14.0625\n",
      "iteration 8427 loss 2.5097758769989014, acc 23.4375\n",
      "iteration 8428 loss 2.5051088333129883, acc 20.3125\n",
      "iteration 8429 loss 2.6688148975372314, acc 18.75\n",
      "iteration 8430 loss 2.5381388664245605, acc 28.125\n",
      "iteration 8431 loss 2.6460530757904053, acc 26.5625\n",
      "iteration 8432 loss 2.483048439025879, acc 28.125\n",
      "iteration 8433 loss 2.4900856018066406, acc 23.4375\n",
      "iteration 8434 loss 2.9030818939208984, acc 17.1875\n",
      "iteration 8435 loss 2.580662727355957, acc 32.8125\n",
      "iteration 8436 loss 2.6973955631256104, acc 29.6875\n",
      "iteration 8437 loss 2.740673780441284, acc 20.3125\n",
      "iteration 8438 loss 2.7331061363220215, acc 18.75\n",
      "iteration 8439 loss 2.6069889068603516, acc 28.125\n",
      "iteration 8440 loss 2.5908825397491455, acc 21.875\n",
      "iteration 8441 loss 2.778423547744751, acc 18.75\n",
      "iteration 8442 loss 2.669839859008789, acc 18.75\n",
      "iteration 8443 loss 2.533118724822998, acc 23.4375\n",
      "iteration 8444 loss 2.5079147815704346, acc 31.25\n",
      "iteration 8445 loss 2.682054281234741, acc 17.1875\n",
      "iteration 8446 loss 2.6140635013580322, acc 32.8125\n",
      "iteration 8447 loss 2.696683645248413, acc 21.875\n",
      "iteration 8448 loss 2.6544110774993896, acc 18.75\n",
      "iteration 8449 loss 2.514232635498047, acc 35.9375\n",
      "iteration 8450 loss 2.7450902462005615, acc 20.3125\n",
      "iteration 8451 loss 2.584378480911255, acc 28.125\n",
      "iteration 8452 loss 2.8884429931640625, acc 14.0625\n",
      "iteration 8453 loss 2.7750422954559326, acc 17.1875\n",
      "iteration 8454 loss 3.014556884765625, acc 12.5\n",
      "iteration 8455 loss 2.48622465133667, acc 26.5625\n",
      "iteration 8456 loss 2.829568386077881, acc 14.0625\n",
      "iteration 8457 loss 2.612034559249878, acc 21.875\n",
      "iteration 8458 loss 2.718564987182617, acc 21.875\n",
      "iteration 8459 loss 2.6930058002471924, acc 15.625\n",
      "iteration 8460 loss 2.6483068466186523, acc 20.3125\n",
      "iteration 8461 loss 2.8024635314941406, acc 20.3125\n",
      "iteration 8462 loss 2.74196457862854, acc 15.625\n",
      "iteration 8463 loss 2.7135283946990967, acc 20.3125\n",
      "iteration 8464 loss 2.670901298522949, acc 26.5625\n",
      "iteration 8465 loss 2.642183303833008, acc 26.5625\n",
      "iteration 8466 loss 2.7014896869659424, acc 28.125\n",
      "iteration 8467 loss 2.593620538711548, acc 25.0\n",
      "iteration 8468 loss 2.9994969367980957, acc 12.5\n",
      "iteration 8469 loss 2.633183717727661, acc 25.0\n",
      "iteration 8470 loss 2.5244460105895996, acc 20.3125\n",
      "iteration 8471 loss 2.511521339416504, acc 20.3125\n",
      "iteration 8472 loss 2.6773061752319336, acc 9.375\n",
      "iteration 8473 loss 2.801238775253296, acc 15.625\n",
      "iteration 8474 loss 2.7091422080993652, acc 21.875\n",
      "iteration 8475 loss 2.5850119590759277, acc 23.4375\n",
      "iteration 8476 loss 2.7079029083251953, acc 18.75\n",
      "iteration 8477 loss 2.7618093490600586, acc 17.1875\n",
      "iteration 8478 loss 2.6435914039611816, acc 26.5625\n",
      "iteration 8479 loss 2.653028726577759, acc 26.5625\n",
      "iteration 8480 loss 2.5545263290405273, acc 28.125\n",
      "iteration 8481 loss 2.546616792678833, acc 29.6875\n",
      "iteration 8482 loss 2.546111583709717, acc 26.5625\n",
      "iteration 8483 loss 2.7713429927825928, acc 17.1875\n",
      "iteration 8484 loss 2.5292649269104004, acc 26.5625\n",
      "iteration 8485 loss 2.977672815322876, acc 9.375\n",
      "iteration 8486 loss 2.630230665206909, acc 25.0\n",
      "iteration 8487 loss 2.611908435821533, acc 26.5625\n",
      "iteration 8488 loss 2.819946765899658, acc 15.625\n",
      "iteration 8489 loss 2.466541051864624, acc 23.4375\n",
      "iteration 8490 loss 2.650887966156006, acc 26.5625\n",
      "iteration 8491 loss 2.7226715087890625, acc 17.1875\n",
      "iteration 8492 loss 2.732160806655884, acc 20.3125\n",
      "iteration 8493 loss 2.5757482051849365, acc 35.9375\n",
      "iteration 8494 loss 2.5517830848693848, acc 21.875\n",
      "iteration 8495 loss 2.579907178878784, acc 25.0\n",
      "iteration 8496 loss 2.6554253101348877, acc 26.5625\n",
      "iteration 8497 loss 2.7827346324920654, acc 20.3125\n",
      "iteration 8498 loss 2.867082118988037, acc 12.5\n",
      "iteration 8499 loss 2.586674690246582, acc 10.9375\n",
      "iteration 8500 loss 2.7129576206207275, acc 21.875\n",
      "iteration 8501 loss 2.6664681434631348, acc 23.4375\n",
      "iteration 8502 loss 2.6857335567474365, acc 21.875\n",
      "iteration 8503 loss 2.575829267501831, acc 21.875\n",
      "iteration 8504 loss 2.6359078884124756, acc 25.0\n",
      "iteration 8505 loss 2.604846715927124, acc 25.0\n",
      "iteration 8506 loss 2.6741998195648193, acc 21.875\n",
      "iteration 8507 loss 2.490765333175659, acc 28.125\n",
      "iteration 8508 loss 2.5759332180023193, acc 23.4375\n",
      "iteration 8509 loss 2.6468453407287598, acc 23.4375\n",
      "iteration 8510 loss 2.7819457054138184, acc 20.3125\n",
      "iteration 8511 loss 2.922760009765625, acc 15.625\n",
      "iteration 8512 loss 2.6232821941375732, acc 23.4375\n",
      "iteration 8513 loss 2.6146984100341797, acc 32.8125\n",
      "iteration 8514 loss 2.5883097648620605, acc 26.5625\n",
      "iteration 8515 loss 2.7790632247924805, acc 18.75\n",
      "iteration 8516 loss 2.881422519683838, acc 15.625\n",
      "iteration 8517 loss 2.841548204421997, acc 21.875\n",
      "iteration 8518 loss 2.5695528984069824, acc 23.4375\n",
      "iteration 8519 loss 2.780104637145996, acc 15.625\n",
      "iteration 8520 loss 2.6803791522979736, acc 18.75\n",
      "iteration 8521 loss 2.5813698768615723, acc 21.875\n",
      "iteration 8522 loss 2.50762677192688, acc 28.125\n",
      "iteration 8523 loss 2.6876676082611084, acc 23.4375\n",
      "iteration 8524 loss 2.6683108806610107, acc 26.5625\n",
      "iteration 8525 loss 2.6095778942108154, acc 20.3125\n",
      "iteration 8526 loss 2.826399326324463, acc 20.3125\n",
      "iteration 8527 loss 2.758904218673706, acc 21.875\n",
      "iteration 8528 loss 2.8778185844421387, acc 17.1875\n",
      "iteration 8529 loss 2.7072813510894775, acc 26.5625\n",
      "iteration 8530 loss 2.7444489002227783, acc 21.875\n",
      "iteration 8531 loss 2.739572763442993, acc 25.0\n",
      "iteration 8532 loss 2.561981201171875, acc 25.0\n",
      "iteration 8533 loss 2.7733542919158936, acc 21.875\n",
      "iteration 8534 loss 2.6421959400177, acc 26.5625\n",
      "iteration 8535 loss 2.80635929107666, acc 26.5625\n",
      "iteration 8536 loss 2.8313841819763184, acc 17.1875\n",
      "iteration 8537 loss 2.8433516025543213, acc 23.4375\n",
      "iteration 8538 loss 2.580688714981079, acc 23.4375\n",
      "iteration 8539 loss 2.8231778144836426, acc 23.4375\n",
      "iteration 8540 loss 2.575040817260742, acc 26.5625\n",
      "iteration 8541 loss 2.6080334186553955, acc 18.75\n",
      "iteration 8542 loss 2.655142068862915, acc 21.875\n",
      "iteration 8543 loss 2.5427887439727783, acc 21.875\n",
      "iteration 8544 loss 2.7423479557037354, acc 17.1875\n",
      "iteration 8545 loss 2.5395584106445312, acc 26.5625\n",
      "iteration 8546 loss 2.5915493965148926, acc 20.3125\n",
      "iteration 8547 loss 2.4058117866516113, acc 31.25\n",
      "iteration 8548 loss 2.822967052459717, acc 15.625\n",
      "iteration 8549 loss 2.731705904006958, acc 23.4375\n",
      "iteration 8550 loss 2.6653473377227783, acc 20.3125\n",
      "iteration 8551 loss 2.78094482421875, acc 17.1875\n",
      "iteration 8552 loss 2.890711784362793, acc 9.375\n",
      "iteration 8553 loss 2.7335705757141113, acc 25.0\n",
      "iteration 8554 loss 2.590726375579834, acc 28.125\n",
      "iteration 8555 loss 2.798570156097412, acc 14.0625\n",
      "iteration 8556 loss 2.745469331741333, acc 17.1875\n",
      "iteration 8557 loss 2.723851442337036, acc 18.75\n",
      "iteration 8558 loss 2.7584595680236816, acc 18.75\n",
      "iteration 8559 loss 2.767277479171753, acc 18.75\n",
      "iteration 8560 loss 2.7778666019439697, acc 18.75\n",
      "iteration 8561 loss 2.639336347579956, acc 26.5625\n",
      "iteration 8562 loss 2.450327157974243, acc 29.6875\n",
      "iteration 8563 loss 2.765824556350708, acc 17.1875\n",
      "iteration 8564 loss 2.8649027347564697, acc 18.75\n",
      "iteration 8565 loss 2.719331741333008, acc 14.0625\n",
      "iteration 8566 loss 2.7488479614257812, acc 20.3125\n",
      "iteration 8567 loss 2.5044875144958496, acc 32.8125\n",
      "iteration 8568 loss 2.9105775356292725, acc 14.0625\n",
      "iteration 8569 loss 2.8524186611175537, acc 18.75\n",
      "iteration 8570 loss 2.849672794342041, acc 20.3125\n",
      "iteration 8571 loss 2.6278209686279297, acc 23.4375\n",
      "iteration 8572 loss 2.6336417198181152, acc 28.125\n",
      "iteration 8573 loss 2.947601795196533, acc 17.1875\n",
      "iteration 8574 loss 2.670706272125244, acc 25.0\n",
      "iteration 8575 loss 2.701727867126465, acc 26.5625\n",
      "iteration 8576 loss 2.6171491146087646, acc 20.3125\n",
      "iteration 8577 loss 2.8906619548797607, acc 18.75\n",
      "iteration 8578 loss 2.4921836853027344, acc 21.875\n",
      "iteration 8579 loss 2.6354331970214844, acc 21.875\n",
      "iteration 8580 loss 2.56705379486084, acc 21.875\n",
      "iteration 8581 loss 2.7326526641845703, acc 20.3125\n",
      "iteration 8582 loss 2.7049343585968018, acc 15.625\n",
      "iteration 8583 loss 2.7195146083831787, acc 21.875\n",
      "iteration 8584 loss 2.6490683555603027, acc 29.6875\n",
      "iteration 8585 loss 2.6340086460113525, acc 34.375\n",
      "iteration 8586 loss 2.897517442703247, acc 15.625\n",
      "iteration 8587 loss 2.521263599395752, acc 20.3125\n",
      "iteration 8588 loss 2.824568510055542, acc 17.1875\n",
      "iteration 8589 loss 2.62998628616333, acc 17.1875\n",
      "iteration 8590 loss 2.77500581741333, acc 14.0625\n",
      "iteration 8591 loss 2.868623971939087, acc 15.625\n",
      "iteration 8592 loss 2.4885880947113037, acc 25.0\n",
      "iteration 8593 loss 2.6605701446533203, acc 28.125\n",
      "iteration 8594 loss 2.6056108474731445, acc 18.75\n",
      "iteration 8595 loss 2.719780683517456, acc 23.4375\n",
      "iteration 8596 loss 2.9306232929229736, acc 14.0625\n",
      "iteration 8597 loss 2.685579299926758, acc 20.3125\n",
      "iteration 8598 loss 2.5390162467956543, acc 29.6875\n",
      "iteration 8599 loss 2.4197330474853516, acc 32.8125\n",
      "iteration 8600 loss 2.534914493560791, acc 28.125\n",
      "iteration 8601 loss 2.5919132232666016, acc 20.3125\n",
      "iteration 8602 loss 2.7935633659362793, acc 23.4375\n",
      "iteration 8603 loss 2.5177688598632812, acc 28.125\n",
      "iteration 8604 loss 2.7896316051483154, acc 20.3125\n",
      "iteration 8605 loss 2.544327735900879, acc 25.0\n",
      "iteration 8606 loss 2.4812378883361816, acc 20.3125\n",
      "iteration 8607 loss 2.536705255508423, acc 25.0\n",
      "iteration 8608 loss 2.7742836475372314, acc 21.875\n",
      "iteration 8609 loss 2.5391643047332764, acc 21.875\n",
      "iteration 8610 loss 2.630866765975952, acc 25.0\n",
      "iteration 8611 loss 2.8607287406921387, acc 15.625\n",
      "iteration 8612 loss 2.7264292240142822, acc 21.875\n",
      "iteration 8613 loss 2.6825110912323, acc 28.125\n",
      "iteration 8614 loss 2.7538676261901855, acc 25.0\n",
      "iteration 8615 loss 2.5852105617523193, acc 23.4375\n",
      "iteration 8616 loss 2.720423936843872, acc 20.3125\n",
      "iteration 8617 loss 2.6340365409851074, acc 21.875\n",
      "iteration 8618 loss 2.5825512409210205, acc 25.0\n",
      "iteration 8619 loss 2.7927300930023193, acc 12.5\n",
      "iteration 8620 loss 2.734243392944336, acc 26.5625\n",
      "iteration 8621 loss 2.32759952545166, acc 32.8125\n",
      "iteration 8622 loss 2.7108771800994873, acc 18.75\n",
      "iteration 8623 loss 2.6636104583740234, acc 21.875\n",
      "iteration 8624 loss 2.7206532955169678, acc 15.625\n",
      "iteration 8625 loss 2.614203453063965, acc 21.875\n",
      "iteration 8626 loss 2.8314616680145264, acc 15.625\n",
      "iteration 8627 loss 2.7501933574676514, acc 14.0625\n",
      "iteration 8628 loss 2.8929669857025146, acc 15.625\n",
      "iteration 8629 loss 2.529407501220703, acc 29.6875\n",
      "iteration 8630 loss 2.6037087440490723, acc 25.0\n",
      "iteration 8631 loss 2.737515926361084, acc 21.875\n",
      "iteration 8632 loss 2.758363723754883, acc 17.1875\n",
      "iteration 8633 loss 2.5602123737335205, acc 17.1875\n",
      "iteration 8634 loss 2.8331921100616455, acc 23.4375\n",
      "iteration 8635 loss 2.5391862392425537, acc 23.4375\n",
      "iteration 8636 loss 2.5683023929595947, acc 28.125\n",
      "iteration 8637 loss 2.691344738006592, acc 25.0\n",
      "iteration 8638 loss 2.618520736694336, acc 23.4375\n",
      "iteration 8639 loss 2.6427478790283203, acc 20.3125\n",
      "iteration 8640 loss 2.588937520980835, acc 25.0\n",
      "iteration 8641 loss 2.4785330295562744, acc 28.125\n",
      "iteration 8642 loss 2.48811674118042, acc 35.9375\n",
      "iteration 8643 loss 2.660979747772217, acc 25.0\n",
      "iteration 8644 loss 2.473253011703491, acc 29.6875\n",
      "iteration 8645 loss 2.5548932552337646, acc 29.6875\n",
      "iteration 8646 loss 2.787235975265503, acc 23.4375\n",
      "iteration 8647 loss 2.9080264568328857, acc 15.625\n",
      "iteration 8648 loss 2.7177376747131348, acc 23.4375\n",
      "iteration 8649 loss 2.6884188652038574, acc 23.4375\n",
      "iteration 8650 loss 2.7707509994506836, acc 17.1875\n",
      "iteration 8651 loss 2.871366262435913, acc 15.625\n",
      "iteration 8652 loss 2.5698153972625732, acc 31.25\n",
      "iteration 8653 loss 2.5402584075927734, acc 23.4375\n",
      "iteration 8654 loss 2.8180859088897705, acc 17.1875\n",
      "iteration 8655 loss 2.654479742050171, acc 21.875\n",
      "iteration 8656 loss 2.7427821159362793, acc 26.5625\n",
      "iteration 8657 loss 2.5434677600860596, acc 31.25\n",
      "iteration 8658 loss 2.6264100074768066, acc 23.4375\n",
      "iteration 8659 loss 2.6091880798339844, acc 25.0\n",
      "iteration 8660 loss 2.583599805831909, acc 25.0\n",
      "iteration 8661 loss 2.728943347930908, acc 15.625\n",
      "iteration 8662 loss 2.850759506225586, acc 14.0625\n",
      "iteration 8663 loss 2.6619372367858887, acc 21.875\n",
      "iteration 8664 loss 2.6629984378814697, acc 20.3125\n",
      "iteration 8665 loss 2.5361335277557373, acc 25.0\n",
      "iteration 8666 loss 2.4539973735809326, acc 31.25\n",
      "iteration 8667 loss 2.8861005306243896, acc 20.3125\n",
      "iteration 8668 loss 2.9954729080200195, acc 25.0\n",
      "iteration 8669 loss 2.834134340286255, acc 18.75\n",
      "iteration 8670 loss 2.655587673187256, acc 18.75\n",
      "iteration 8671 loss 2.773787498474121, acc 23.4375\n",
      "iteration 8672 loss 2.4702656269073486, acc 28.125\n",
      "iteration 8673 loss 2.452059030532837, acc 29.6875\n",
      "iteration 8674 loss 2.500519037246704, acc 29.6875\n",
      "iteration 8675 loss 2.8126718997955322, acc 17.1875\n",
      "iteration 8676 loss 2.433363676071167, acc 32.8125\n",
      "iteration 8677 loss 2.7681121826171875, acc 15.625\n",
      "iteration 8678 loss 2.737551212310791, acc 18.75\n",
      "iteration 8679 loss 2.442232370376587, acc 23.4375\n",
      "iteration 8680 loss 2.7858176231384277, acc 18.75\n",
      "iteration 8681 loss 2.4886176586151123, acc 25.0\n",
      "iteration 8682 loss 2.6989452838897705, acc 18.75\n",
      "iteration 8683 loss 2.6443562507629395, acc 21.875\n",
      "iteration 8684 loss 2.9284439086914062, acc 14.0625\n",
      "iteration 8685 loss 2.8729894161224365, acc 18.75\n",
      "iteration 8686 loss 2.527024984359741, acc 26.5625\n",
      "iteration 8687 loss 2.6593942642211914, acc 20.3125\n",
      "iteration 8688 loss 2.6221320629119873, acc 20.3125\n",
      "iteration 8689 loss 2.6901865005493164, acc 17.1875\n",
      "iteration 8690 loss 2.549955368041992, acc 25.0\n",
      "iteration 8691 loss 2.6709349155426025, acc 23.4375\n",
      "iteration 8692 loss 2.8516347408294678, acc 14.0625\n",
      "iteration 8693 loss 2.640073776245117, acc 20.3125\n",
      "iteration 8694 loss 2.832367181777954, acc 9.375\n",
      "iteration 8695 loss 2.677786350250244, acc 28.125\n",
      "iteration 8696 loss 2.7159245014190674, acc 25.0\n",
      "iteration 8697 loss 2.79140305519104, acc 15.625\n",
      "iteration 8698 loss 2.544398546218872, acc 18.75\n",
      "iteration 8699 loss 2.646747350692749, acc 25.0\n",
      "iteration 8700 loss 2.6254427433013916, acc 25.0\n",
      "iteration 8701 loss 2.7287590503692627, acc 25.0\n",
      "iteration 8702 loss 2.9436092376708984, acc 12.5\n",
      "iteration 8703 loss 2.742006301879883, acc 18.75\n",
      "iteration 8704 loss 2.754303216934204, acc 17.1875\n",
      "iteration 8705 loss 2.821687698364258, acc 18.75\n",
      "iteration 8706 loss 2.509403944015503, acc 26.5625\n",
      "iteration 8707 loss 2.53680682182312, acc 28.125\n",
      "iteration 8708 loss 2.549696207046509, acc 28.125\n",
      "iteration 8709 loss 2.683847188949585, acc 28.125\n",
      "iteration 8710 loss 2.8005990982055664, acc 15.625\n",
      "iteration 8711 loss 2.6085293292999268, acc 21.875\n",
      "iteration 8712 loss 2.6903960704803467, acc 23.4375\n",
      "iteration 8713 loss 2.814424514770508, acc 17.1875\n",
      "iteration 8714 loss 2.5893847942352295, acc 32.8125\n",
      "iteration 8715 loss 2.90840220451355, acc 18.75\n",
      "iteration 8716 loss 2.596730947494507, acc 23.4375\n",
      "iteration 8717 loss 2.5265345573425293, acc 23.4375\n",
      "iteration 8718 loss 2.780280828475952, acc 14.0625\n",
      "iteration 8719 loss 2.7059779167175293, acc 23.4375\n",
      "iteration 8720 loss 2.688640594482422, acc 26.5625\n",
      "iteration 8721 loss 2.778996467590332, acc 17.1875\n",
      "iteration 8722 loss 2.6100308895111084, acc 20.3125\n",
      "iteration 8723 loss 2.5698604583740234, acc 29.6875\n",
      "iteration 8724 loss 2.713923454284668, acc 17.1875\n",
      "iteration 8725 loss 2.666224241256714, acc 21.875\n",
      "iteration 8726 loss 2.6416659355163574, acc 26.5625\n",
      "iteration 8727 loss 2.642880439758301, acc 18.75\n",
      "iteration 8728 loss 2.795999526977539, acc 10.9375\n",
      "iteration 8729 loss 2.6035892963409424, acc 21.875\n",
      "iteration 8730 loss 2.5947370529174805, acc 21.875\n",
      "iteration 8731 loss 2.5471062660217285, acc 34.375\n",
      "iteration 8732 loss 2.6744120121002197, acc 21.875\n",
      "iteration 8733 loss 2.8188669681549072, acc 23.4375\n",
      "iteration 8734 loss 2.7708332538604736, acc 14.0625\n",
      "iteration 8735 loss 2.5504472255706787, acc 20.3125\n",
      "iteration 8736 loss 2.6908974647521973, acc 21.875\n",
      "iteration 8737 loss 2.722548007965088, acc 20.3125\n",
      "iteration 8738 loss 2.6653881072998047, acc 20.3125\n",
      "iteration 8739 loss 2.6106014251708984, acc 26.5625\n",
      "iteration 8740 loss 2.6125106811523438, acc 29.6875\n",
      "iteration 8741 loss 2.5683372020721436, acc 29.6875\n",
      "iteration 8742 loss 2.68084979057312, acc 21.875\n",
      "iteration 8743 loss 2.7937099933624268, acc 20.3125\n",
      "iteration 8744 loss 2.5819427967071533, acc 26.5625\n",
      "iteration 8745 loss 2.5725815296173096, acc 21.875\n",
      "iteration 8746 loss 2.62628436088562, acc 20.3125\n",
      "iteration 8747 loss 2.9850082397460938, acc 12.5\n",
      "iteration 8748 loss 2.7169110774993896, acc 10.9375\n",
      "iteration 8749 loss 2.6419382095336914, acc 25.0\n",
      "iteration 8750 loss 2.8719100952148438, acc 21.875\n",
      "iteration 8751 loss 2.568511962890625, acc 28.125\n",
      "iteration 8752 loss 2.609116554260254, acc 25.0\n",
      "iteration 8753 loss 2.596463203430176, acc 20.3125\n",
      "iteration 8754 loss 2.738825798034668, acc 28.125\n",
      "iteration 8755 loss 2.5171568393707275, acc 25.0\n",
      "iteration 8756 loss 2.8062033653259277, acc 23.4375\n",
      "iteration 8757 loss 2.5309364795684814, acc 26.5625\n",
      "iteration 8758 loss 2.704184055328369, acc 18.75\n",
      "iteration 8759 loss 2.8437509536743164, acc 21.875\n",
      "iteration 8760 loss 2.416828155517578, acc 40.625\n",
      "iteration 8761 loss 2.781679153442383, acc 21.875\n",
      "iteration 8762 loss 2.5057997703552246, acc 34.375\n",
      "iteration 8763 loss 2.6632909774780273, acc 23.4375\n",
      "iteration 8764 loss 2.744685411453247, acc 18.75\n",
      "iteration 8765 loss 2.6067986488342285, acc 25.0\n",
      "iteration 8766 loss 2.553244113922119, acc 20.3125\n",
      "iteration 8767 loss 2.659921169281006, acc 26.5625\n",
      "iteration 8768 loss 2.6399261951446533, acc 21.875\n",
      "iteration 8769 loss 2.7800986766815186, acc 15.625\n",
      "iteration 8770 loss 2.608412027359009, acc 28.125\n",
      "iteration 8771 loss 2.5997979640960693, acc 26.5625\n",
      "iteration 8772 loss 2.6157519817352295, acc 17.1875\n",
      "iteration 8773 loss 2.9833483695983887, acc 10.9375\n",
      "iteration 8774 loss 2.683063268661499, acc 23.4375\n",
      "iteration 8775 loss 2.5989670753479004, acc 21.875\n",
      "iteration 8776 loss 2.773024320602417, acc 21.875\n",
      "iteration 8777 loss 2.6908204555511475, acc 23.4375\n",
      "iteration 8778 loss 2.607802391052246, acc 23.4375\n",
      "iteration 8779 loss 2.5675342082977295, acc 21.875\n",
      "iteration 8780 loss 2.593749761581421, acc 29.6875\n",
      "iteration 8781 loss 2.741781711578369, acc 18.75\n",
      "iteration 8782 loss 2.666952133178711, acc 28.125\n",
      "iteration 8783 loss 2.510197162628174, acc 29.6875\n",
      "iteration 8784 loss 2.824970006942749, acc 18.75\n",
      "iteration 8785 loss 2.6503562927246094, acc 25.0\n",
      "iteration 8786 loss 2.7981412410736084, acc 18.75\n",
      "iteration 8787 loss 2.8144869804382324, acc 26.5625\n",
      "iteration 8788 loss 2.4822142124176025, acc 28.125\n",
      "iteration 8789 loss 2.6470818519592285, acc 21.875\n",
      "iteration 8790 loss 2.529055118560791, acc 26.5625\n",
      "iteration 8791 loss 2.7495975494384766, acc 20.3125\n",
      "iteration 8792 loss 2.5769617557525635, acc 15.625\n",
      "iteration 8793 loss 2.6807916164398193, acc 28.125\n",
      "iteration 8794 loss 2.581817626953125, acc 26.5625\n",
      "iteration 8795 loss 2.5830488204956055, acc 25.0\n",
      "iteration 8796 loss 2.7356386184692383, acc 20.3125\n",
      "iteration 8797 loss 2.754014015197754, acc 20.3125\n",
      "iteration 8798 loss 2.6824328899383545, acc 23.4375\n",
      "iteration 8799 loss 2.6022815704345703, acc 28.125\n",
      "iteration 8800 loss 2.6837148666381836, acc 20.3125\n",
      "iteration 8801 loss 2.6153690814971924, acc 21.875\n",
      "iteration 8802 loss 2.482377529144287, acc 29.6875\n",
      "iteration 8803 loss 2.6778297424316406, acc 20.3125\n",
      "iteration 8804 loss 2.5725255012512207, acc 31.25\n",
      "iteration 8805 loss 2.500373125076294, acc 25.0\n",
      "iteration 8806 loss 2.8203125, acc 15.625\n",
      "iteration 8807 loss 2.6863343715667725, acc 28.125\n",
      "iteration 8808 loss 2.6162662506103516, acc 18.75\n",
      "iteration 8809 loss 2.6153523921966553, acc 23.4375\n",
      "iteration 8810 loss 2.817016363143921, acc 15.625\n",
      "iteration 8811 loss 2.4916841983795166, acc 28.125\n",
      "iteration 8812 loss 2.587752342224121, acc 20.3125\n",
      "iteration 8813 loss 2.5715320110321045, acc 20.3125\n",
      "iteration 8814 loss 2.7588887214660645, acc 17.1875\n",
      "iteration 8815 loss 2.5468053817749023, acc 21.875\n",
      "iteration 8816 loss 2.7403337955474854, acc 15.625\n",
      "iteration 8817 loss 2.5121471881866455, acc 32.8125\n",
      "iteration 8818 loss 2.694589853286743, acc 18.75\n",
      "iteration 8819 loss 2.5690598487854004, acc 28.125\n",
      "iteration 8820 loss 2.85558819770813, acc 23.4375\n",
      "iteration 8821 loss 2.5035760402679443, acc 25.0\n",
      "iteration 8822 loss 2.736457109451294, acc 23.4375\n",
      "iteration 8823 loss 2.546316146850586, acc 21.875\n",
      "iteration 8824 loss 2.5900025367736816, acc 20.3125\n",
      "iteration 8825 loss 2.6898481845855713, acc 14.0625\n",
      "iteration 8826 loss 2.6098365783691406, acc 21.875\n",
      "iteration 8827 loss 2.800727605819702, acc 17.1875\n",
      "iteration 8828 loss 2.530297040939331, acc 25.0\n",
      "iteration 8829 loss 2.403146266937256, acc 31.25\n",
      "iteration 8830 loss 2.5485422611236572, acc 23.4375\n",
      "iteration 8831 loss 2.938347339630127, acc 18.75\n",
      "iteration 8832 loss 2.5936543941497803, acc 29.6875\n",
      "iteration 8833 loss 2.771463394165039, acc 21.875\n",
      "iteration 8834 loss 2.9660232067108154, acc 9.375\n",
      "iteration 8835 loss 2.807525157928467, acc 23.4375\n",
      "iteration 8836 loss 2.7490158081054688, acc 21.875\n",
      "iteration 8837 loss 2.860074996948242, acc 23.4375\n",
      "iteration 8838 loss 2.5685250759124756, acc 29.6875\n",
      "iteration 8839 loss 2.575585126876831, acc 28.125\n",
      "iteration 8840 loss 2.8308675289154053, acc 10.9375\n",
      "iteration 8841 loss 2.7179925441741943, acc 17.1875\n",
      "iteration 8842 loss 2.8713128566741943, acc 15.625\n",
      "iteration 8843 loss 2.733327627182007, acc 17.1875\n",
      "iteration 8844 loss 2.7413980960845947, acc 15.625\n",
      "iteration 8845 loss 2.469395399093628, acc 28.125\n",
      "iteration 8846 loss 2.7423460483551025, acc 15.625\n",
      "iteration 8847 loss 2.488356828689575, acc 23.4375\n",
      "iteration 8848 loss 2.8552865982055664, acc 20.3125\n",
      "iteration 8849 loss 2.705198287963867, acc 20.3125\n",
      "iteration 8850 loss 2.5911989212036133, acc 32.8125\n",
      "iteration 8851 loss 2.4268369674682617, acc 26.5625\n",
      "iteration 8852 loss 2.767523765563965, acc 20.3125\n",
      "iteration 8853 loss 2.6239702701568604, acc 23.4375\n",
      "iteration 8854 loss 2.7720446586608887, acc 14.0625\n",
      "iteration 8855 loss 2.5418179035186768, acc 17.1875\n",
      "iteration 8856 loss 2.7501060962677, acc 23.4375\n",
      "iteration 8857 loss 2.495251178741455, acc 23.4375\n",
      "iteration 8858 loss 2.6046202182769775, acc 23.4375\n",
      "iteration 8859 loss 2.5861666202545166, acc 29.6875\n",
      "iteration 8860 loss 2.5253796577453613, acc 26.5625\n",
      "iteration 8861 loss 2.586601495742798, acc 26.5625\n",
      "iteration 8862 loss 2.789130687713623, acc 15.625\n",
      "iteration 8863 loss 2.638129711151123, acc 25.0\n",
      "iteration 8864 loss 2.825383186340332, acc 9.375\n",
      "iteration 8865 loss 2.5600743293762207, acc 17.1875\n",
      "iteration 8866 loss 2.633866310119629, acc 20.3125\n",
      "iteration 8867 loss 2.6037468910217285, acc 23.4375\n",
      "iteration 8868 loss 2.5651841163635254, acc 21.875\n",
      "iteration 8869 loss 2.7677786350250244, acc 25.0\n",
      "iteration 8870 loss 2.629789113998413, acc 28.125\n",
      "iteration 8871 loss 2.3859994411468506, acc 34.375\n",
      "iteration 8872 loss 2.8419110774993896, acc 14.0625\n",
      "iteration 8873 loss 2.5769922733306885, acc 21.875\n",
      "iteration 8874 loss 2.673914909362793, acc 23.4375\n",
      "iteration 8875 loss 2.5293490886688232, acc 29.6875\n",
      "iteration 8876 loss 2.742478847503662, acc 18.75\n",
      "iteration 8877 loss 2.659733533859253, acc 17.1875\n",
      "iteration 8878 loss 2.7067878246307373, acc 14.0625\n",
      "iteration 8879 loss 2.65614914894104, acc 21.875\n",
      "iteration 8880 loss 2.5451529026031494, acc 28.125\n",
      "iteration 8881 loss 2.558248996734619, acc 21.875\n",
      "iteration 8882 loss 2.611527442932129, acc 20.3125\n",
      "iteration 8883 loss 2.46117901802063, acc 29.6875\n",
      "iteration 8884 loss 2.65578031539917, acc 23.4375\n",
      "iteration 8885 loss 2.512899398803711, acc 29.6875\n",
      "iteration 8886 loss 2.606163501739502, acc 21.875\n",
      "iteration 8887 loss 2.6418211460113525, acc 18.75\n",
      "iteration 8888 loss 2.5592150688171387, acc 26.5625\n",
      "iteration 8889 loss 2.8696205615997314, acc 18.75\n",
      "iteration 8890 loss 2.4784963130950928, acc 21.875\n",
      "iteration 8891 loss 2.6472177505493164, acc 26.5625\n",
      "iteration 8892 loss 2.715451955795288, acc 20.3125\n",
      "iteration 8893 loss 2.733433961868286, acc 20.3125\n",
      "iteration 8894 loss 2.831777572631836, acc 20.3125\n",
      "iteration 8895 loss 2.8508362770080566, acc 20.3125\n",
      "iteration 8896 loss 2.5384864807128906, acc 31.25\n",
      "iteration 8897 loss 2.7023725509643555, acc 14.0625\n",
      "iteration 8898 loss 2.855755090713501, acc 15.625\n",
      "iteration 8899 loss 2.547428846359253, acc 21.875\n",
      "iteration 8900 loss 2.665114402770996, acc 21.875\n",
      "iteration 8901 loss 2.8860371112823486, acc 9.375\n",
      "iteration 8902 loss 2.7800838947296143, acc 18.75\n",
      "iteration 8903 loss 2.5699949264526367, acc 23.4375\n",
      "iteration 8904 loss 2.8543856143951416, acc 12.5\n",
      "iteration 8905 loss 2.5818235874176025, acc 25.0\n",
      "iteration 8906 loss 2.556284189224243, acc 25.0\n",
      "iteration 8907 loss 2.821931838989258, acc 25.0\n",
      "iteration 8908 loss 2.723848581314087, acc 12.5\n",
      "iteration 8909 loss 2.781134605407715, acc 17.1875\n",
      "iteration 8910 loss 2.886824607849121, acc 23.4375\n",
      "iteration 8911 loss 2.6739635467529297, acc 18.75\n",
      "iteration 8912 loss 2.725067377090454, acc 15.625\n",
      "iteration 8913 loss 2.6682114601135254, acc 18.75\n",
      "iteration 8914 loss 2.7820844650268555, acc 23.4375\n",
      "iteration 8915 loss 2.5602505207061768, acc 23.4375\n",
      "iteration 8916 loss 2.5844132900238037, acc 25.0\n",
      "iteration 8917 loss 2.5583205223083496, acc 29.6875\n",
      "iteration 8918 loss 2.3888723850250244, acc 31.25\n",
      "iteration 8919 loss 2.733175277709961, acc 17.1875\n",
      "iteration 8920 loss 2.7414963245391846, acc 21.875\n",
      "iteration 8921 loss 2.761975049972534, acc 15.625\n",
      "iteration 8922 loss 2.7037737369537354, acc 21.875\n",
      "iteration 8923 loss 2.581840753555298, acc 21.875\n",
      "iteration 8924 loss 2.632415294647217, acc 20.3125\n",
      "iteration 8925 loss 2.5344839096069336, acc 25.0\n",
      "iteration 8926 loss 2.5606400966644287, acc 25.0\n",
      "iteration 8927 loss 2.4489171504974365, acc 29.6875\n",
      "iteration 8928 loss 3.110217332839966, acc 18.75\n",
      "iteration 8929 loss 2.7351036071777344, acc 18.75\n",
      "iteration 8930 loss 2.5764482021331787, acc 21.875\n",
      "iteration 8931 loss 2.6451351642608643, acc 25.0\n",
      "iteration 8932 loss 2.5327303409576416, acc 26.5625\n",
      "iteration 8933 loss 2.6820952892303467, acc 25.0\n",
      "iteration 8934 loss 2.7845590114593506, acc 20.3125\n",
      "iteration 8935 loss 2.5376269817352295, acc 25.0\n",
      "iteration 8936 loss 2.785770893096924, acc 14.0625\n",
      "iteration 8937 loss 2.565664291381836, acc 20.3125\n",
      "iteration 8938 loss 2.7266182899475098, acc 15.625\n",
      "iteration 8939 loss 2.75173020362854, acc 20.3125\n",
      "iteration 8940 loss 2.760166883468628, acc 21.875\n",
      "iteration 8941 loss 2.7268145084381104, acc 18.75\n",
      "iteration 8942 loss 2.6370458602905273, acc 28.125\n",
      "iteration 8943 loss 2.668015956878662, acc 21.875\n",
      "iteration 8944 loss 2.500123977661133, acc 23.4375\n",
      "iteration 8945 loss 2.527123212814331, acc 28.125\n",
      "iteration 8946 loss 2.6358280181884766, acc 28.125\n",
      "iteration 8947 loss 2.7053396701812744, acc 20.3125\n",
      "iteration 8948 loss 2.653197765350342, acc 26.5625\n",
      "iteration 8949 loss 2.840955972671509, acc 23.4375\n",
      "iteration 8950 loss 2.6909966468811035, acc 15.625\n",
      "iteration 8951 loss 2.9350337982177734, acc 12.5\n",
      "iteration 8952 loss 2.605726718902588, acc 26.5625\n",
      "iteration 8953 loss 2.458188533782959, acc 34.375\n",
      "iteration 8954 loss 2.6030194759368896, acc 21.875\n",
      "iteration 8955 loss 2.7215895652770996, acc 17.1875\n",
      "iteration 8956 loss 2.614893913269043, acc 21.875\n",
      "iteration 8957 loss 2.7529916763305664, acc 20.3125\n",
      "iteration 8958 loss 2.7482569217681885, acc 20.3125\n",
      "iteration 8959 loss 2.668769359588623, acc 18.75\n",
      "iteration 8960 loss 3.0010151863098145, acc 10.9375\n",
      "iteration 8961 loss 2.6996891498565674, acc 21.875\n",
      "iteration 8962 loss 2.5742909908294678, acc 21.875\n",
      "iteration 8963 loss 2.8026483058929443, acc 12.5\n",
      "iteration 8964 loss 2.452726364135742, acc 32.8125\n",
      "iteration 8965 loss 2.560568332672119, acc 29.6875\n",
      "iteration 8966 loss 2.483968496322632, acc 17.1875\n",
      "iteration 8967 loss 2.698347330093384, acc 15.625\n",
      "iteration 8968 loss 2.720742702484131, acc 18.75\n",
      "iteration 8969 loss 2.8408215045928955, acc 17.1875\n",
      "iteration 8970 loss 2.7997283935546875, acc 12.5\n",
      "iteration 8971 loss 2.8111846446990967, acc 31.25\n",
      "iteration 8972 loss 2.675708293914795, acc 21.875\n",
      "iteration 8973 loss 3.1164393424987793, acc 10.9375\n",
      "iteration 8974 loss 2.739743232727051, acc 21.875\n",
      "iteration 8975 loss 2.53479266166687, acc 26.5625\n",
      "iteration 8976 loss 2.5268630981445312, acc 28.125\n",
      "iteration 8977 loss 2.4852755069732666, acc 32.8125\n",
      "iteration 8978 loss 2.6792752742767334, acc 15.625\n",
      "iteration 8979 loss 2.698251247406006, acc 23.4375\n",
      "iteration 8980 loss 2.57808780670166, acc 28.125\n",
      "iteration 8981 loss 2.4452035427093506, acc 26.5625\n",
      "iteration 8982 loss 2.395871162414551, acc 34.375\n",
      "iteration 8983 loss 2.705822706222534, acc 23.4375\n",
      "iteration 8984 loss 2.480466365814209, acc 34.375\n",
      "iteration 8985 loss 3.1953070163726807, acc 9.375\n",
      "iteration 8986 loss 2.5955142974853516, acc 21.875\n",
      "iteration 8987 loss 2.7432758808135986, acc 21.875\n",
      "iteration 8988 loss 2.5096092224121094, acc 31.25\n",
      "iteration 8989 loss 2.793834924697876, acc 25.0\n",
      "iteration 8990 loss 2.6494839191436768, acc 14.0625\n",
      "iteration 8991 loss 2.6169800758361816, acc 26.5625\n",
      "iteration 8992 loss 2.892584800720215, acc 17.1875\n",
      "iteration 8993 loss 2.498347759246826, acc 15.625\n",
      "iteration 8994 loss 2.735334873199463, acc 18.75\n",
      "iteration 8995 loss 2.667198896408081, acc 18.75\n",
      "iteration 8996 loss 2.407773733139038, acc 37.5\n",
      "iteration 8997 loss 2.7184488773345947, acc 20.3125\n",
      "iteration 8998 loss 2.57030987739563, acc 20.3125\n",
      "iteration 8999 loss 2.569988489151001, acc 28.125\n",
      "iteration 9000 loss 2.78002667427063, acc 21.875\n",
      "iteration 9001 loss 2.761521339416504, acc 23.4375\n",
      "iteration 9002 loss 2.5905075073242188, acc 25.0\n",
      "iteration 9003 loss 2.9634909629821777, acc 14.0625\n",
      "iteration 9004 loss 2.667799472808838, acc 26.5625\n",
      "iteration 9005 loss 2.6167221069335938, acc 23.4375\n",
      "iteration 9006 loss 2.8570504188537598, acc 17.1875\n",
      "iteration 9007 loss 2.8619697093963623, acc 17.1875\n",
      "iteration 9008 loss 2.6255292892456055, acc 25.0\n",
      "iteration 9009 loss 2.802485942840576, acc 14.0625\n",
      "iteration 9010 loss 2.618309736251831, acc 21.875\n",
      "iteration 9011 loss 2.5964765548706055, acc 20.3125\n",
      "iteration 9012 loss 2.6227619647979736, acc 28.125\n",
      "iteration 9013 loss 2.5344176292419434, acc 26.5625\n",
      "iteration 9014 loss 2.477459192276001, acc 32.8125\n",
      "iteration 9015 loss 2.8292243480682373, acc 17.1875\n",
      "iteration 9016 loss 2.695370674133301, acc 18.75\n",
      "iteration 9017 loss 2.576714277267456, acc 31.25\n",
      "iteration 9018 loss 2.686361789703369, acc 26.5625\n",
      "iteration 9019 loss 2.779024124145508, acc 14.0625\n",
      "iteration 9020 loss 2.8274691104888916, acc 15.625\n",
      "iteration 9021 loss 2.7252511978149414, acc 26.5625\n",
      "iteration 9022 loss 2.810232400894165, acc 18.75\n",
      "iteration 9023 loss 2.8312604427337646, acc 17.1875\n",
      "iteration 9024 loss 2.733380079269409, acc 23.4375\n",
      "iteration 9025 loss 2.6653499603271484, acc 25.0\n",
      "iteration 9026 loss 2.8348958492279053, acc 17.1875\n",
      "iteration 9027 loss 2.762402057647705, acc 15.625\n",
      "iteration 9028 loss 2.6562397480010986, acc 23.4375\n",
      "iteration 9029 loss 3.0347650051116943, acc 17.1875\n",
      "iteration 9030 loss 2.6036455631256104, acc 23.4375\n",
      "iteration 9031 loss 2.7286183834075928, acc 23.4375\n",
      "iteration 9032 loss 2.537534475326538, acc 25.0\n",
      "iteration 9033 loss 2.65451717376709, acc 18.75\n",
      "iteration 9034 loss 2.5756773948669434, acc 25.0\n",
      "iteration 9035 loss 2.568089485168457, acc 25.0\n",
      "iteration 9036 loss 2.845280170440674, acc 23.4375\n",
      "iteration 9037 loss 2.4243991374969482, acc 34.375\n",
      "iteration 9038 loss 2.647700071334839, acc 20.3125\n",
      "iteration 9039 loss 2.7382144927978516, acc 20.3125\n",
      "iteration 9040 loss 2.5040712356567383, acc 34.375\n",
      "iteration 9041 loss 2.553928852081299, acc 21.875\n",
      "iteration 9042 loss 2.556380033493042, acc 20.3125\n",
      "iteration 9043 loss 2.969139575958252, acc 20.3125\n",
      "iteration 9044 loss 2.681297779083252, acc 18.75\n",
      "iteration 9045 loss 2.680204391479492, acc 26.5625\n",
      "iteration 9046 loss 2.439390182495117, acc 23.4375\n",
      "iteration 9047 loss 2.820885419845581, acc 20.3125\n",
      "iteration 9048 loss 2.922438621520996, acc 14.0625\n",
      "iteration 9049 loss 2.817413091659546, acc 20.3125\n",
      "iteration 9050 loss 2.6297309398651123, acc 23.4375\n",
      "iteration 9051 loss 2.6086323261260986, acc 25.0\n",
      "iteration 9052 loss 2.681070327758789, acc 23.4375\n",
      "iteration 9053 loss 2.744678020477295, acc 20.3125\n",
      "iteration 9054 loss 2.8123016357421875, acc 14.0625\n",
      "iteration 9055 loss 2.6380019187927246, acc 25.0\n",
      "iteration 9056 loss 2.75797963142395, acc 15.625\n",
      "iteration 9057 loss 2.737152099609375, acc 28.125\n",
      "iteration 9058 loss 2.5618317127227783, acc 23.4375\n",
      "iteration 9059 loss 2.6938111782073975, acc 17.1875\n",
      "iteration 9060 loss 2.718092203140259, acc 20.3125\n",
      "iteration 9061 loss 2.6844959259033203, acc 20.3125\n",
      "iteration 9062 loss 2.477454900741577, acc 34.375\n",
      "iteration 9063 loss 2.6975302696228027, acc 14.0625\n",
      "iteration 9064 loss 2.6291909217834473, acc 26.5625\n",
      "iteration 9065 loss 2.632594347000122, acc 23.4375\n",
      "iteration 9066 loss 2.5172176361083984, acc 29.6875\n",
      "iteration 9067 loss 2.5038487911224365, acc 31.25\n",
      "iteration 9068 loss 2.4123735427856445, acc 28.125\n",
      "iteration 9069 loss 2.926528215408325, acc 20.3125\n",
      "iteration 9070 loss 2.7116858959198, acc 23.4375\n",
      "iteration 9071 loss 2.621074676513672, acc 25.0\n",
      "iteration 9072 loss 2.7974984645843506, acc 20.3125\n",
      "iteration 9073 loss 2.7218902111053467, acc 25.0\n",
      "iteration 9074 loss 2.620656728744507, acc 31.25\n",
      "iteration 9075 loss 2.5788028240203857, acc 25.0\n",
      "iteration 9076 loss 2.4667441844940186, acc 29.6875\n",
      "iteration 9077 loss 2.8634164333343506, acc 18.75\n",
      "iteration 9078 loss 2.6426258087158203, acc 21.875\n",
      "iteration 9079 loss 2.9536993503570557, acc 14.0625\n",
      "iteration 9080 loss 2.5581612586975098, acc 28.125\n",
      "iteration 9081 loss 2.9457836151123047, acc 20.3125\n",
      "iteration 9082 loss 2.5763368606567383, acc 29.6875\n",
      "iteration 9083 loss 2.787693500518799, acc 25.0\n",
      "iteration 9084 loss 2.7160229682922363, acc 18.75\n",
      "iteration 9085 loss 2.6861557960510254, acc 17.1875\n",
      "iteration 9086 loss 2.6528589725494385, acc 21.875\n",
      "iteration 9087 loss 2.5773942470550537, acc 25.0\n",
      "iteration 9088 loss 2.5301098823547363, acc 26.5625\n",
      "iteration 9089 loss 2.439645290374756, acc 26.5625\n",
      "iteration 9090 loss 2.621011734008789, acc 21.875\n",
      "iteration 9091 loss 2.5992677211761475, acc 23.4375\n",
      "iteration 9092 loss 2.5775113105773926, acc 20.3125\n",
      "iteration 9093 loss 2.889455556869507, acc 15.625\n",
      "iteration 9094 loss 2.7680580615997314, acc 17.1875\n",
      "iteration 9095 loss 2.7399771213531494, acc 21.875\n",
      "iteration 9096 loss 2.8138349056243896, acc 12.5\n",
      "iteration 9097 loss 2.622746229171753, acc 18.75\n",
      "iteration 9098 loss 2.6848807334899902, acc 21.875\n",
      "iteration 9099 loss 2.5883054733276367, acc 25.0\n",
      "iteration 9100 loss 2.673215389251709, acc 29.6875\n",
      "iteration 9101 loss 2.7322189807891846, acc 18.75\n",
      "iteration 9102 loss 2.3766591548919678, acc 34.375\n",
      "iteration 9103 loss 2.824364423751831, acc 18.75\n",
      "iteration 9104 loss 2.7850449085235596, acc 23.4375\n",
      "iteration 9105 loss 2.6912145614624023, acc 23.4375\n",
      "iteration 9106 loss 2.8703179359436035, acc 15.625\n",
      "iteration 9107 loss 2.4933037757873535, acc 26.5625\n",
      "iteration 9108 loss 2.547487258911133, acc 28.125\n",
      "iteration 9109 loss 2.7350006103515625, acc 21.875\n",
      "iteration 9110 loss 2.816657304763794, acc 20.3125\n",
      "iteration 9111 loss 2.5925636291503906, acc 23.4375\n",
      "iteration 9112 loss 2.6023242473602295, acc 20.3125\n",
      "iteration 9113 loss 2.4979140758514404, acc 25.0\n",
      "iteration 9114 loss 2.9158284664154053, acc 18.75\n",
      "iteration 9115 loss 2.53159761428833, acc 26.5625\n",
      "iteration 9116 loss 2.596806764602661, acc 28.125\n",
      "iteration 9117 loss 2.799771547317505, acc 20.3125\n",
      "iteration 9118 loss 2.617819309234619, acc 17.1875\n",
      "iteration 9119 loss 2.702768325805664, acc 28.125\n",
      "iteration 9120 loss 2.830116033554077, acc 14.0625\n",
      "iteration 9121 loss 2.7171268463134766, acc 20.3125\n",
      "iteration 9122 loss 2.6669270992279053, acc 15.625\n",
      "iteration 9123 loss 2.734797239303589, acc 12.5\n",
      "iteration 9124 loss 2.59270977973938, acc 18.75\n",
      "iteration 9125 loss 2.862672805786133, acc 18.75\n",
      "iteration 9126 loss 2.5984365940093994, acc 26.5625\n",
      "iteration 9127 loss 2.772684097290039, acc 26.5625\n",
      "iteration 9128 loss 2.654402494430542, acc 18.75\n",
      "iteration 9129 loss 2.5937609672546387, acc 23.4375\n",
      "iteration 9130 loss 2.7667500972747803, acc 21.875\n",
      "iteration 9131 loss 2.6154425144195557, acc 25.0\n",
      "iteration 9132 loss 2.464165210723877, acc 29.6875\n",
      "iteration 9133 loss 2.8041415214538574, acc 20.3125\n",
      "iteration 9134 loss 2.6759862899780273, acc 20.3125\n",
      "iteration 9135 loss 2.5682578086853027, acc 28.125\n",
      "iteration 9136 loss 2.492783784866333, acc 34.375\n",
      "iteration 9137 loss 2.875532865524292, acc 17.1875\n",
      "iteration 9138 loss 2.748940944671631, acc 18.75\n",
      "iteration 9139 loss 2.5798559188842773, acc 21.875\n",
      "iteration 9140 loss 2.7005937099456787, acc 23.4375\n",
      "iteration 9141 loss 2.7171947956085205, acc 17.1875\n",
      "iteration 9142 loss 2.6590607166290283, acc 17.1875\n",
      "iteration 9143 loss 2.3561596870422363, acc 35.9375\n",
      "iteration 9144 loss 2.573000431060791, acc 26.5625\n",
      "iteration 9145 loss 2.6300413608551025, acc 25.0\n",
      "iteration 9146 loss 2.5988059043884277, acc 21.875\n",
      "iteration 9147 loss 2.817432165145874, acc 25.0\n",
      "iteration 9148 loss 2.6968908309936523, acc 26.5625\n",
      "iteration 9149 loss 2.568455696105957, acc 28.125\n",
      "iteration 9150 loss 2.7387442588806152, acc 21.875\n",
      "iteration 9151 loss 2.672776222229004, acc 17.1875\n",
      "iteration 9152 loss 2.8772945404052734, acc 17.1875\n",
      "iteration 9153 loss 2.5884032249450684, acc 29.6875\n",
      "iteration 9154 loss 2.640882730484009, acc 29.6875\n",
      "iteration 9155 loss 2.6815459728240967, acc 20.3125\n",
      "iteration 9156 loss 2.754065752029419, acc 21.875\n",
      "iteration 9157 loss 2.678478240966797, acc 17.1875\n",
      "iteration 9158 loss 2.931871175765991, acc 15.625\n",
      "iteration 9159 loss 2.81978178024292, acc 21.875\n",
      "iteration 9160 loss 2.63602614402771, acc 23.4375\n",
      "iteration 9161 loss 2.6656787395477295, acc 20.3125\n",
      "iteration 9162 loss 2.4995977878570557, acc 29.6875\n",
      "iteration 9163 loss 2.7739057540893555, acc 17.1875\n",
      "iteration 9164 loss 2.850152015686035, acc 17.1875\n",
      "iteration 9165 loss 2.730363607406616, acc 17.1875\n",
      "iteration 9166 loss 2.764448881149292, acc 18.75\n",
      "iteration 9167 loss 2.521799325942993, acc 25.0\n",
      "iteration 9168 loss 2.478104829788208, acc 25.0\n",
      "iteration 9169 loss 2.7942981719970703, acc 28.125\n",
      "iteration 9170 loss 2.6305980682373047, acc 20.3125\n",
      "iteration 9171 loss 2.770873546600342, acc 23.4375\n",
      "iteration 9172 loss 2.895297050476074, acc 12.5\n",
      "iteration 9173 loss 2.7788047790527344, acc 25.0\n",
      "iteration 9174 loss 2.693624496459961, acc 17.1875\n",
      "iteration 9175 loss 2.4724233150482178, acc 25.0\n",
      "iteration 9176 loss 2.6363584995269775, acc 23.4375\n",
      "iteration 9177 loss 2.729977607727051, acc 15.625\n",
      "iteration 9178 loss 2.8179149627685547, acc 21.875\n",
      "iteration 9179 loss 2.497270345687866, acc 26.5625\n",
      "iteration 9180 loss 2.74222469329834, acc 26.5625\n",
      "iteration 9181 loss 2.616271495819092, acc 21.875\n",
      "iteration 9182 loss 2.9247045516967773, acc 15.625\n",
      "iteration 9183 loss 2.6925487518310547, acc 25.0\n",
      "iteration 9184 loss 2.8308613300323486, acc 15.625\n",
      "iteration 9185 loss 2.7684431076049805, acc 18.75\n",
      "iteration 9186 loss 2.8463618755340576, acc 15.625\n",
      "iteration 9187 loss 2.4577300548553467, acc 26.5625\n",
      "iteration 9188 loss 2.7679896354675293, acc 17.1875\n",
      "iteration 9189 loss 2.600084066390991, acc 20.3125\n",
      "iteration 9190 loss 2.773562431335449, acc 14.0625\n",
      "iteration 9191 loss 2.6265244483947754, acc 25.0\n",
      "iteration 9192 loss 2.6400909423828125, acc 29.6875\n",
      "iteration 9193 loss 2.5843734741210938, acc 31.25\n",
      "iteration 9194 loss 2.4073824882507324, acc 32.8125\n",
      "iteration 9195 loss 2.7143306732177734, acc 15.625\n",
      "iteration 9196 loss 2.6903650760650635, acc 28.125\n",
      "iteration 9197 loss 2.6994080543518066, acc 17.1875\n",
      "iteration 9198 loss 2.620584011077881, acc 25.0\n",
      "iteration 9199 loss 2.608272075653076, acc 18.75\n",
      "iteration 9200 loss 2.561600923538208, acc 23.4375\n",
      "iteration 9201 loss 2.7796545028686523, acc 20.3125\n",
      "iteration 9202 loss 2.546109676361084, acc 23.4375\n",
      "iteration 9203 loss 2.658522844314575, acc 14.0625\n",
      "iteration 9204 loss 2.7244718074798584, acc 17.1875\n",
      "iteration 9205 loss 2.565922498703003, acc 23.4375\n",
      "iteration 9206 loss 2.8506603240966797, acc 14.0625\n",
      "iteration 9207 loss 2.5239553451538086, acc 25.0\n",
      "iteration 9208 loss 2.682171583175659, acc 21.875\n",
      "iteration 9209 loss 2.919806480407715, acc 7.8125\n",
      "iteration 9210 loss 2.693023920059204, acc 17.1875\n",
      "iteration 9211 loss 2.631910800933838, acc 23.4375\n",
      "iteration 9212 loss 2.675840139389038, acc 18.75\n",
      "iteration 9213 loss 2.674062490463257, acc 15.625\n",
      "iteration 9214 loss 2.653559923171997, acc 18.75\n",
      "iteration 9215 loss 2.6273279190063477, acc 21.875\n",
      "iteration 9216 loss 2.638537645339966, acc 26.5625\n",
      "iteration 9217 loss 2.738351345062256, acc 18.75\n",
      "iteration 9218 loss 2.3559634685516357, acc 29.6875\n",
      "iteration 9219 loss 2.8195531368255615, acc 14.0625\n",
      "iteration 9220 loss 2.79709529876709, acc 18.75\n",
      "iteration 9221 loss 2.736417770385742, acc 20.3125\n",
      "iteration 9222 loss 2.7286293506622314, acc 23.4375\n",
      "iteration 9223 loss 2.7010908126831055, acc 14.0625\n",
      "iteration 9224 loss 2.8040311336517334, acc 18.75\n",
      "iteration 9225 loss 2.739654064178467, acc 14.0625\n",
      "iteration 9226 loss 2.7010176181793213, acc 21.875\n",
      "iteration 9227 loss 2.676985025405884, acc 28.125\n",
      "iteration 9228 loss 2.5689096450805664, acc 28.125\n",
      "iteration 9229 loss 2.46882700920105, acc 29.6875\n",
      "iteration 9230 loss 2.760392427444458, acc 20.3125\n",
      "iteration 9231 loss 2.730301856994629, acc 21.875\n",
      "iteration 9232 loss 2.924328327178955, acc 12.5\n",
      "iteration 9233 loss 2.5110793113708496, acc 21.875\n",
      "iteration 9234 loss 2.775097131729126, acc 23.4375\n",
      "iteration 9235 loss 2.7158689498901367, acc 20.3125\n",
      "iteration 9236 loss 2.6118643283843994, acc 15.625\n",
      "iteration 9237 loss 2.873859405517578, acc 18.75\n",
      "iteration 9238 loss 2.654470682144165, acc 23.4375\n",
      "iteration 9239 loss 2.4792072772979736, acc 29.6875\n",
      "iteration 9240 loss 2.681201219558716, acc 18.75\n",
      "iteration 9241 loss 2.6213369369506836, acc 21.875\n",
      "iteration 9242 loss 2.6406314373016357, acc 21.875\n",
      "iteration 9243 loss 2.657029628753662, acc 25.0\n",
      "iteration 9244 loss 2.7863948345184326, acc 18.75\n",
      "iteration 9245 loss 2.7160165309906006, acc 20.3125\n",
      "iteration 9246 loss 2.495245933532715, acc 28.125\n",
      "iteration 9247 loss 2.8360581398010254, acc 23.4375\n",
      "iteration 9248 loss 2.762399435043335, acc 20.3125\n",
      "iteration 9249 loss 2.8790838718414307, acc 14.0625\n",
      "iteration 9250 loss 2.681871175765991, acc 21.875\n",
      "iteration 9251 loss 2.7517998218536377, acc 25.0\n",
      "iteration 9252 loss 2.451786756515503, acc 20.3125\n",
      "iteration 9253 loss 2.737922191619873, acc 23.4375\n",
      "iteration 9254 loss 2.625281572341919, acc 29.6875\n",
      "iteration 9255 loss 2.593641757965088, acc 18.75\n",
      "iteration 9256 loss 2.831195592880249, acc 18.75\n",
      "iteration 9257 loss 2.851759672164917, acc 17.1875\n",
      "iteration 9258 loss 2.7007193565368652, acc 26.5625\n",
      "iteration 9259 loss 2.5174691677093506, acc 25.0\n",
      "iteration 9260 loss 2.7066330909729004, acc 26.5625\n",
      "iteration 9261 loss 2.6860427856445312, acc 17.1875\n",
      "iteration 9262 loss 2.724641799926758, acc 15.625\n",
      "iteration 9263 loss 2.6623051166534424, acc 20.3125\n",
      "iteration 9264 loss 2.5543456077575684, acc 26.5625\n",
      "iteration 9265 loss 2.5138845443725586, acc 26.5625\n",
      "iteration 9266 loss 2.8701038360595703, acc 18.75\n",
      "iteration 9267 loss 2.615304708480835, acc 20.3125\n",
      "iteration 9268 loss 2.476482391357422, acc 37.5\n",
      "iteration 9269 loss 2.5349559783935547, acc 26.5625\n",
      "iteration 9270 loss 2.600451946258545, acc 20.3125\n",
      "iteration 9271 loss 2.7130649089813232, acc 28.125\n",
      "iteration 9272 loss 2.4216320514678955, acc 29.6875\n",
      "iteration 9273 loss 2.6500632762908936, acc 21.875\n",
      "iteration 9274 loss 2.63020396232605, acc 23.4375\n",
      "iteration 9275 loss 2.6641058921813965, acc 17.1875\n",
      "iteration 9276 loss 2.6119344234466553, acc 23.4375\n",
      "iteration 9277 loss 2.5522830486297607, acc 23.4375\n",
      "iteration 9278 loss 2.613075017929077, acc 23.4375\n",
      "iteration 9279 loss 2.6534979343414307, acc 26.5625\n",
      "iteration 9280 loss 2.7809715270996094, acc 25.0\n",
      "iteration 9281 loss 2.7697718143463135, acc 15.625\n",
      "iteration 9282 loss 2.7578954696655273, acc 20.3125\n",
      "iteration 9283 loss 2.5694756507873535, acc 25.0\n",
      "iteration 9284 loss 2.6419405937194824, acc 25.0\n",
      "iteration 9285 loss 2.7916412353515625, acc 23.4375\n",
      "iteration 9286 loss 2.9502220153808594, acc 14.0625\n",
      "iteration 9287 loss 2.618407964706421, acc 21.875\n",
      "iteration 9288 loss 2.6689772605895996, acc 25.0\n",
      "iteration 9289 loss 2.780143976211548, acc 23.4375\n",
      "iteration 9290 loss 2.659228563308716, acc 26.5625\n",
      "iteration 9291 loss 2.6298344135284424, acc 23.4375\n",
      "iteration 9292 loss 2.7994792461395264, acc 15.625\n",
      "iteration 9293 loss 2.621298313140869, acc 26.5625\n",
      "iteration 9294 loss 2.836280584335327, acc 12.5\n",
      "iteration 9295 loss 2.705629825592041, acc 20.3125\n",
      "iteration 9296 loss 2.7612717151641846, acc 15.625\n",
      "iteration 9297 loss 2.768235206604004, acc 9.375\n",
      "iteration 9298 loss 2.790621757507324, acc 12.5\n",
      "iteration 9299 loss 2.447239637374878, acc 28.125\n",
      "iteration 9300 loss 2.726320266723633, acc 21.875\n",
      "iteration 9301 loss 2.799694061279297, acc 26.5625\n",
      "iteration 9302 loss 2.845074415206909, acc 14.0625\n",
      "iteration 9303 loss 2.824392557144165, acc 15.625\n",
      "iteration 9304 loss 2.4795267581939697, acc 28.125\n",
      "iteration 9305 loss 2.6526548862457275, acc 20.3125\n",
      "iteration 9306 loss 2.539804458618164, acc 29.6875\n",
      "iteration 9307 loss 2.59004545211792, acc 15.625\n",
      "iteration 9308 loss 2.8149313926696777, acc 12.5\n",
      "iteration 9309 loss 2.590049982070923, acc 21.875\n",
      "iteration 9310 loss 2.6913654804229736, acc 20.3125\n",
      "iteration 9311 loss 2.628739356994629, acc 14.0625\n",
      "iteration 9312 loss 2.6431684494018555, acc 20.3125\n",
      "iteration 9313 loss 2.5590810775756836, acc 25.0\n",
      "iteration 9314 loss 2.926612377166748, acc 12.5\n",
      "iteration 9315 loss 2.649569034576416, acc 23.4375\n",
      "iteration 9316 loss 2.6129558086395264, acc 21.875\n",
      "iteration 9317 loss 2.667527675628662, acc 25.0\n",
      "iteration 9318 loss 2.559196949005127, acc 23.4375\n",
      "iteration 9319 loss 2.6621696949005127, acc 21.875\n",
      "iteration 9320 loss 2.7270867824554443, acc 20.3125\n",
      "iteration 9321 loss 2.8185770511627197, acc 18.75\n",
      "iteration 9322 loss 2.706563711166382, acc 18.75\n",
      "iteration 9323 loss 2.5813021659851074, acc 29.6875\n",
      "iteration 9324 loss 2.6349868774414062, acc 18.75\n",
      "iteration 9325 loss 2.68430495262146, acc 26.5625\n",
      "iteration 9326 loss 2.877023220062256, acc 17.1875\n",
      "iteration 9327 loss 2.642446756362915, acc 18.75\n",
      "iteration 9328 loss 2.5218682289123535, acc 35.9375\n",
      "iteration 9329 loss 2.664243459701538, acc 28.125\n",
      "iteration 9330 loss 2.7120394706726074, acc 15.625\n",
      "iteration 9331 loss 2.7810733318328857, acc 15.625\n",
      "iteration 9332 loss 2.6112868785858154, acc 26.5625\n",
      "iteration 9333 loss 2.4764628410339355, acc 32.8125\n",
      "iteration 9334 loss 2.7546799182891846, acc 17.1875\n",
      "iteration 9335 loss 2.6858205795288086, acc 18.75\n",
      "iteration 9336 loss 2.8931851387023926, acc 18.75\n",
      "iteration 9337 loss 2.648249626159668, acc 15.625\n",
      "iteration 9338 loss 2.733158826828003, acc 23.4375\n",
      "iteration 9339 loss 2.585705518722534, acc 23.4375\n",
      "iteration 9340 loss 2.838489055633545, acc 23.4375\n",
      "iteration 9341 loss 2.7777798175811768, acc 20.3125\n",
      "iteration 9342 loss 2.5170998573303223, acc 26.5625\n",
      "iteration 9343 loss 2.7821624279022217, acc 18.75\n",
      "iteration 9344 loss 2.637246608734131, acc 18.75\n",
      "iteration 9345 loss 2.5803463459014893, acc 28.125\n",
      "iteration 9346 loss 2.886850118637085, acc 17.1875\n",
      "iteration 9347 loss 2.8542306423187256, acc 28.125\n",
      "iteration 9348 loss 2.647611141204834, acc 28.125\n",
      "iteration 9349 loss 2.5914902687072754, acc 21.875\n",
      "iteration 9350 loss 2.5510995388031006, acc 28.125\n",
      "iteration 9351 loss 2.6101458072662354, acc 17.1875\n",
      "iteration 9352 loss 2.8716835975646973, acc 17.1875\n",
      "iteration 9353 loss 2.7082533836364746, acc 23.4375\n",
      "iteration 9354 loss 2.814258098602295, acc 18.75\n",
      "iteration 9355 loss 2.6069071292877197, acc 20.3125\n",
      "iteration 9356 loss 2.772221803665161, acc 17.1875\n",
      "iteration 9357 loss 2.5393896102905273, acc 21.875\n",
      "iteration 9358 loss 2.6345818042755127, acc 14.0625\n",
      "iteration 9359 loss 2.6178784370422363, acc 25.0\n",
      "iteration 9360 loss 2.679487943649292, acc 25.0\n",
      "iteration 9361 loss 2.763876438140869, acc 20.3125\n",
      "iteration 9362 loss 2.6523313522338867, acc 21.875\n",
      "iteration 9363 loss 2.555065155029297, acc 26.5625\n",
      "iteration 9364 loss 2.5610013008117676, acc 29.6875\n",
      "iteration 9365 loss 2.6148529052734375, acc 26.5625\n",
      "iteration 9366 loss 2.79402494430542, acc 28.125\n",
      "iteration 9367 loss 2.7122881412506104, acc 25.0\n",
      "iteration 9368 loss 2.785259485244751, acc 17.1875\n",
      "iteration 9369 loss 2.676405191421509, acc 10.9375\n",
      "iteration 9370 loss 2.579535961151123, acc 31.25\n",
      "iteration 9371 loss 2.6100807189941406, acc 26.5625\n",
      "iteration 9372 loss 2.6766135692596436, acc 18.75\n",
      "iteration 9373 loss 2.566290855407715, acc 26.5625\n",
      "iteration 9374 loss 2.6207268238067627, acc 26.5625\n",
      "iteration 9375 loss 2.8140127658843994, acc 23.4375\n",
      "iteration 9376 loss 2.8051669597625732, acc 15.625\n",
      "iteration 9377 loss 2.635643243789673, acc 25.0\n",
      "iteration 9378 loss 2.566509962081909, acc 21.875\n",
      "iteration 9379 loss 2.584200620651245, acc 25.0\n",
      "iteration 9380 loss 2.4678876399993896, acc 25.0\n",
      "iteration 9381 loss 2.6373894214630127, acc 20.3125\n",
      "iteration 9382 loss 2.7688701152801514, acc 25.0\n",
      "iteration 9383 loss 2.74936580657959, acc 28.125\n",
      "iteration 9384 loss 2.8503592014312744, acc 12.5\n",
      "iteration 9385 loss 2.778811454772949, acc 23.4375\n",
      "iteration 9386 loss 2.6488921642303467, acc 14.0625\n",
      "iteration 9387 loss 2.6434645652770996, acc 21.875\n",
      "iteration 9388 loss 2.7280948162078857, acc 23.4375\n",
      "iteration 9389 loss 2.5976908206939697, acc 25.0\n",
      "iteration 9390 loss 3.163296699523926, acc 12.5\n",
      "iteration 9391 loss 2.5232155323028564, acc 26.5625\n",
      "iteration 9392 loss 2.757174015045166, acc 20.3125\n",
      "iteration 9393 loss 2.523678779602051, acc 29.6875\n",
      "iteration 9394 loss 2.677457332611084, acc 21.875\n",
      "iteration 9395 loss 2.77795147895813, acc 17.1875\n",
      "iteration 9396 loss 2.8177196979522705, acc 21.875\n",
      "iteration 9397 loss 2.432112693786621, acc 35.9375\n",
      "iteration 9398 loss 2.6971635818481445, acc 20.3125\n",
      "iteration 9399 loss 2.7302279472351074, acc 20.3125\n",
      "iteration 9400 loss 2.5988965034484863, acc 26.5625\n",
      "iteration 9401 loss 2.7909770011901855, acc 18.75\n",
      "iteration 9402 loss 2.9441733360290527, acc 20.3125\n",
      "iteration 9403 loss 2.6155593395233154, acc 28.125\n",
      "iteration 9404 loss 2.5374438762664795, acc 29.6875\n",
      "iteration 9405 loss 2.5887343883514404, acc 18.75\n",
      "iteration 9406 loss 2.561350107192993, acc 20.3125\n",
      "iteration 9407 loss 2.658884048461914, acc 17.1875\n",
      "iteration 9408 loss 2.6898932456970215, acc 14.0625\n",
      "iteration 9409 loss 2.5772459506988525, acc 28.125\n",
      "iteration 9410 loss 2.848750591278076, acc 20.3125\n",
      "iteration 9411 loss 2.8294034004211426, acc 15.625\n",
      "iteration 9412 loss 2.9515459537506104, acc 14.0625\n",
      "iteration 9413 loss 2.6935904026031494, acc 18.75\n",
      "iteration 9414 loss 2.8013968467712402, acc 10.9375\n",
      "iteration 9415 loss 2.6103341579437256, acc 28.125\n",
      "iteration 9416 loss 2.599210023880005, acc 26.5625\n",
      "iteration 9417 loss 2.5575485229492188, acc 31.25\n",
      "iteration 9418 loss 2.729130744934082, acc 17.1875\n",
      "iteration 9419 loss 2.720475196838379, acc 21.875\n",
      "iteration 9420 loss 2.694103240966797, acc 18.75\n",
      "iteration 9421 loss 2.7451562881469727, acc 18.75\n",
      "iteration 9422 loss 2.791181802749634, acc 18.75\n",
      "iteration 9423 loss 2.5685067176818848, acc 26.5625\n",
      "iteration 9424 loss 2.7262656688690186, acc 18.75\n",
      "iteration 9425 loss 2.9482100009918213, acc 12.5\n",
      "iteration 9426 loss 2.4455442428588867, acc 31.25\n",
      "iteration 9427 loss 2.549229145050049, acc 26.5625\n",
      "iteration 9428 loss 2.6091225147247314, acc 23.4375\n",
      "iteration 9429 loss 2.640883207321167, acc 23.4375\n",
      "iteration 9430 loss 2.6673715114593506, acc 18.75\n",
      "iteration 9431 loss 2.6378860473632812, acc 20.3125\n",
      "iteration 9432 loss 2.482433319091797, acc 31.25\n",
      "iteration 9433 loss 2.552450656890869, acc 25.0\n",
      "iteration 9434 loss 2.8506059646606445, acc 18.75\n",
      "iteration 9435 loss 2.5864319801330566, acc 25.0\n",
      "iteration 9436 loss 2.7969017028808594, acc 17.1875\n",
      "iteration 9437 loss 2.664560079574585, acc 10.9375\n",
      "iteration 9438 loss 2.4688236713409424, acc 28.125\n",
      "iteration 9439 loss 2.44038462638855, acc 25.0\n",
      "iteration 9440 loss 2.5087859630584717, acc 29.6875\n",
      "iteration 9441 loss 2.6564600467681885, acc 18.75\n",
      "iteration 9442 loss 2.655158519744873, acc 20.3125\n",
      "iteration 9443 loss 2.8509976863861084, acc 12.5\n",
      "iteration 9444 loss 2.59116268157959, acc 23.4375\n",
      "iteration 9445 loss 2.760167121887207, acc 20.3125\n",
      "iteration 9446 loss 2.603688955307007, acc 18.75\n",
      "iteration 9447 loss 2.6373651027679443, acc 29.6875\n",
      "iteration 9448 loss 2.7239344120025635, acc 18.75\n",
      "iteration 9449 loss 2.749376058578491, acc 20.3125\n",
      "iteration 9450 loss 3.015486001968384, acc 15.625\n",
      "iteration 9451 loss 2.7374696731567383, acc 17.1875\n",
      "iteration 9452 loss 2.5872273445129395, acc 23.4375\n",
      "iteration 9453 loss 2.527615785598755, acc 28.125\n",
      "iteration 9454 loss 2.6572048664093018, acc 18.75\n",
      "iteration 9455 loss 2.6905667781829834, acc 23.4375\n",
      "iteration 9456 loss 2.726494789123535, acc 25.0\n",
      "iteration 9457 loss 2.677893877029419, acc 17.1875\n",
      "iteration 9458 loss 2.8298544883728027, acc 21.875\n",
      "iteration 9459 loss 2.8376669883728027, acc 20.3125\n",
      "iteration 9460 loss 2.732100248336792, acc 17.1875\n",
      "iteration 9461 loss 2.641174793243408, acc 25.0\n",
      "iteration 9462 loss 2.635354518890381, acc 20.3125\n",
      "iteration 9463 loss 2.6087353229522705, acc 25.0\n",
      "iteration 9464 loss 2.618089199066162, acc 23.4375\n",
      "iteration 9465 loss 2.5825114250183105, acc 18.75\n",
      "iteration 9466 loss 2.6978275775909424, acc 17.1875\n",
      "iteration 9467 loss 2.684718608856201, acc 17.1875\n",
      "iteration 9468 loss 2.637197256088257, acc 18.75\n",
      "iteration 9469 loss 2.870419979095459, acc 18.75\n",
      "iteration 9470 loss 2.8847315311431885, acc 12.5\n",
      "iteration 9471 loss 2.768463611602783, acc 23.4375\n",
      "iteration 9472 loss 2.5928242206573486, acc 23.4375\n",
      "iteration 9473 loss 2.7665553092956543, acc 15.625\n",
      "iteration 9474 loss 2.615654945373535, acc 28.125\n",
      "iteration 9475 loss 2.4854133129119873, acc 25.0\n",
      "iteration 9476 loss 2.4872171878814697, acc 25.0\n",
      "iteration 9477 loss 2.6778199672698975, acc 23.4375\n",
      "iteration 9478 loss 2.707669258117676, acc 17.1875\n",
      "iteration 9479 loss 2.6600708961486816, acc 25.0\n",
      "iteration 9480 loss 2.6756601333618164, acc 26.5625\n",
      "iteration 9481 loss 2.6690642833709717, acc 25.0\n",
      "iteration 9482 loss 2.69811749458313, acc 23.4375\n",
      "iteration 9483 loss 2.883373737335205, acc 20.3125\n",
      "iteration 9484 loss 2.7223174571990967, acc 15.625\n",
      "iteration 9485 loss 2.759247064590454, acc 20.3125\n",
      "iteration 9486 loss 2.5018482208251953, acc 35.9375\n",
      "iteration 9487 loss 2.638981580734253, acc 21.875\n",
      "iteration 9488 loss 2.620478630065918, acc 26.5625\n",
      "iteration 9489 loss 2.8889918327331543, acc 10.9375\n",
      "iteration 9490 loss 2.5063931941986084, acc 28.125\n",
      "iteration 9491 loss 2.815185070037842, acc 15.625\n",
      "iteration 9492 loss 2.964418649673462, acc 18.75\n",
      "iteration 9493 loss 2.53346848487854, acc 28.125\n",
      "iteration 9494 loss 2.6636462211608887, acc 28.125\n",
      "iteration 9495 loss 2.3568365573883057, acc 29.6875\n",
      "iteration 9496 loss 2.6353611946105957, acc 18.75\n",
      "iteration 9497 loss 2.746457815170288, acc 21.875\n",
      "iteration 9498 loss 2.483142614364624, acc 32.8125\n",
      "iteration 9499 loss 2.841616153717041, acc 15.625\n",
      "iteration 9500 loss 2.7600765228271484, acc 26.5625\n",
      "iteration 9501 loss 2.567850351333618, acc 26.5625\n",
      "iteration 9502 loss 2.765575408935547, acc 21.875\n",
      "iteration 9503 loss 2.675267457962036, acc 28.125\n",
      "iteration 9504 loss 2.8322205543518066, acc 12.5\n",
      "iteration 9505 loss 2.833041191101074, acc 17.1875\n",
      "iteration 9506 loss 2.66129994392395, acc 20.3125\n",
      "iteration 9507 loss 2.807429075241089, acc 21.875\n",
      "iteration 9508 loss 2.7595438957214355, acc 15.625\n",
      "iteration 9509 loss 2.6324729919433594, acc 29.6875\n",
      "iteration 9510 loss 2.7784290313720703, acc 14.0625\n",
      "iteration 9511 loss 2.5903069972991943, acc 21.875\n",
      "iteration 9512 loss 2.728572130203247, acc 18.75\n",
      "iteration 9513 loss 2.636455535888672, acc 21.875\n",
      "iteration 9514 loss 2.525228977203369, acc 21.875\n",
      "iteration 9515 loss 2.781191110610962, acc 29.6875\n",
      "iteration 9516 loss 2.7546422481536865, acc 18.75\n",
      "iteration 9517 loss 2.72064471244812, acc 15.625\n",
      "iteration 9518 loss 2.600728750228882, acc 23.4375\n",
      "iteration 9519 loss 2.8985695838928223, acc 17.1875\n",
      "iteration 9520 loss 2.5953176021575928, acc 28.125\n",
      "iteration 9521 loss 2.8000175952911377, acc 20.3125\n",
      "iteration 9522 loss 2.65657377243042, acc 25.0\n",
      "iteration 9523 loss 2.675875663757324, acc 25.0\n",
      "iteration 9524 loss 2.3051581382751465, acc 29.6875\n",
      "iteration 9525 loss 2.7427432537078857, acc 18.75\n",
      "iteration 9526 loss 2.736905813217163, acc 23.4375\n",
      "iteration 9527 loss 2.7168657779693604, acc 21.875\n",
      "iteration 9528 loss 2.8095083236694336, acc 17.1875\n",
      "iteration 9529 loss 2.8703696727752686, acc 18.75\n",
      "iteration 9530 loss 2.7635600566864014, acc 20.3125\n",
      "iteration 9531 loss 2.673713445663452, acc 17.1875\n",
      "iteration 9532 loss 2.8853328227996826, acc 15.625\n",
      "iteration 9533 loss 2.913238286972046, acc 15.625\n",
      "iteration 9534 loss 2.6481611728668213, acc 20.3125\n",
      "iteration 9535 loss 2.780545949935913, acc 20.3125\n",
      "iteration 9536 loss 2.5951502323150635, acc 29.6875\n",
      "iteration 9537 loss 2.5621590614318848, acc 14.0625\n",
      "iteration 9538 loss 2.644503593444824, acc 21.875\n",
      "iteration 9539 loss 2.639186382293701, acc 18.75\n",
      "iteration 9540 loss 2.6964361667633057, acc 17.1875\n",
      "iteration 9541 loss 2.6244306564331055, acc 17.1875\n",
      "iteration 9542 loss 2.656601667404175, acc 26.5625\n",
      "iteration 9543 loss 2.665564775466919, acc 26.5625\n",
      "iteration 9544 loss 2.617281675338745, acc 28.125\n",
      "iteration 9545 loss 2.739683151245117, acc 26.5625\n",
      "iteration 9546 loss 2.5808188915252686, acc 29.6875\n",
      "iteration 9547 loss 2.5742380619049072, acc 20.3125\n",
      "iteration 9548 loss 2.659573554992676, acc 18.75\n",
      "iteration 9549 loss 2.661177158355713, acc 21.875\n",
      "iteration 9550 loss 2.5408639907836914, acc 28.125\n",
      "iteration 9551 loss 2.6817595958709717, acc 20.3125\n",
      "iteration 9552 loss 2.60290789604187, acc 28.125\n",
      "iteration 9553 loss 2.6253678798675537, acc 21.875\n",
      "iteration 9554 loss 2.753570556640625, acc 23.4375\n",
      "iteration 9555 loss 2.732022285461426, acc 15.625\n",
      "iteration 9556 loss 2.4284133911132812, acc 23.4375\n",
      "iteration 9557 loss 2.605294704437256, acc 31.25\n",
      "iteration 9558 loss 2.8537609577178955, acc 23.4375\n",
      "iteration 9559 loss 2.7300920486450195, acc 25.0\n",
      "iteration 9560 loss 2.6822667121887207, acc 21.875\n",
      "iteration 9561 loss 2.66110897064209, acc 26.5625\n",
      "iteration 9562 loss 2.655978202819824, acc 17.1875\n",
      "iteration 9563 loss 2.5825142860412598, acc 23.4375\n",
      "iteration 9564 loss 2.8325321674346924, acc 15.625\n",
      "iteration 9565 loss 2.950193166732788, acc 20.3125\n",
      "iteration 9566 loss 2.88287091255188, acc 21.875\n",
      "iteration 9567 loss 2.596095323562622, acc 21.875\n",
      "iteration 9568 loss 2.6137192249298096, acc 25.0\n",
      "iteration 9569 loss 2.6371841430664062, acc 17.1875\n",
      "iteration 9570 loss 2.5725958347320557, acc 32.8125\n",
      "iteration 9571 loss 2.60764479637146, acc 26.5625\n",
      "iteration 9572 loss 2.7802820205688477, acc 21.875\n",
      "iteration 9573 loss 2.4654908180236816, acc 26.5625\n",
      "iteration 9574 loss 2.5679357051849365, acc 18.75\n",
      "iteration 9575 loss 2.5092275142669678, acc 23.4375\n",
      "iteration 9576 loss 2.6183276176452637, acc 20.3125\n",
      "iteration 9577 loss 2.782689094543457, acc 14.0625\n",
      "iteration 9578 loss 2.9765427112579346, acc 15.625\n",
      "iteration 9579 loss 2.7184700965881348, acc 17.1875\n",
      "iteration 9580 loss 2.7216458320617676, acc 20.3125\n",
      "iteration 9581 loss 2.456756114959717, acc 29.6875\n",
      "iteration 9582 loss 2.7486488819122314, acc 25.0\n",
      "iteration 9583 loss 2.551103115081787, acc 25.0\n",
      "iteration 9584 loss 2.8119568824768066, acc 17.1875\n",
      "iteration 9585 loss 2.7036001682281494, acc 18.75\n",
      "iteration 9586 loss 2.5488290786743164, acc 28.125\n",
      "iteration 9587 loss 2.771550178527832, acc 20.3125\n",
      "iteration 9588 loss 2.851902961730957, acc 15.625\n",
      "iteration 9589 loss 2.551548719406128, acc 28.125\n",
      "iteration 9590 loss 2.7767531871795654, acc 15.625\n",
      "iteration 9591 loss 2.76715087890625, acc 18.75\n",
      "iteration 9592 loss 2.726120948791504, acc 17.1875\n",
      "iteration 9593 loss 2.713473081588745, acc 20.3125\n",
      "iteration 9594 loss 2.7077832221984863, acc 23.4375\n",
      "iteration 9595 loss 2.662726879119873, acc 28.125\n",
      "iteration 9596 loss 2.7521097660064697, acc 18.75\n",
      "iteration 9597 loss 2.704702615737915, acc 12.5\n",
      "iteration 9598 loss 2.5477187633514404, acc 20.3125\n",
      "iteration 9599 loss 2.8918676376342773, acc 15.625\n",
      "iteration 9600 loss 2.5583438873291016, acc 15.625\n",
      "iteration 9601 loss 2.6900241374969482, acc 29.6875\n",
      "iteration 9602 loss 2.5876636505126953, acc 20.3125\n",
      "iteration 9603 loss 2.5754170417785645, acc 26.5625\n",
      "iteration 9604 loss 2.4150490760803223, acc 31.25\n",
      "iteration 9605 loss 2.502601146697998, acc 29.6875\n",
      "iteration 9606 loss 2.6387617588043213, acc 31.25\n",
      "iteration 9607 loss 2.7696633338928223, acc 15.625\n",
      "iteration 9608 loss 2.6547446250915527, acc 17.1875\n",
      "iteration 9609 loss 2.6219067573547363, acc 21.875\n",
      "iteration 9610 loss 2.7173497676849365, acc 15.625\n",
      "iteration 9611 loss 2.541248321533203, acc 28.125\n",
      "iteration 9612 loss 2.6681711673736572, acc 29.6875\n",
      "iteration 9613 loss 2.8650734424591064, acc 12.5\n",
      "iteration 9614 loss 2.9677906036376953, acc 17.1875\n",
      "iteration 9615 loss 2.7469637393951416, acc 18.75\n",
      "iteration 9616 loss 2.638519763946533, acc 20.3125\n",
      "iteration 9617 loss 2.499274730682373, acc 31.25\n",
      "iteration 9618 loss 2.658642530441284, acc 23.4375\n",
      "iteration 9619 loss 2.7163867950439453, acc 28.125\n",
      "iteration 9620 loss 2.7161614894866943, acc 23.4375\n",
      "iteration 9621 loss 2.6197657585144043, acc 18.75\n",
      "iteration 9622 loss 2.75012469291687, acc 28.125\n",
      "iteration 9623 loss 2.611226797103882, acc 15.625\n",
      "iteration 9624 loss 2.567042350769043, acc 20.3125\n",
      "iteration 9625 loss 2.7212235927581787, acc 25.0\n",
      "iteration 9626 loss 2.6539859771728516, acc 20.3125\n",
      "iteration 9627 loss 2.6351025104522705, acc 28.125\n",
      "iteration 9628 loss 2.485687255859375, acc 29.6875\n",
      "iteration 9629 loss 2.4828944206237793, acc 28.125\n",
      "iteration 9630 loss 2.666813611984253, acc 28.125\n",
      "iteration 9631 loss 2.499483108520508, acc 25.0\n",
      "iteration 9632 loss 2.686391592025757, acc 25.0\n",
      "iteration 9633 loss 2.60970139503479, acc 29.6875\n",
      "iteration 9634 loss 2.7859299182891846, acc 18.75\n",
      "iteration 9635 loss 2.75186824798584, acc 17.1875\n",
      "iteration 9636 loss 2.6315667629241943, acc 20.3125\n",
      "iteration 9637 loss 2.5739457607269287, acc 12.5\n",
      "iteration 9638 loss 2.5357093811035156, acc 25.0\n",
      "iteration 9639 loss 2.4021008014678955, acc 32.8125\n",
      "iteration 9640 loss 2.684199094772339, acc 20.3125\n",
      "iteration 9641 loss 2.618016004562378, acc 21.875\n",
      "iteration 9642 loss 2.709974527359009, acc 25.0\n",
      "iteration 9643 loss 2.528916597366333, acc 28.125\n",
      "iteration 9644 loss 2.63265323638916, acc 23.4375\n",
      "iteration 9645 loss 2.5006632804870605, acc 25.0\n",
      "iteration 9646 loss 2.866576671600342, acc 20.3125\n",
      "iteration 9647 loss 2.819545030593872, acc 18.75\n",
      "iteration 9648 loss 2.884753942489624, acc 18.75\n",
      "iteration 9649 loss 2.770287275314331, acc 25.0\n",
      "iteration 9650 loss 2.6815435886383057, acc 20.3125\n",
      "iteration 9651 loss 2.760993242263794, acc 14.0625\n",
      "iteration 9652 loss 2.613757610321045, acc 25.0\n",
      "iteration 9653 loss 2.619903802871704, acc 28.125\n",
      "iteration 9654 loss 2.5961179733276367, acc 25.0\n",
      "iteration 9655 loss 2.653179407119751, acc 20.3125\n",
      "iteration 9656 loss 2.8116793632507324, acc 17.1875\n",
      "iteration 9657 loss 2.8532121181488037, acc 18.75\n",
      "iteration 9658 loss 2.769718885421753, acc 14.0625\n",
      "iteration 9659 loss 2.727914810180664, acc 20.3125\n",
      "iteration 9660 loss 2.7222840785980225, acc 28.125\n",
      "iteration 9661 loss 2.6789565086364746, acc 26.5625\n",
      "iteration 9662 loss 2.603370189666748, acc 18.75\n",
      "iteration 9663 loss 2.7542996406555176, acc 21.875\n",
      "iteration 9664 loss 2.915898561477661, acc 17.1875\n",
      "iteration 9665 loss 2.7939603328704834, acc 15.625\n",
      "iteration 9666 loss 2.781860113143921, acc 25.0\n",
      "iteration 9667 loss 2.6390368938446045, acc 26.5625\n",
      "iteration 9668 loss 2.6863505840301514, acc 18.75\n",
      "iteration 9669 loss 2.721860885620117, acc 21.875\n",
      "iteration 9670 loss 2.55614972114563, acc 25.0\n",
      "iteration 9671 loss 2.4816994667053223, acc 26.5625\n",
      "iteration 9672 loss 2.539036750793457, acc 25.0\n",
      "iteration 9673 loss 2.8275184631347656, acc 17.1875\n",
      "iteration 9674 loss 2.7193667888641357, acc 20.3125\n",
      "iteration 9675 loss 2.6448261737823486, acc 18.75\n",
      "iteration 9676 loss 2.5077996253967285, acc 25.0\n",
      "iteration 9677 loss 2.5482544898986816, acc 21.875\n",
      "iteration 9678 loss 2.6631436347961426, acc 21.875\n",
      "iteration 9679 loss 2.647850275039673, acc 20.3125\n",
      "iteration 9680 loss 2.549508571624756, acc 26.5625\n",
      "iteration 9681 loss 2.6267471313476562, acc 25.0\n",
      "iteration 9682 loss 2.549853801727295, acc 28.125\n",
      "iteration 9683 loss 2.862727165222168, acc 10.9375\n",
      "iteration 9684 loss 2.568047046661377, acc 21.875\n",
      "iteration 9685 loss 2.7838854789733887, acc 17.1875\n",
      "iteration 9686 loss 2.789494752883911, acc 21.875\n",
      "iteration 9687 loss 2.6452786922454834, acc 26.5625\n",
      "iteration 9688 loss 2.532097578048706, acc 23.4375\n",
      "iteration 9689 loss 2.9191346168518066, acc 14.0625\n",
      "iteration 9690 loss 2.589505434036255, acc 26.5625\n",
      "iteration 9691 loss 2.715837240219116, acc 23.4375\n",
      "iteration 9692 loss 2.537680149078369, acc 25.0\n",
      "iteration 9693 loss 2.625544786453247, acc 31.25\n",
      "iteration 9694 loss 2.8229284286499023, acc 18.75\n",
      "iteration 9695 loss 2.659689426422119, acc 21.875\n",
      "iteration 9696 loss 2.6028172969818115, acc 18.75\n",
      "iteration 9697 loss 2.6956980228424072, acc 12.5\n",
      "iteration 9698 loss 2.872882843017578, acc 18.75\n",
      "iteration 9699 loss 2.777045249938965, acc 23.4375\n",
      "iteration 9700 loss 2.825159788131714, acc 20.3125\n",
      "iteration 9701 loss 2.684356451034546, acc 20.3125\n",
      "iteration 9702 loss 2.614509105682373, acc 21.875\n",
      "iteration 9703 loss 2.683176040649414, acc 17.1875\n",
      "iteration 9704 loss 2.563326835632324, acc 23.4375\n",
      "iteration 9705 loss 2.797966241836548, acc 23.4375\n",
      "iteration 9706 loss 2.5332143306732178, acc 23.4375\n",
      "iteration 9707 loss 2.5614712238311768, acc 28.125\n",
      "iteration 9708 loss 2.4938340187072754, acc 25.0\n",
      "iteration 9709 loss 2.430795192718506, acc 34.375\n",
      "iteration 9710 loss 2.481968879699707, acc 26.5625\n",
      "iteration 9711 loss 2.8629791736602783, acc 9.375\n",
      "iteration 9712 loss 2.83402419090271, acc 18.75\n",
      "iteration 9713 loss 2.7472519874572754, acc 23.4375\n",
      "iteration 9714 loss 2.786928176879883, acc 21.875\n",
      "iteration 9715 loss 2.7482924461364746, acc 17.1875\n",
      "iteration 9716 loss 2.8794243335723877, acc 18.75\n",
      "iteration 9717 loss 2.609722375869751, acc 17.1875\n",
      "iteration 9718 loss 2.5783088207244873, acc 26.5625\n",
      "iteration 9719 loss 2.704866886138916, acc 17.1875\n",
      "iteration 9720 loss 2.661756992340088, acc 21.875\n",
      "iteration 9721 loss 2.6069772243499756, acc 31.25\n",
      "iteration 9722 loss 2.5232980251312256, acc 21.875\n",
      "iteration 9723 loss 2.5795493125915527, acc 26.5625\n",
      "iteration 9724 loss 2.706427812576294, acc 17.1875\n",
      "iteration 9725 loss 2.8507957458496094, acc 25.0\n",
      "iteration 9726 loss 2.7706592082977295, acc 15.625\n",
      "iteration 9727 loss 2.562635660171509, acc 17.1875\n",
      "iteration 9728 loss 2.6375319957733154, acc 14.0625\n",
      "iteration 9729 loss 2.7671279907226562, acc 20.3125\n",
      "iteration 9730 loss 2.9111721515655518, acc 18.75\n",
      "iteration 9731 loss 2.8180365562438965, acc 17.1875\n",
      "iteration 9732 loss 2.626150369644165, acc 23.4375\n",
      "iteration 9733 loss 2.7213711738586426, acc 25.0\n",
      "iteration 9734 loss 2.690742015838623, acc 21.875\n",
      "iteration 9735 loss 2.6625490188598633, acc 17.1875\n",
      "iteration 9736 loss 2.6417272090911865, acc 17.1875\n",
      "iteration 9737 loss 2.646613121032715, acc 21.875\n",
      "iteration 9738 loss 2.764070510864258, acc 25.0\n",
      "iteration 9739 loss 2.710233211517334, acc 18.75\n",
      "iteration 9740 loss 2.828059434890747, acc 15.625\n",
      "iteration 9741 loss 2.604729652404785, acc 21.875\n",
      "iteration 9742 loss 2.7207889556884766, acc 21.875\n",
      "iteration 9743 loss 2.6180241107940674, acc 18.75\n",
      "iteration 9744 loss 2.561593770980835, acc 26.5625\n",
      "iteration 9745 loss 2.5223841667175293, acc 25.0\n",
      "iteration 9746 loss 2.820665121078491, acc 20.3125\n",
      "iteration 9747 loss 2.7464957237243652, acc 23.4375\n",
      "iteration 9748 loss 2.704808473587036, acc 23.4375\n",
      "iteration 9749 loss 2.7695062160491943, acc 15.625\n",
      "iteration 9750 loss 2.615152359008789, acc 12.5\n",
      "iteration 9751 loss 2.9147121906280518, acc 20.3125\n",
      "iteration 9752 loss 2.5860488414764404, acc 26.5625\n",
      "iteration 9753 loss 2.5124261379241943, acc 23.4375\n",
      "iteration 9754 loss 2.58056378364563, acc 21.875\n",
      "iteration 9755 loss 2.60123348236084, acc 23.4375\n",
      "iteration 9756 loss 2.600440740585327, acc 25.0\n",
      "iteration 9757 loss 2.6000938415527344, acc 23.4375\n",
      "iteration 9758 loss 2.9249861240386963, acc 12.5\n",
      "iteration 9759 loss 2.5175435543060303, acc 31.25\n",
      "iteration 9760 loss 2.7283413410186768, acc 21.875\n",
      "iteration 9761 loss 2.6992239952087402, acc 15.625\n",
      "iteration 9762 loss 2.5394086837768555, acc 28.125\n",
      "iteration 9763 loss 2.621422052383423, acc 21.875\n",
      "iteration 9764 loss 2.646590232849121, acc 14.0625\n",
      "iteration 9765 loss 2.4916834831237793, acc 28.125\n",
      "iteration 9766 loss 2.6091156005859375, acc 25.0\n",
      "iteration 9767 loss 2.6757020950317383, acc 26.5625\n",
      "iteration 9768 loss 2.653750419616699, acc 20.3125\n",
      "iteration 9769 loss 2.9941022396087646, acc 20.3125\n",
      "iteration 9770 loss 2.8409554958343506, acc 17.1875\n",
      "iteration 9771 loss 2.8105459213256836, acc 15.625\n",
      "iteration 9772 loss 2.640214443206787, acc 17.1875\n",
      "iteration 9773 loss 2.8710777759552, acc 21.875\n",
      "iteration 9774 loss 2.6213347911834717, acc 21.875\n",
      "iteration 9775 loss 2.494523763656616, acc 32.8125\n",
      "iteration 9776 loss 2.726506233215332, acc 23.4375\n",
      "iteration 9777 loss 2.7687366008758545, acc 14.0625\n",
      "iteration 9778 loss 2.788238525390625, acc 23.4375\n",
      "iteration 9779 loss 2.5411386489868164, acc 23.4375\n",
      "iteration 9780 loss 2.7127344608306885, acc 21.875\n",
      "iteration 9781 loss 2.5343377590179443, acc 28.125\n",
      "iteration 9782 loss 2.6742091178894043, acc 28.125\n",
      "iteration 9783 loss 2.9370334148406982, acc 17.1875\n",
      "iteration 9784 loss 2.7019057273864746, acc 21.875\n",
      "iteration 9785 loss 2.6575138568878174, acc 29.6875\n",
      "iteration 9786 loss 2.8340516090393066, acc 20.3125\n",
      "iteration 9787 loss 2.829263210296631, acc 14.0625\n",
      "iteration 9788 loss 2.8901326656341553, acc 20.3125\n",
      "iteration 9789 loss 2.33208966255188, acc 37.5\n",
      "iteration 9790 loss 2.8225083351135254, acc 9.375\n",
      "iteration 9791 loss 2.84470272064209, acc 14.0625\n",
      "iteration 9792 loss 2.6201348304748535, acc 26.5625\n",
      "iteration 9793 loss 2.810732364654541, acc 17.1875\n",
      "iteration 9794 loss 2.795445442199707, acc 18.75\n",
      "iteration 9795 loss 2.6447792053222656, acc 25.0\n",
      "iteration 9796 loss 2.7937750816345215, acc 17.1875\n",
      "iteration 9797 loss 2.7067830562591553, acc 21.875\n",
      "iteration 9798 loss 2.8085811138153076, acc 17.1875\n",
      "iteration 9799 loss 2.6580944061279297, acc 23.4375\n",
      "iteration 9800 loss 2.533008337020874, acc 29.6875\n",
      "iteration 9801 loss 2.829620599746704, acc 15.625\n",
      "iteration 9802 loss 2.769105911254883, acc 18.75\n",
      "iteration 9803 loss 2.6591579914093018, acc 21.875\n",
      "iteration 9804 loss 2.5033459663391113, acc 28.125\n",
      "iteration 9805 loss 2.8219475746154785, acc 15.625\n",
      "iteration 9806 loss 2.752575397491455, acc 15.625\n",
      "iteration 9807 loss 2.625819206237793, acc 21.875\n",
      "iteration 9808 loss 2.679199695587158, acc 21.875\n",
      "iteration 9809 loss 2.6063637733459473, acc 15.625\n",
      "iteration 9810 loss 2.6311726570129395, acc 20.3125\n",
      "iteration 9811 loss 2.6078739166259766, acc 28.125\n",
      "iteration 9812 loss 2.6762938499450684, acc 31.25\n",
      "iteration 9813 loss 2.665600061416626, acc 17.1875\n",
      "iteration 9814 loss 2.559551954269409, acc 23.4375\n",
      "iteration 9815 loss 2.809605836868286, acc 18.75\n",
      "iteration 9816 loss 2.617804527282715, acc 31.25\n",
      "iteration 9817 loss 2.3768293857574463, acc 29.6875\n",
      "iteration 9818 loss 2.727229595184326, acc 17.1875\n",
      "iteration 9819 loss 2.6959898471832275, acc 15.625\n",
      "iteration 9820 loss 2.525489091873169, acc 25.0\n",
      "iteration 9821 loss 2.8137357234954834, acc 14.0625\n",
      "iteration 9822 loss 2.5393450260162354, acc 32.8125\n",
      "iteration 9823 loss 2.5132319927215576, acc 26.5625\n",
      "iteration 9824 loss 2.5903451442718506, acc 26.5625\n",
      "iteration 9825 loss 2.5952136516571045, acc 23.4375\n",
      "iteration 9826 loss 2.7110724449157715, acc 21.875\n",
      "iteration 9827 loss 2.752631425857544, acc 18.75\n",
      "iteration 9828 loss 2.806636333465576, acc 18.75\n",
      "iteration 9829 loss 2.6338412761688232, acc 23.4375\n",
      "iteration 9830 loss 2.787010669708252, acc 20.3125\n",
      "iteration 9831 loss 2.5706229209899902, acc 28.125\n",
      "iteration 9832 loss 2.559746742248535, acc 28.125\n",
      "iteration 9833 loss 2.588804244995117, acc 23.4375\n",
      "iteration 9834 loss 2.6456167697906494, acc 29.6875\n",
      "iteration 9835 loss 2.6254489421844482, acc 29.6875\n",
      "iteration 9836 loss 2.7044687271118164, acc 15.625\n",
      "iteration 9837 loss 2.987541913986206, acc 15.625\n",
      "iteration 9838 loss 2.7565670013427734, acc 21.875\n",
      "iteration 9839 loss 3.029858350753784, acc 6.25\n",
      "iteration 9840 loss 2.5468645095825195, acc 20.3125\n",
      "iteration 9841 loss 2.785963296890259, acc 15.625\n",
      "iteration 9842 loss 2.8052000999450684, acc 21.875\n",
      "iteration 9843 loss 2.8155460357666016, acc 25.0\n",
      "iteration 9844 loss 2.6324820518493652, acc 23.4375\n",
      "iteration 9845 loss 2.8037331104278564, acc 23.4375\n",
      "iteration 9846 loss 2.605983018875122, acc 15.625\n",
      "iteration 9847 loss 2.833838701248169, acc 18.75\n",
      "iteration 9848 loss 2.5755391120910645, acc 29.6875\n",
      "iteration 9849 loss 2.6539549827575684, acc 21.875\n",
      "iteration 9850 loss 2.456421136856079, acc 23.4375\n",
      "iteration 9851 loss 2.61628794670105, acc 18.75\n",
      "iteration 9852 loss 2.5683722496032715, acc 18.75\n",
      "iteration 9853 loss 2.7600111961364746, acc 20.3125\n",
      "iteration 9854 loss 2.6775195598602295, acc 20.3125\n",
      "iteration 9855 loss 2.6268889904022217, acc 31.25\n",
      "iteration 9856 loss 2.7991020679473877, acc 17.1875\n",
      "iteration 9857 loss 2.6706173419952393, acc 20.3125\n",
      "iteration 9858 loss 2.759181261062622, acc 20.3125\n",
      "iteration 9859 loss 2.8474788665771484, acc 18.75\n",
      "iteration 9860 loss 2.635098934173584, acc 23.4375\n",
      "iteration 9861 loss 2.570387840270996, acc 25.0\n",
      "iteration 9862 loss 2.6490135192871094, acc 25.0\n",
      "iteration 9863 loss 2.785175085067749, acc 25.0\n",
      "iteration 9864 loss 2.6404049396514893, acc 28.125\n",
      "iteration 9865 loss 2.6248390674591064, acc 25.0\n",
      "iteration 9866 loss 2.66023325920105, acc 20.3125\n",
      "iteration 9867 loss 2.554424285888672, acc 25.0\n",
      "iteration 9868 loss 2.6602954864501953, acc 20.3125\n",
      "iteration 9869 loss 2.8040666580200195, acc 25.0\n",
      "iteration 9870 loss 2.8370776176452637, acc 18.75\n",
      "iteration 9871 loss 2.789372205734253, acc 25.0\n",
      "iteration 9872 loss 2.5503952503204346, acc 28.125\n",
      "iteration 9873 loss 2.655933380126953, acc 18.75\n",
      "iteration 9874 loss 2.9464006423950195, acc 14.0625\n",
      "iteration 9875 loss 2.603041410446167, acc 26.5625\n",
      "iteration 9876 loss 2.949622392654419, acc 14.0625\n",
      "iteration 9877 loss 2.985933303833008, acc 7.8125\n",
      "iteration 9878 loss 2.690969944000244, acc 28.125\n",
      "iteration 9879 loss 2.638962745666504, acc 23.4375\n",
      "iteration 9880 loss 2.6990091800689697, acc 18.75\n",
      "iteration 9881 loss 2.8452460765838623, acc 20.3125\n",
      "iteration 9882 loss 2.7867956161499023, acc 18.75\n",
      "iteration 9883 loss 2.5967133045196533, acc 31.25\n",
      "iteration 9884 loss 2.8738956451416016, acc 21.875\n",
      "iteration 9885 loss 2.725825548171997, acc 20.3125\n",
      "iteration 9886 loss 2.6983895301818848, acc 25.0\n",
      "iteration 9887 loss 2.7818474769592285, acc 18.75\n",
      "iteration 9888 loss 2.671675682067871, acc 20.3125\n",
      "iteration 9889 loss 2.5881459712982178, acc 20.3125\n",
      "iteration 9890 loss 2.705453872680664, acc 18.75\n",
      "iteration 9891 loss 2.7253963947296143, acc 26.5625\n",
      "iteration 9892 loss 2.8187978267669678, acc 17.1875\n",
      "iteration 9893 loss 2.7063825130462646, acc 18.75\n",
      "iteration 9894 loss 2.563842296600342, acc 25.0\n",
      "iteration 9895 loss 2.754945993423462, acc 18.75\n",
      "iteration 9896 loss 2.5259788036346436, acc 26.5625\n",
      "iteration 9897 loss 2.7454612255096436, acc 18.75\n",
      "iteration 9898 loss 2.659005641937256, acc 21.875\n",
      "iteration 9899 loss 2.8258063793182373, acc 17.1875\n",
      "iteration 9900 loss 2.6736063957214355, acc 14.0625\n",
      "iteration 9901 loss 2.803551435470581, acc 14.0625\n",
      "iteration 9902 loss 2.7240943908691406, acc 25.0\n",
      "iteration 9903 loss 2.6828575134277344, acc 25.0\n",
      "iteration 9904 loss 2.651832342147827, acc 23.4375\n",
      "iteration 9905 loss 2.6503987312316895, acc 26.5625\n",
      "iteration 9906 loss 2.606128692626953, acc 21.875\n",
      "iteration 9907 loss 2.6189520359039307, acc 25.0\n",
      "iteration 9908 loss 2.3742716312408447, acc 34.375\n",
      "iteration 9909 loss 2.722209930419922, acc 20.3125\n",
      "iteration 9910 loss 2.688105344772339, acc 23.4375\n",
      "iteration 9911 loss 2.8345515727996826, acc 17.1875\n",
      "iteration 9912 loss 2.701537847518921, acc 15.625\n",
      "iteration 9913 loss 2.8869967460632324, acc 17.1875\n",
      "iteration 9914 loss 2.6457390785217285, acc 17.1875\n",
      "iteration 9915 loss 2.50899338722229, acc 29.6875\n",
      "iteration 9916 loss 2.767212390899658, acc 21.875\n",
      "iteration 9917 loss 2.668990135192871, acc 14.0625\n",
      "iteration 9918 loss 2.6909127235412598, acc 25.0\n",
      "iteration 9919 loss 2.651385545730591, acc 23.4375\n",
      "iteration 9920 loss 2.820237398147583, acc 20.3125\n",
      "iteration 9921 loss 2.573441743850708, acc 21.875\n",
      "iteration 9922 loss 2.7121105194091797, acc 17.1875\n",
      "iteration 9923 loss 2.7282156944274902, acc 18.75\n",
      "iteration 9924 loss 2.670635938644409, acc 25.0\n",
      "iteration 9925 loss 2.7320666313171387, acc 20.3125\n",
      "iteration 9926 loss 2.860219717025757, acc 21.875\n",
      "iteration 9927 loss 2.695030927658081, acc 21.875\n",
      "iteration 9928 loss 2.7302582263946533, acc 18.75\n",
      "iteration 9929 loss 2.689500570297241, acc 28.125\n",
      "iteration 9930 loss 2.4084930419921875, acc 34.375\n",
      "iteration 9931 loss 2.6713218688964844, acc 25.0\n",
      "iteration 9932 loss 2.779130458831787, acc 15.625\n",
      "iteration 9933 loss 2.70099139213562, acc 31.25\n",
      "iteration 9934 loss 2.7033164501190186, acc 28.125\n",
      "iteration 9935 loss 2.4777486324310303, acc 26.5625\n",
      "iteration 9936 loss 2.752185344696045, acc 18.75\n",
      "iteration 9937 loss 2.7278237342834473, acc 18.75\n",
      "iteration 9938 loss 2.8191370964050293, acc 14.0625\n",
      "iteration 9939 loss 2.606034755706787, acc 31.25\n",
      "iteration 9940 loss 2.5437188148498535, acc 21.875\n",
      "iteration 9941 loss 2.7676494121551514, acc 28.125\n",
      "iteration 9942 loss 2.800382614135742, acc 23.4375\n",
      "iteration 9943 loss 2.5914151668548584, acc 20.3125\n",
      "iteration 9944 loss 2.691479444503784, acc 21.875\n",
      "iteration 9945 loss 2.709494113922119, acc 14.0625\n",
      "iteration 9946 loss 2.613333225250244, acc 23.4375\n",
      "iteration 9947 loss 2.5023369789123535, acc 31.25\n",
      "iteration 9948 loss 2.710983991622925, acc 21.875\n",
      "iteration 9949 loss 2.4489755630493164, acc 29.6875\n",
      "iteration 9950 loss 2.659735918045044, acc 28.125\n",
      "iteration 9951 loss 2.69313907623291, acc 18.75\n",
      "iteration 9952 loss 2.50980806350708, acc 28.125\n",
      "iteration 9953 loss 2.7138512134552, acc 21.875\n",
      "iteration 9954 loss 2.4972522258758545, acc 31.25\n",
      "iteration 9955 loss 2.577190399169922, acc 23.4375\n",
      "iteration 9956 loss 2.657618284225464, acc 21.875\n",
      "iteration 9957 loss 2.658985137939453, acc 25.0\n",
      "iteration 9958 loss 2.550147533416748, acc 23.4375\n",
      "iteration 9959 loss 2.494044065475464, acc 29.6875\n",
      "iteration 9960 loss 2.908033609390259, acc 17.1875\n",
      "iteration 9961 loss 2.7330617904663086, acc 17.1875\n",
      "iteration 9962 loss 2.7626962661743164, acc 21.875\n",
      "iteration 9963 loss 2.7363345623016357, acc 29.6875\n",
      "iteration 9964 loss 2.594646453857422, acc 23.4375\n",
      "iteration 9965 loss 2.7696728706359863, acc 20.3125\n",
      "iteration 9966 loss 2.675715446472168, acc 21.875\n",
      "iteration 9967 loss 2.546473741531372, acc 20.3125\n",
      "iteration 9968 loss 2.715395212173462, acc 21.875\n",
      "iteration 9969 loss 2.798603057861328, acc 17.1875\n",
      "iteration 9970 loss 2.705612897872925, acc 28.125\n",
      "iteration 9971 loss 2.6680963039398193, acc 25.0\n",
      "iteration 9972 loss 2.596031904220581, acc 23.4375\n",
      "iteration 9973 loss 2.584545373916626, acc 26.5625\n",
      "iteration 9974 loss 2.719820022583008, acc 18.75\n",
      "iteration 9975 loss 2.5018310546875, acc 39.0625\n",
      "iteration 9976 loss 2.6831655502319336, acc 23.4375\n",
      "iteration 9977 loss 2.7201344966888428, acc 23.4375\n",
      "iteration 9978 loss 2.5139694213867188, acc 32.8125\n",
      "iteration 9979 loss 2.7899045944213867, acc 25.0\n",
      "iteration 9980 loss 2.4438960552215576, acc 28.125\n",
      "iteration 9981 loss 2.69754695892334, acc 21.875\n",
      "iteration 9982 loss 2.43613862991333, acc 32.8125\n",
      "iteration 9983 loss 2.739901065826416, acc 23.4375\n",
      "iteration 9984 loss 2.4525415897369385, acc 34.375\n",
      "iteration 9985 loss 2.583714723587036, acc 28.125\n",
      "iteration 9986 loss 2.467395305633545, acc 25.0\n",
      "iteration 9987 loss 2.6331636905670166, acc 17.1875\n",
      "iteration 9988 loss 2.739225149154663, acc 20.3125\n",
      "iteration 9989 loss 2.6190555095672607, acc 21.875\n",
      "iteration 9990 loss 2.710391044616699, acc 23.4375\n",
      "iteration 9991 loss 2.716804265975952, acc 21.875\n",
      "iteration 9992 loss 2.533754825592041, acc 20.3125\n",
      "iteration 9993 loss 2.5607550144195557, acc 25.0\n",
      "iteration 9994 loss 3.0064589977264404, acc 17.1875\n",
      "iteration 9995 loss 2.672008991241455, acc 20.3125\n",
      "iteration 9996 loss 2.707155704498291, acc 20.3125\n",
      "iteration 9997 loss 2.518429756164551, acc 29.6875\n",
      "iteration 9998 loss 2.7139925956726074, acc 17.1875\n",
      "iteration 9999 loss 2.6096742153167725, acc 21.875\n"
     ]
    }
   ],
   "source": [
    "iterations = 10000\n",
    "lr = 0.1\n",
    "reg  = 0.01\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "stepi = []\n",
    "lossi = []\n",
    "\n",
    "for k in range(iterations):\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    idx = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "    logits = model(X_train[idx])\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y_train[idx])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "    stepi.append(k)\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "    pred = logits.argmax(dim = 1)\n",
    "    acc = (pred == Y_train[idx]).float().mean().data\n",
    "    print(f\"iteration {k} loss {loss.data}, acc {acc * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "498cc9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model(X_train)\n",
    "model.layers[2].setTraining(False)\n",
    "model.layers[2].Folding = True\n",
    "\n",
    "model.layers[4].setTraining(False)\n",
    "model.layers[4].Folding = True\n",
    "\n",
    "model.layers[6].setTraining(False)\n",
    "model.layers[6].Folding = True\n",
    "\n",
    "model.layers[8].setTraining(False)\n",
    "model.layers[8].Folding = True\n",
    "\n",
    "\n",
    "torch.allclose(model(X_train), x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "c4fb0b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-79.6805, -73.8564, -75.5004,  ..., -77.7164, -75.3075, -75.8941],\n",
       "        [-77.8611, -72.1248, -73.7959,  ..., -75.9888, -73.5700, -74.1813],\n",
       "        [-63.4256, -58.3874, -60.2727,  ..., -62.2816, -59.7854, -60.5923],\n",
       "        ...,\n",
       "        [ 61.1063,  60.1216,  56.3878,  ...,  55.9667,  59.1320,  56.6361],\n",
       "        [-79.6805, -73.8564, -75.5004,  ..., -77.7164, -75.3075, -75.8941],\n",
       "        [-68.3681, -63.0909, -64.9028,  ..., -66.9748, -64.5051, -65.2451]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model2(X_train)\n",
    "model.layers[3].Training = False\n",
    "model.layers[6].Training = False\n",
    "model.layers[9].Training = False\n",
    "model.layers[12].Training = False\n",
    "model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0.5, 0.2, 0.3])\n",
    "y = torch.tensor([2])\n",
    "\n",
    "layers = Sequential([\n",
    "    Embedding(nchars, emb_dim),\n",
    "    Flatten(),\n",
    "    Linear(block_size*emb_dim, hidden_dim),\n",
    "    Linear(hidden_dim, hidden_dim)\n",
    "])\n",
    "\n",
    "for p in layers.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "3b614449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182517, 100])"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(logits.grad @ layers.layers[-1].W.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "82f2e874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), tensor(5))"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape, Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "4ff003c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.2958, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = layers(X_train[0][None, :])\n",
    "print(logits)\n",
    "loss = F.cross_entropy(logits, Y_train[0][None])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "3ca6bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "c2b24654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 10])\n"
     ]
    }
   ],
   "source": [
    "print(layers.parameters()[0].shape)\n",
    "#print(layers.parameters()[0].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "404f59ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[380], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m layers\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m      9\u001b[0m     p\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[372], line 86\u001b[0m, in \u001b[0;36mSequential.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 86\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[372], line 9\u001b[0m, in \u001b[0;36mLinear.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb:\n\u001b[1;32m     11\u001b[0m         out \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.8, 0.2]])\n",
    "y = torch.tensor([1])\n",
    "\n",
    "layers = Sequential([\n",
    "    Linear(3, 2),\n",
    "])\n",
    "\n",
    "for p in layers.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "logits = layers(x)\n",
    "\n",
    "loss = F.cross_entropy(logits, y)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(2, 2)\n",
    "])\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "y = torch.randint(0, 2,(40000, 2)).float()\n",
    "for _ in range(10000):\n",
    "    # Create a meshgrid of points\n",
    "    x_min, x_max = -1, 2\n",
    "    y_min, y_max = -1, 2\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                        np.linspace(y_min, y_max, 200))\n",
    "    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "    # Get model predictions\n",
    "    logits = model(grid)\n",
    "    preds = torch.argmax(logits, dim=1).numpy()\n",
    "\n",
    "    # Plot\n",
    "    plt.contourf(xx, yy, preds.reshape(xx.shape), alpha=0.5, cmap='coolwarm')\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    print(logits.shape)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    loss.backward()\n",
    "    print(loss.item())\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data -= 0.1 * p.grad\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad1611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50084f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753d2fa",
   "metadata": {},
   "source": [
    "### 1-Layer with 0 init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "e529795d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0986, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[1.0,2.0,3.0],  [4.0,5.0,6.0]])\n",
    "truth = torch.tensor([1, 2])\n",
    "W = torch.zeros(((3,3)))\n",
    "b = torch.zeros((3))\n",
    "\n",
    "input.requires_grad = True\n",
    "W.requires_grad = True\n",
    "b.requires_grad = True\n",
    "\n",
    "logits = input @ W + b\n",
    "\n",
    "logits.retain_grad()\n",
    "loss = F.cross_entropy(logits, truth)\n",
    "\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "00c2cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed4ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.3333e-01,  3.3333e-01, -1.1667e+00],\n",
      "        [ 1.1667e+00,  1.6667e-01, -1.3333e+00],\n",
      "        [ 1.5000e+00, -5.9605e-08, -1.5000e+00]])\n",
      "tensor([ 0.3333, -0.1667, -0.1667])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[ 0.1667, -0.3333,  0.1667],\n",
      "        [ 0.1667,  0.1667, -0.3333]])\n"
     ]
    }
   ],
   "source": [
    "print(W.grad) \n",
    "# input.T @ dlogits\n",
    "# 1 4       0.1667 -0.333 0.1667\n",
    "# 2 5   x   0.1667 0.1667 -0.333\n",
    "# 3 6\n",
    "print(b.grad)\n",
    "# dlogits.sum(0)\n",
    "\n",
    "print(input.grad)\n",
    "# dlogits @ W.T\n",
    "# 0.1667 -0.333 0.1667      0 0 0 \n",
    "# 0.1667 0.1667 -0.333   x  0 0 0 \n",
    "#                           0 0 0\n",
    "print(logits.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4897745",
   "metadata": {},
   "source": [
    "### 1-Layer with constant (1) init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "9726106b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0986, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[1.0,2.0,3.0],  [4.0,5.0,6.0]])\n",
    "truth = torch.tensor([1, 2])\n",
    "W = torch.ones(((3,3)))\n",
    "b = torch.zeros((3))\n",
    "\n",
    "input.requires_grad = True\n",
    "W.requires_grad = True\n",
    "b.requires_grad = True\n",
    "\n",
    "logits = input @ W + b\n",
    "\n",
    "logits.retain_grad()\n",
    "loss = F.cross_entropy(logits, truth)\n",
    "\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "77e6f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d321adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.3333e-01,  3.3333e-01, -1.1667e+00],\n",
      "        [ 1.1667e+00,  1.6667e-01, -1.3333e+00],\n",
      "        [ 1.5000e+00, -5.9605e-08, -1.5000e+00]])\n",
      "tensor([ 0.3333, -0.1667, -0.1667])\n",
      "tensor([[-2.9802e-08, -2.9802e-08, -2.9802e-08],\n",
      "        [-2.9802e-08, -2.9802e-08, -2.9802e-08]])\n",
      "tensor([[ 0.1667, -0.3333,  0.1667],\n",
      "        [ 0.1667,  0.1667, -0.3333]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1667, -0.3333,  0.1667],\n",
       "        [ 0.1667,  0.1667, -0.3333]])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(W.grad) \n",
    "# input.T @ dlogits\n",
    "# 1 4       0.1667 -0.333 0.1667\n",
    "# 2 5   x   0.1667 0.1667 -0.333\n",
    "# 3 6\n",
    "print(b.grad)\n",
    "# dlogits.sum(0)\n",
    "\n",
    "print(input.grad)\n",
    "# dlogits @ W.T\n",
    "\n",
    "# 0.1667 -0.333 0.1667      1 1 1\n",
    "# 0.1667 0.1667 -0.333   x  1 1 1\n",
    "#                           1 1 1\n",
    "print(logits.grad)\n",
    "\n",
    "\n",
    "## Zero grad for the input when using crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79972037",
   "metadata": {},
   "source": [
    "### 2-Layer with zero init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a3930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0986, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[1.0,2.0,3.0],  [4.0,5.0,6.0]])\n",
    "truth = torch.tensor([1, 2])\n",
    "W1 = torch.zeros(((3,3)))\n",
    "b1 = torch.zeros((3))\n",
    "W2 = torch.zeros(((3,3)))\n",
    "b2 = torch.zeros((3))\n",
    "W1.requires_grad = True\n",
    "b1.requires_grad = True\n",
    "W2.requires_grad = True\n",
    "b2.requires_grad = True\n",
    "\n",
    "\n",
    "h = input @ W1 + b1\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "logits.retain_grad()\n",
    "h.retain_grad()\n",
    "loss = F.cross_entropy(logits, truth)\n",
    "\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "cf44f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea6b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1667, -0.3333,  0.1667],\n",
      "        [ 0.1667,  0.1667, -0.3333]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([ 0.3333, -0.1667, -0.1667])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(logits.grad)\n",
    "\n",
    "print(W2.grad)\n",
    "# h.T @ dlogits\n",
    "# 0 0       0.1667 -0.333 0.1667\n",
    "# 0 0   x   0.1667 0.1667 -0.333\n",
    "# 0 0\n",
    "print(b2.grad)\n",
    "# dlogits.sum(0)\n",
    "\n",
    "print(h.grad)\n",
    "# dlogits @ W2.T\n",
    "# 0.1667 -0.333 0.1667      0 0 0\n",
    "# 0.1667 0.1667 -0.333   x  0 0 0\n",
    "#                           0 0 0\n",
    "\n",
    "\n",
    "print(W1.grad)\n",
    "# input.T @ dh\n",
    "# 1 4       0.0 0.0 0.0\n",
    "# 2 5   x   0.0 0.0 0.0  = \n",
    "# 3 6\n",
    "\n",
    "print(b1.grad)\n",
    "# dlogits.sum(0) = 0\n",
    "\n",
    "\n",
    "## Zero grad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a57a8",
   "metadata": {},
   "source": [
    "### 2-Layer with constant (1) init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "ac6065c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0986, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[1.0,2.0,3.0],  [4.0,5.0,6.0]])\n",
    "truth = torch.tensor([1, 2])\n",
    "W1 = torch.ones(((3,3)))\n",
    "b1 = torch.zeros((3))\n",
    "W2 = torch.ones(((3,3)))\n",
    "b2 = torch.zeros((3))\n",
    "W1.requires_grad = True\n",
    "b1.requires_grad = True\n",
    "W2.requires_grad = True\n",
    "b2.requires_grad = True\n",
    "\n",
    "\n",
    "h = input @ W1 + b1\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "logits.retain_grad()\n",
    "h.retain_grad()\n",
    "loss = F.cross_entropy(logits, truth)\n",
    "\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "91decb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "f9cd1002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1667, -0.3333,  0.1667],\n",
       "        [ 0.1667,  0.1667, -0.3333]])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0a76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1667, -0.3333,  0.1667],\n",
      "        [ 0.1667,  0.1667, -0.3333]])\n",
      "tensor([[ 3.5000,  0.5000, -4.0000],\n",
      "        [ 3.5000,  0.5000, -4.0000],\n",
      "        [ 3.5000,  0.5000, -4.0000]], grad_fn=<MmBackward0>)\n",
      "tensor([[ 3.5000,  0.5000, -4.0000],\n",
      "        [ 3.5000,  0.5000, -4.0000],\n",
      "        [ 3.5000,  0.5000, -4.0000]])\n",
      "tensor([ 0.3333, -0.1667, -0.1667])\n",
      "tensor([[-2.9802e-08, -2.9802e-08, -2.9802e-08],\n",
      "        [-2.9802e-08, -2.9802e-08, -2.9802e-08]])\n",
      "tensor([[-1.4901e-07, -1.4901e-07, -1.4901e-07],\n",
      "        [-2.0862e-07, -2.0862e-07, -2.0862e-07],\n",
      "        [-2.6822e-07, -2.6822e-07, -2.6822e-07]])\n",
      "tensor([-5.9605e-08, -5.9605e-08, -5.9605e-08])\n"
     ]
    }
   ],
   "source": [
    "print(logits.grad)\n",
    "print(W2.grad)\n",
    "# h.T @ dlogits\n",
    "# 6 15       0.1667 -0.333 0.1667\n",
    "# 6 15   x   0.1667 0.1667 -0.333\n",
    "# 6 15\n",
    "print(b2.grad)\n",
    "# dlogits.sum(0)\n",
    "\n",
    "print(h.grad)\n",
    "# dlogits @ W2.T\n",
    "# 0.1667 -0.333 0.1667      1 1 1\n",
    "# 0.1667 0.1667 -0.333   x  1 1 1\n",
    "#                           1 1 1\n",
    "\n",
    "## Zero grad for the input when using crossentropy\n",
    "\n",
    "print(W1.grad)\n",
    "# input.T @ dh\n",
    "# 1 4       0.0 0.0 0.0\n",
    "# 2 5   x   0.0 0.0 0.0  = \n",
    "# 3 6\n",
    "\n",
    "print(b1.grad)\n",
    "# dlogits.sum(0) = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef700ca8",
   "metadata": {},
   "source": [
    "dL/a^l * (da^l / dz^l) * (dz^l / dWl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8babbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# aL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "77b46134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1667, -0.3333,  0.1667],\n",
       "        [ 0.1667,  0.1667, -0.3333]])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.grad #why not divde by 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "1c44a364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1667, -0.3333,  0.1667],\n",
       "        [ 0.1667,  0.1667, -0.3333]])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.grad\n",
    "#input.T @ logits.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a9d287e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdabe2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
