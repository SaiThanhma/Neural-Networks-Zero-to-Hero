{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eda3dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c4e389eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read_words('names.txt')\n",
    "stoi, itos = get_mapping(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cfc63c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, val_split = 0.8, 0.1\n",
    "X, Y = build_dataset(words, stoi, block_size=3)\n",
    "X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "\n",
    "n = len(X)\n",
    "n1 = round(n * train_split)\n",
    "n2 = round(n * val_split)\n",
    "\n",
    "\n",
    "X_train, Y_train = X[:n1], Y[:n1]\n",
    "X_val, Y_val = X[n1:n1+n2], Y[n1:n1+n2]\n",
    "X_test, Y_test = X[n1+n2:], Y[n1+n2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0a252431",
   "metadata": {},
   "outputs": [],
   "source": [
    "nchars = len(stoi.keys())\n",
    "W = torch.rand(nchars, nchars, nchars) * 0.1\n",
    "W.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c766e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 loss 3.1808459758758545, acc 19.68255043029785\n",
      "iteration 1 loss 3.180293321609497, acc 19.68255043029785\n",
      "iteration 2 loss 3.1797478199005127, acc 19.713232040405273\n",
      "iteration 3 loss 3.1792094707489014, acc 19.718711853027344\n",
      "iteration 4 loss 3.178678035736084, acc 19.718711853027344\n",
      "iteration 5 loss 3.1781527996063232, acc 19.718711853027344\n",
      "iteration 6 loss 3.1776347160339355, acc 19.73185920715332\n",
      "iteration 7 loss 3.1771228313446045, acc 19.73185920715332\n",
      "iteration 8 loss 3.176616907119751, acc 19.73185920715332\n",
      "iteration 9 loss 3.176116943359375, acc 19.77733612060547\n",
      "iteration 10 loss 3.1756224632263184, acc 19.77733612060547\n",
      "iteration 11 loss 3.1751341819763184, acc 19.77733612060547\n",
      "iteration 12 loss 3.1746509075164795, acc 19.77733612060547\n",
      "iteration 13 loss 3.17417311668396, acc 19.77733612060547\n",
      "iteration 14 loss 3.1737005710601807, acc 19.82171630859375\n",
      "iteration 15 loss 3.1732327938079834, acc 19.82171630859375\n",
      "iteration 16 loss 3.172769784927368, acc 19.82171630859375\n",
      "iteration 17 loss 3.172311544418335, acc 19.82171630859375\n",
      "iteration 18 loss 3.1718578338623047, acc 19.91485595703125\n",
      "iteration 19 loss 3.1714086532592773, acc 19.91485595703125\n",
      "iteration 20 loss 3.1709635257720947, acc 19.973482131958008\n",
      "iteration 21 loss 3.170522928237915, acc 20.045257568359375\n",
      "iteration 22 loss 3.170086145401001, acc 20.045257568359375\n",
      "iteration 23 loss 3.1696531772613525, acc 20.17401123046875\n",
      "iteration 24 loss 3.1692240238189697, acc 20.188257217407227\n",
      "iteration 25 loss 3.1687984466552734, acc 20.234827041625977\n",
      "iteration 26 loss 3.168376922607422, acc 20.234827041625977\n",
      "iteration 27 loss 3.1679584980010986, acc 20.234827041625977\n",
      "iteration 28 loss 3.16754412651062, acc 20.263317108154297\n",
      "iteration 29 loss 3.1671316623687744, acc 20.263317108154297\n",
      "iteration 30 loss 3.1667234897613525, acc 20.2934513092041\n",
      "iteration 31 loss 3.16631817817688, acc 20.29454803466797\n",
      "iteration 32 loss 3.1659157276153564, acc 20.29454803466797\n",
      "iteration 33 loss 3.1655161380767822, acc 20.308792114257812\n",
      "iteration 34 loss 3.1651196479797363, acc 20.332901000976562\n",
      "iteration 35 loss 3.1647257804870605, acc 20.354267120361328\n",
      "iteration 36 loss 3.164335012435913, acc 20.354267120361328\n",
      "iteration 37 loss 3.1639463901519775, acc 20.363035202026367\n",
      "iteration 38 loss 3.1635608673095703, acc 20.363035202026367\n",
      "iteration 39 loss 3.163177251815796, acc 20.394264221191406\n",
      "iteration 40 loss 3.1627964973449707, acc 20.49507713317871\n",
      "iteration 41 loss 3.1624176502227783, acc 20.552059173583984\n",
      "iteration 42 loss 3.162041664123535, acc 20.552059173583984\n",
      "iteration 43 loss 3.161668062210083, acc 20.552059173583984\n",
      "iteration 44 loss 3.1612966060638428, acc 20.56082534790039\n",
      "iteration 45 loss 3.160926580429077, acc 20.56082534790039\n",
      "iteration 46 loss 3.1605591773986816, acc 20.798063278198242\n",
      "iteration 47 loss 3.16019344329834, acc 20.798063278198242\n",
      "iteration 48 loss 3.1598305702209473, acc 20.824359893798828\n",
      "iteration 49 loss 3.1594691276550293, acc 20.824359893798828\n",
      "iteration 50 loss 3.159109592437744, acc 20.824359893798828\n",
      "iteration 51 loss 3.158752202987671, acc 20.834224700927734\n",
      "iteration 52 loss 3.1583962440490723, acc 20.834224700927734\n",
      "iteration 53 loss 3.1580419540405273, acc 20.83806037902832\n",
      "iteration 54 loss 3.1576898097991943, acc 20.964622497558594\n",
      "iteration 55 loss 3.157339572906494, acc 20.964622497558594\n",
      "iteration 56 loss 3.1569907665252686, acc 20.965171813964844\n",
      "iteration 57 loss 3.1566433906555176, acc 21.0440673828125\n",
      "iteration 58 loss 3.1562976837158203, acc 21.272539138793945\n",
      "iteration 59 loss 3.1559536457061768, acc 21.272539138793945\n",
      "iteration 60 loss 3.1556105613708496, acc 21.27637481689453\n",
      "iteration 61 loss 3.1552693843841553, acc 21.280210494995117\n",
      "iteration 62 loss 3.154930353164673, acc 21.280210494995117\n",
      "iteration 63 loss 3.1545915603637695, acc 21.280210494995117\n",
      "iteration 64 loss 3.1542553901672363, acc 21.280210494995117\n",
      "iteration 65 loss 3.1539201736450195, acc 21.43581199645996\n",
      "iteration 66 loss 3.1535863876342773, acc 21.456083297729492\n",
      "iteration 67 loss 3.1532540321350098, acc 21.50594139099121\n",
      "iteration 68 loss 3.1529223918914795, acc 21.714141845703125\n",
      "iteration 69 loss 3.152592658996582, acc 21.727291107177734\n",
      "iteration 70 loss 3.15226411819458, acc 21.733318328857422\n",
      "iteration 71 loss 3.1519367694854736, acc 21.733318328857422\n",
      "iteration 72 loss 3.1516106128692627, acc 21.754138946533203\n",
      "iteration 73 loss 3.1512856483459473, acc 21.754138946533203\n",
      "iteration 74 loss 3.1509618759155273, acc 21.845088958740234\n",
      "iteration 75 loss 3.150639533996582, acc 21.88234519958496\n",
      "iteration 76 loss 3.150318145751953, acc 21.88234519958496\n",
      "iteration 77 loss 3.1499979496002197, acc 21.960145950317383\n",
      "iteration 78 loss 3.149678945541382, acc 21.994115829467773\n",
      "iteration 79 loss 3.1493611335754395, acc 21.994115829467773\n",
      "iteration 80 loss 3.1490440368652344, acc 22.069177627563477\n",
      "iteration 81 loss 3.148728370666504, acc 22.153553009033203\n",
      "iteration 82 loss 3.1484131813049316, acc 22.153553009033203\n",
      "iteration 83 loss 3.148099899291992, acc 22.155744552612305\n",
      "iteration 84 loss 3.147786855697632, acc 22.166702270507812\n",
      "iteration 85 loss 3.147475242614746, acc 22.166702270507812\n",
      "iteration 86 loss 3.1471645832061768, acc 22.166702270507812\n",
      "iteration 87 loss 3.1468546390533447, acc 22.189165115356445\n",
      "iteration 88 loss 3.1465461254119873, acc 22.283403396606445\n",
      "iteration 89 loss 3.146238327026367, acc 22.340383529663086\n",
      "iteration 90 loss 3.1459317207336426, acc 22.340383529663086\n",
      "iteration 91 loss 3.145625352859497, acc 22.340383529663086\n",
      "iteration 92 loss 3.145320415496826, acc 22.383121490478516\n",
      "iteration 93 loss 3.1450164318084717, acc 22.41270637512207\n",
      "iteration 94 loss 3.1447131633758545, acc 22.41270637512207\n",
      "iteration 95 loss 3.1444106101989746, acc 22.42585563659668\n",
      "iteration 96 loss 3.1441092491149902, acc 22.42585563659668\n",
      "iteration 97 loss 3.1438093185424805, acc 22.434621810913086\n",
      "iteration 98 loss 3.14350962638855, acc 22.568309783935547\n",
      "iteration 99 loss 3.1432106494903564, acc 22.568309783935547\n",
      "iteration 100 loss 3.1429123878479004, acc 22.59899139404297\n",
      "iteration 101 loss 3.14261531829834, acc 22.59899139404297\n",
      "iteration 102 loss 3.1423189640045166, acc 22.60337257385254\n",
      "iteration 103 loss 3.142023801803589, acc 22.60337257385254\n",
      "iteration 104 loss 3.1417288780212402, acc 22.60337257385254\n",
      "iteration 105 loss 3.141435146331787, acc 22.631864547729492\n",
      "iteration 106 loss 3.141141891479492, acc 22.631864547729492\n",
      "iteration 107 loss 3.1408495903015137, acc 22.631864547729492\n",
      "iteration 108 loss 3.1405577659606934, acc 22.631864547729492\n",
      "iteration 109 loss 3.1402671337127686, acc 22.709665298461914\n",
      "iteration 110 loss 3.139977216720581, acc 22.753496170043945\n",
      "iteration 111 loss 3.1396875381469727, acc 22.753496170043945\n",
      "iteration 112 loss 3.139399290084839, acc 22.753496170043945\n",
      "iteration 113 loss 3.139111280441284, acc 22.753496170043945\n",
      "iteration 114 loss 3.138823986053467, acc 22.753496170043945\n",
      "iteration 115 loss 3.138537645339966, acc 22.753496170043945\n",
      "iteration 116 loss 3.138251781463623, acc 22.753496170043945\n",
      "iteration 117 loss 3.1379666328430176, acc 22.761167526245117\n",
      "iteration 118 loss 3.1376824378967285, acc 22.761167526245117\n",
      "iteration 119 loss 3.1373984813690186, acc 22.761167526245117\n",
      "iteration 120 loss 3.137115716934204, acc 22.76555061340332\n",
      "iteration 121 loss 3.1368329524993896, acc 22.76555061340332\n",
      "iteration 122 loss 3.136551856994629, acc 22.767742156982422\n",
      "iteration 123 loss 3.136270761489868, acc 22.767742156982422\n",
      "iteration 124 loss 3.1359903812408447, acc 22.767742156982422\n",
      "iteration 125 loss 3.1357107162475586, acc 22.79184913635254\n",
      "iteration 126 loss 3.1354315280914307, acc 22.813217163085938\n",
      "iteration 127 loss 3.13515305519104, acc 22.813217163085938\n",
      "iteration 128 loss 3.134875535964966, acc 22.902524948120117\n",
      "iteration 129 loss 3.1345980167388916, acc 22.902524948120117\n",
      "iteration 130 loss 3.134321689605713, acc 22.902524948120117\n",
      "iteration 131 loss 3.1340456008911133, acc 22.91183853149414\n",
      "iteration 132 loss 3.133769989013672, acc 22.91183853149414\n",
      "iteration 133 loss 3.133495330810547, acc 22.91183853149414\n",
      "iteration 134 loss 3.133220672607422, acc 22.91457748413086\n",
      "iteration 135 loss 3.1329474449157715, acc 22.91457748413086\n",
      "iteration 136 loss 3.1326744556427, acc 22.931013107299805\n",
      "iteration 137 loss 3.132401943206787, acc 22.931013107299805\n",
      "iteration 138 loss 3.1321299076080322, acc 22.931013107299805\n",
      "iteration 139 loss 3.131859064102173, acc 22.957860946655273\n",
      "iteration 140 loss 3.1315882205963135, acc 22.983064651489258\n",
      "iteration 141 loss 3.131317615509033, acc 22.99785804748535\n",
      "iteration 142 loss 3.1310486793518066, acc 22.99785804748535\n",
      "iteration 143 loss 3.130779504776001, acc 23.009363174438477\n",
      "iteration 144 loss 3.1305108070373535, acc 23.009363174438477\n",
      "iteration 145 loss 3.1302425861358643, acc 23.009363174438477\n",
      "iteration 146 loss 3.1299755573272705, acc 23.009363174438477\n",
      "iteration 147 loss 3.1297085285186768, acc 23.009363174438477\n",
      "iteration 148 loss 3.1294426918029785, acc 23.009363174438477\n",
      "iteration 149 loss 3.129176616668701, acc 23.104698181152344\n",
      "iteration 150 loss 3.128911256790161, acc 23.104698181152344\n",
      "iteration 151 loss 3.1286466121673584, acc 23.151817321777344\n",
      "iteration 152 loss 3.1283822059631348, acc 23.151817321777344\n",
      "iteration 153 loss 3.1281180381774902, acc 23.151817321777344\n",
      "iteration 154 loss 3.127855062484741, acc 23.158390045166016\n",
      "iteration 155 loss 3.1275928020477295, acc 23.19674301147461\n",
      "iteration 156 loss 3.1273300647735596, acc 23.19674301147461\n",
      "iteration 157 loss 3.127068042755127, acc 23.19674301147461\n",
      "iteration 158 loss 3.1268064975738525, acc 23.19674301147461\n",
      "iteration 159 loss 3.1265459060668945, acc 23.25098419189453\n",
      "iteration 160 loss 3.1262855529785156, acc 23.25098419189453\n",
      "iteration 161 loss 3.126025915145874, acc 23.25098419189453\n",
      "iteration 162 loss 3.1257665157318115, acc 23.25098419189453\n",
      "iteration 163 loss 3.1255075931549072, acc 23.25098419189453\n",
      "iteration 164 loss 3.125248908996582, acc 23.25098419189453\n",
      "iteration 165 loss 3.124990701675415, acc 23.301939010620117\n",
      "iteration 166 loss 3.1247339248657227, acc 23.301939010620117\n",
      "iteration 167 loss 3.124476432800293, acc 23.301939010620117\n",
      "iteration 168 loss 3.1242198944091797, acc 23.30413055419922\n",
      "iteration 169 loss 3.1239633560180664, acc 23.30413055419922\n",
      "iteration 170 loss 3.1237075328826904, acc 23.30413055419922\n",
      "iteration 171 loss 3.1234524250030518, acc 23.30413055419922\n",
      "iteration 172 loss 3.123197555541992, acc 23.3271427154541\n",
      "iteration 173 loss 3.122943162918091, acc 23.3271427154541\n",
      "iteration 174 loss 3.1226894855499268, acc 23.328784942626953\n",
      "iteration 175 loss 3.1224358081817627, acc 23.388505935668945\n",
      "iteration 176 loss 3.122182607650757, acc 23.40056037902832\n",
      "iteration 177 loss 3.121929883956909, acc 23.40056037902832\n",
      "iteration 178 loss 3.121677875518799, acc 23.40056037902832\n",
      "iteration 179 loss 3.1214258670806885, acc 23.40056037902832\n",
      "iteration 180 loss 3.1211743354797363, acc 23.412065505981445\n",
      "iteration 181 loss 3.1209230422973633, acc 23.412065505981445\n",
      "iteration 182 loss 3.1206724643707275, acc 23.4169979095459\n",
      "iteration 183 loss 3.120422124862671, acc 23.4169979095459\n",
      "iteration 184 loss 3.1201727390289307, acc 23.4169979095459\n",
      "iteration 185 loss 3.1199231147766113, acc 23.4169979095459\n",
      "iteration 186 loss 3.119673728942871, acc 23.4169979095459\n",
      "iteration 187 loss 3.1194255352020264, acc 23.419736862182617\n",
      "iteration 188 loss 3.1191773414611816, acc 23.42137908935547\n",
      "iteration 189 loss 3.118929624557495, acc 23.434528350830078\n",
      "iteration 190 loss 3.1186819076538086, acc 23.45206069946289\n",
      "iteration 191 loss 3.1184346675872803, acc 23.454801559448242\n",
      "iteration 192 loss 3.1181881427764893, acc 23.498634338378906\n",
      "iteration 193 loss 3.1179416179656982, acc 23.498634338378906\n",
      "iteration 194 loss 3.1176958084106445, acc 23.498634338378906\n",
      "iteration 195 loss 3.1174497604370117, acc 23.57259750366211\n",
      "iteration 196 loss 3.1172046661376953, acc 23.57259750366211\n",
      "iteration 197 loss 3.116959810256958, acc 23.57259750366211\n",
      "iteration 198 loss 3.1167151927948, acc 23.57259750366211\n",
      "iteration 199 loss 3.1164710521698, acc 23.57259750366211\n",
      "iteration 200 loss 3.116227388381958, acc 23.597253799438477\n",
      "iteration 201 loss 3.1159844398498535, acc 23.597803115844727\n",
      "iteration 202 loss 3.115741014480591, acc 23.597803115844727\n",
      "iteration 203 loss 3.1154980659484863, acc 23.597803115844727\n",
      "iteration 204 loss 3.1152560710906982, acc 23.597803115844727\n",
      "iteration 205 loss 3.115013837814331, acc 23.62574577331543\n",
      "iteration 206 loss 3.114772319793701, acc 23.648208618164062\n",
      "iteration 207 loss 3.1145312786102295, acc 23.64875602722168\n",
      "iteration 208 loss 3.114290237426758, acc 23.681631088256836\n",
      "iteration 209 loss 3.1140499114990234, acc 23.707380294799805\n",
      "iteration 210 loss 3.113809585571289, acc 23.707380294799805\n",
      "iteration 211 loss 3.113569498062134, acc 23.721078872680664\n",
      "iteration 212 loss 3.1133298873901367, acc 23.74628257751465\n",
      "iteration 213 loss 3.113090753555298, acc 23.749568939208984\n",
      "iteration 214 loss 3.112851619720459, acc 23.749568939208984\n",
      "iteration 215 loss 3.1126136779785156, acc 23.749568939208984\n",
      "iteration 216 loss 3.112375259399414, acc 23.796688079833984\n",
      "iteration 217 loss 3.11213755607605, acc 23.796688079833984\n",
      "iteration 218 loss 3.1118996143341064, acc 23.804906845092773\n",
      "iteration 219 loss 3.1116621494293213, acc 23.804906845092773\n",
      "iteration 220 loss 3.1114251613616943, acc 23.804906845092773\n",
      "iteration 221 loss 3.1111886501312256, acc 23.804906845092773\n",
      "iteration 222 loss 3.110952377319336, acc 23.804906845092773\n",
      "iteration 223 loss 3.1107165813446045, acc 23.80545425415039\n",
      "iteration 224 loss 3.110481023788452, acc 23.806549072265625\n",
      "iteration 225 loss 3.1102454662323, acc 23.806549072265625\n",
      "iteration 226 loss 3.1100106239318848, acc 23.806549072265625\n",
      "iteration 227 loss 3.1097757816314697, acc 23.806549072265625\n",
      "iteration 228 loss 3.109541416168213, acc 23.816959381103516\n",
      "iteration 229 loss 3.109307050704956, acc 23.864627838134766\n",
      "iteration 230 loss 3.1090731620788574, acc 23.885446548461914\n",
      "iteration 231 loss 3.108839750289917, acc 23.89750099182129\n",
      "iteration 232 loss 3.1086065769195557, acc 23.89750099182129\n",
      "iteration 233 loss 3.1083736419677734, acc 23.89750099182129\n",
      "iteration 234 loss 3.1081409454345703, acc 23.95941162109375\n",
      "iteration 235 loss 3.1079084873199463, acc 23.95941162109375\n",
      "iteration 236 loss 3.1076762676239014, acc 23.95941162109375\n",
      "iteration 237 loss 3.1074447631835938, acc 23.95941162109375\n",
      "iteration 238 loss 3.107213258743286, acc 23.95941162109375\n",
      "iteration 239 loss 3.1069822311401367, acc 23.95941162109375\n",
      "iteration 240 loss 3.1067512035369873, acc 23.95941162109375\n",
      "iteration 241 loss 3.106520414352417, acc 24.025707244873047\n",
      "iteration 242 loss 3.106290340423584, acc 24.025707244873047\n",
      "iteration 243 loss 3.106060266494751, acc 24.081592559814453\n",
      "iteration 244 loss 3.105830669403076, acc 24.081592559814453\n",
      "iteration 245 loss 3.1056010723114014, acc 24.088714599609375\n",
      "iteration 246 loss 3.1053719520568848, acc 24.088714599609375\n",
      "iteration 247 loss 3.105142831802368, acc 24.091453552246094\n",
      "iteration 248 loss 3.1049139499664307, acc 24.091453552246094\n",
      "iteration 249 loss 3.1046857833862305, acc 24.091453552246094\n",
      "iteration 250 loss 3.1044578552246094, acc 24.091453552246094\n",
      "iteration 251 loss 3.1042301654815674, acc 24.11610984802246\n",
      "iteration 252 loss 3.1040024757385254, acc 24.11610984802246\n",
      "iteration 253 loss 3.1037750244140625, acc 24.11610984802246\n",
      "iteration 254 loss 3.103548288345337, acc 24.11610984802246\n",
      "iteration 255 loss 3.1033215522766113, acc 24.12706756591797\n",
      "iteration 256 loss 3.1030948162078857, acc 24.12706756591797\n",
      "iteration 257 loss 3.1028685569763184, acc 24.12706756591797\n",
      "iteration 258 loss 3.102642774581909, acc 24.12706756591797\n",
      "iteration 259 loss 3.1024169921875, acc 24.12706756591797\n",
      "iteration 260 loss 3.10219144821167, acc 24.166515350341797\n",
      "iteration 261 loss 3.101966381072998, acc 24.18130874633789\n",
      "iteration 262 loss 3.1017415523529053, acc 24.18130874633789\n",
      "iteration 263 loss 3.1015164852142334, acc 24.18130874633789\n",
      "iteration 264 loss 3.101292371749878, acc 24.18130874633789\n",
      "iteration 265 loss 3.1010684967041016, acc 24.18130874633789\n",
      "iteration 266 loss 3.100844144821167, acc 24.18130874633789\n",
      "iteration 267 loss 3.1006202697753906, acc 24.18130874633789\n",
      "iteration 268 loss 3.1003968715667725, acc 24.18130874633789\n",
      "iteration 269 loss 3.1001737117767334, acc 24.18130874633789\n",
      "iteration 270 loss 3.0999512672424316, acc 24.18130874633789\n",
      "iteration 271 loss 3.0997283458709717, acc 24.207059860229492\n",
      "iteration 272 loss 3.099506139755249, acc 24.216920852661133\n",
      "iteration 273 loss 3.0992836952209473, acc 24.216920852661133\n",
      "iteration 274 loss 3.0990617275238037, acc 24.216920852661133\n",
      "iteration 275 loss 3.0988399982452393, acc 24.22897720336914\n",
      "iteration 276 loss 3.098618745803833, acc 24.22897720336914\n",
      "iteration 277 loss 3.0983974933624268, acc 24.24650764465332\n",
      "iteration 278 loss 3.0981767177581787, acc 24.26513671875\n",
      "iteration 279 loss 3.0979557037353516, acc 24.2673282623291\n",
      "iteration 280 loss 3.097735643386841, acc 24.2673282623291\n",
      "iteration 281 loss 3.097515106201172, acc 24.2673282623291\n",
      "iteration 282 loss 3.097294807434082, acc 24.2673282623291\n",
      "iteration 283 loss 3.0970747470855713, acc 24.2673282623291\n",
      "iteration 284 loss 3.096855640411377, acc 24.30403709411621\n",
      "iteration 285 loss 3.0966358184814453, acc 24.311708450317383\n",
      "iteration 286 loss 3.096417188644409, acc 24.311708450317383\n",
      "iteration 287 loss 3.096198320388794, acc 24.311708450317383\n",
      "iteration 288 loss 3.0959794521331787, acc 24.311708450317383\n",
      "iteration 289 loss 3.095761299133301, acc 24.311708450317383\n",
      "iteration 290 loss 3.095543146133423, acc 24.311708450317383\n",
      "iteration 291 loss 3.095325231552124, acc 24.311708450317383\n",
      "iteration 292 loss 3.0951075553894043, acc 24.311708450317383\n",
      "iteration 293 loss 3.0948894023895264, acc 24.311708450317383\n",
      "iteration 294 loss 3.094672203063965, acc 24.311708450317383\n",
      "iteration 295 loss 3.0944554805755615, acc 24.311708450317383\n",
      "iteration 296 loss 3.09423828125, acc 24.311708450317383\n",
      "iteration 297 loss 3.0940215587615967, acc 24.311708450317383\n",
      "iteration 298 loss 3.0938055515289307, acc 24.311708450317383\n",
      "iteration 299 loss 3.0935893058776855, acc 24.313899993896484\n",
      "iteration 300 loss 3.0933732986450195, acc 24.379098892211914\n",
      "iteration 301 loss 3.0931575298309326, acc 24.379098892211914\n",
      "iteration 302 loss 3.092941999435425, acc 24.379098892211914\n",
      "iteration 303 loss 3.092726707458496, acc 24.379098892211914\n",
      "iteration 304 loss 3.0925118923187256, acc 24.379098892211914\n",
      "iteration 305 loss 3.092296838760376, acc 24.390605926513672\n",
      "iteration 306 loss 3.0920822620391846, acc 24.390605926513672\n",
      "iteration 307 loss 3.091867446899414, acc 24.39608383178711\n",
      "iteration 308 loss 3.091653347015381, acc 24.39608383178711\n",
      "iteration 309 loss 3.0914392471313477, acc 24.39608383178711\n",
      "iteration 310 loss 3.0912253856658936, acc 24.39608383178711\n",
      "iteration 311 loss 3.0910115242004395, acc 24.42183494567871\n",
      "iteration 312 loss 3.0907983779907227, acc 24.42183494567871\n",
      "iteration 313 loss 3.090585231781006, acc 24.424026489257812\n",
      "iteration 314 loss 3.0903725624084473, acc 24.424026489257812\n",
      "iteration 315 loss 3.0901591777801514, acc 24.424026489257812\n",
      "iteration 316 loss 3.089946985244751, acc 24.424026489257812\n",
      "iteration 317 loss 3.0897345542907715, acc 24.424026489257812\n",
      "iteration 318 loss 3.08952260017395, acc 24.424026489257812\n",
      "iteration 319 loss 3.0893101692199707, acc 24.428956985473633\n",
      "iteration 320 loss 3.0890984535217285, acc 24.43498420715332\n",
      "iteration 321 loss 3.0888869762420654, acc 24.46840476989746\n",
      "iteration 322 loss 3.0886752605438232, acc 24.47881507873535\n",
      "iteration 323 loss 3.0884644985198975, acc 24.47881507873535\n",
      "iteration 324 loss 3.0882532596588135, acc 24.488677978515625\n",
      "iteration 325 loss 3.088042736053467, acc 24.488677978515625\n",
      "iteration 326 loss 3.08783221244812, acc 24.488677978515625\n",
      "iteration 327 loss 3.0876216888427734, acc 24.488677978515625\n",
      "iteration 328 loss 3.087411642074585, acc 24.488677978515625\n",
      "iteration 329 loss 3.0872015953063965, acc 24.488677978515625\n",
      "iteration 330 loss 3.086991786956787, acc 24.494705200195312\n",
      "iteration 331 loss 3.086782217025757, acc 24.499635696411133\n",
      "iteration 332 loss 3.0865728855133057, acc 24.499635696411133\n",
      "iteration 333 loss 3.0863635540008545, acc 24.499635696411133\n",
      "iteration 334 loss 3.0861544609069824, acc 24.499635696411133\n",
      "iteration 335 loss 3.0859458446502686, acc 24.499635696411133\n",
      "iteration 336 loss 3.0857372283935547, acc 24.53196144104004\n",
      "iteration 337 loss 3.085528612136841, acc 24.55771255493164\n",
      "iteration 338 loss 3.085319995880127, acc 24.55771255493164\n",
      "iteration 339 loss 3.0851123332977295, acc 24.59770965576172\n",
      "iteration 340 loss 3.0849039554595947, acc 24.601545333862305\n",
      "iteration 341 loss 3.0846967697143555, acc 24.601545333862305\n",
      "iteration 342 loss 3.084489107131958, acc 24.601545333862305\n",
      "iteration 343 loss 3.0842816829681396, acc 24.601545333862305\n",
      "iteration 344 loss 3.0840744972229004, acc 24.601545333862305\n",
      "iteration 345 loss 3.0838680267333984, acc 24.601545333862305\n",
      "iteration 346 loss 3.0836610794067383, acc 24.601545333862305\n",
      "iteration 347 loss 3.0834546089172363, acc 24.60209083557129\n",
      "iteration 348 loss 3.0832481384277344, acc 24.60209083557129\n",
      "iteration 349 loss 3.0830416679382324, acc 24.60209083557129\n",
      "iteration 350 loss 3.0828356742858887, acc 24.60209083557129\n",
      "iteration 351 loss 3.0826303958892822, acc 24.60209083557129\n",
      "iteration 352 loss 3.0824241638183594, acc 24.608667373657227\n",
      "iteration 353 loss 3.082218647003174, acc 24.671672821044922\n",
      "iteration 354 loss 3.0820138454437256, acc 24.671672821044922\n",
      "iteration 355 loss 3.081808567047119, acc 24.671672821044922\n",
      "iteration 356 loss 3.081603527069092, acc 24.671672821044922\n",
      "iteration 357 loss 3.0813987255096436, acc 24.671672821044922\n",
      "iteration 358 loss 3.0811944007873535, acc 24.671672821044922\n",
      "iteration 359 loss 3.0809898376464844, acc 24.671672821044922\n",
      "iteration 360 loss 3.0807857513427734, acc 24.68044090270996\n",
      "iteration 361 loss 3.0805811882019043, acc 24.68044090270996\n",
      "iteration 362 loss 3.0803775787353516, acc 24.68044090270996\n",
      "iteration 363 loss 3.080174446105957, acc 24.68044090270996\n",
      "iteration 364 loss 3.079970598220825, acc 24.68044090270996\n",
      "iteration 365 loss 3.0797672271728516, acc 24.68044090270996\n",
      "iteration 366 loss 3.079564094543457, acc 24.68044090270996\n",
      "iteration 367 loss 3.0793607234954834, acc 24.68044090270996\n",
      "iteration 368 loss 3.079158306121826, acc 24.68044090270996\n",
      "iteration 369 loss 3.0789551734924316, acc 24.695234298706055\n",
      "iteration 370 loss 3.0787532329559326, acc 24.695234298706055\n",
      "iteration 371 loss 3.078550338745117, acc 24.695234298706055\n",
      "iteration 372 loss 3.0783486366271973, acc 24.702356338500977\n",
      "iteration 373 loss 3.0781466960906982, acc 24.702905654907227\n",
      "iteration 374 loss 3.07794451713562, acc 24.702905654907227\n",
      "iteration 375 loss 3.0777428150177, acc 24.702905654907227\n",
      "iteration 376 loss 3.0775420665740967, acc 24.736326217651367\n",
      "iteration 377 loss 3.0773403644561768, acc 24.736326217651367\n",
      "iteration 378 loss 3.077139377593994, acc 24.736326217651367\n",
      "iteration 379 loss 3.0769383907318115, acc 24.736326217651367\n",
      "iteration 380 loss 3.076737642288208, acc 24.737422943115234\n",
      "iteration 381 loss 3.0765368938446045, acc 24.737422943115234\n",
      "iteration 382 loss 3.076336622238159, acc 24.737422943115234\n",
      "iteration 383 loss 3.076136350631714, acc 24.737422943115234\n",
      "iteration 384 loss 3.0759360790252686, acc 24.737422943115234\n",
      "iteration 385 loss 3.0757362842559814, acc 24.737422943115234\n",
      "iteration 386 loss 3.0755367279052734, acc 24.737422943115234\n",
      "iteration 387 loss 3.0753366947174072, acc 24.737422943115234\n",
      "iteration 388 loss 3.0751373767852783, acc 24.737422943115234\n",
      "iteration 389 loss 3.0749382972717285, acc 24.737422943115234\n",
      "iteration 390 loss 3.0747389793395996, acc 24.737422943115234\n",
      "iteration 391 loss 3.0745396614074707, acc 24.737422943115234\n",
      "iteration 392 loss 3.074341058731079, acc 24.737422943115234\n",
      "iteration 393 loss 3.0741424560546875, acc 24.737422943115234\n",
      "iteration 394 loss 3.073944091796875, acc 24.737422943115234\n",
      "iteration 395 loss 3.0737462043762207, acc 24.739612579345703\n",
      "iteration 396 loss 3.073547840118408, acc 24.739612579345703\n",
      "iteration 397 loss 3.073349714279175, acc 24.739612579345703\n",
      "iteration 398 loss 3.0731523036956787, acc 24.742353439331055\n",
      "iteration 399 loss 3.0729546546936035, acc 24.742353439331055\n",
      "iteration 400 loss 3.072756767272949, acc 24.766460418701172\n",
      "iteration 401 loss 3.0725595951080322, acc 24.766460418701172\n",
      "iteration 402 loss 3.0723624229431152, acc 24.766460418701172\n",
      "iteration 403 loss 3.0721652507781982, acc 24.766460418701172\n",
      "iteration 404 loss 3.0719683170318604, acc 24.776321411132812\n",
      "iteration 405 loss 3.0717718601226807, acc 24.776321411132812\n",
      "iteration 406 loss 3.071575164794922, acc 24.79823875427246\n",
      "iteration 407 loss 3.0713791847229004, acc 24.79823875427246\n",
      "iteration 408 loss 3.0711827278137207, acc 24.79823875427246\n",
      "iteration 409 loss 3.07098650932312, acc 24.79823875427246\n",
      "iteration 410 loss 3.070791006088257, acc 24.79823875427246\n",
      "iteration 411 loss 3.0705947875976562, acc 24.8037166595459\n",
      "iteration 412 loss 3.070399045944214, acc 24.8037166595459\n",
      "iteration 413 loss 3.0702035427093506, acc 24.833850860595703\n",
      "iteration 414 loss 3.0700087547302246, acc 24.85302734375\n",
      "iteration 415 loss 3.0698134899139404, acc 24.85302734375\n",
      "iteration 416 loss 3.0696182250976562, acc 24.85302734375\n",
      "iteration 417 loss 3.0694236755371094, acc 24.85302734375\n",
      "iteration 418 loss 3.0692286491394043, acc 24.85302734375\n",
      "iteration 419 loss 3.0690343379974365, acc 24.85302734375\n",
      "iteration 420 loss 3.0688397884368896, acc 24.85302734375\n",
      "iteration 421 loss 3.068645715713501, acc 24.85302734375\n",
      "iteration 422 loss 3.068451404571533, acc 24.85302734375\n",
      "iteration 423 loss 3.0682575702667236, acc 24.85302734375\n",
      "iteration 424 loss 3.068063735961914, acc 24.85302734375\n",
      "iteration 425 loss 3.0678703784942627, acc 24.85302734375\n",
      "iteration 426 loss 3.0676767826080322, acc 24.856863021850586\n",
      "iteration 427 loss 3.06748366355896, acc 24.856863021850586\n",
      "iteration 428 loss 3.0672905445098877, acc 24.856863021850586\n",
      "iteration 429 loss 3.0670974254608154, acc 24.856863021850586\n",
      "iteration 430 loss 3.066904306411743, acc 24.856863021850586\n",
      "iteration 431 loss 3.066711902618408, acc 24.893022537231445\n",
      "iteration 432 loss 3.066519260406494, acc 24.893022537231445\n",
      "iteration 433 loss 3.0663270950317383, acc 24.893022537231445\n",
      "iteration 434 loss 3.066134214401245, acc 24.893022537231445\n",
      "iteration 435 loss 3.0659425258636475, acc 24.895214080810547\n",
      "iteration 436 loss 3.0657505989074707, acc 24.895214080810547\n",
      "iteration 437 loss 3.065558671951294, acc 24.906173706054688\n",
      "iteration 438 loss 3.065366506576538, acc 24.90836524963379\n",
      "iteration 439 loss 3.0651752948760986, acc 24.90836524963379\n",
      "iteration 440 loss 3.06498384475708, acc 24.916584014892578\n",
      "iteration 441 loss 3.0647928714752197, acc 24.916584014892578\n",
      "iteration 442 loss 3.0646016597747803, acc 24.935211181640625\n",
      "iteration 443 loss 3.064410924911499, acc 24.935211181640625\n",
      "iteration 444 loss 3.0642197132110596, acc 24.941238403320312\n",
      "iteration 445 loss 3.0640289783477783, acc 24.94178581237793\n",
      "iteration 446 loss 3.063838243484497, acc 24.94178581237793\n",
      "iteration 447 loss 3.063648223876953, acc 24.94178581237793\n",
      "iteration 448 loss 3.063458204269409, acc 24.94178581237793\n",
      "iteration 449 loss 3.063267946243286, acc 24.966440200805664\n",
      "iteration 450 loss 3.063077688217163, acc 24.983427047729492\n",
      "iteration 451 loss 3.0628881454467773, acc 24.98616600036621\n",
      "iteration 452 loss 3.0626983642578125, acc 24.986713409423828\n",
      "iteration 453 loss 3.0625088214874268, acc 24.986713409423828\n",
      "iteration 454 loss 3.062319755554199, acc 24.986713409423828\n",
      "iteration 455 loss 3.0621302127838135, acc 24.986713409423828\n",
      "iteration 456 loss 3.061941146850586, acc 24.986713409423828\n",
      "iteration 457 loss 3.0617523193359375, acc 24.988357543945312\n",
      "iteration 458 loss 3.061563491821289, acc 24.988357543945312\n",
      "iteration 459 loss 3.061375141143799, acc 24.99712371826172\n",
      "iteration 460 loss 3.0611863136291504, acc 24.99712371826172\n",
      "iteration 461 loss 3.060997724533081, acc 24.99712371826172\n",
      "iteration 462 loss 3.060809373855591, acc 24.99712371826172\n",
      "iteration 463 loss 3.060621976852417, acc 24.99712371826172\n",
      "iteration 464 loss 3.0604336261749268, acc 24.99712371826172\n",
      "iteration 465 loss 3.0602457523345947, acc 24.99712371826172\n",
      "iteration 466 loss 3.060058116912842, acc 24.99712371826172\n",
      "iteration 467 loss 3.059870958328247, acc 24.99712371826172\n",
      "iteration 468 loss 3.059683322906494, acc 24.99712371826172\n",
      "iteration 469 loss 3.059495687484741, acc 24.99712371826172\n",
      "iteration 470 loss 3.0593087673187256, acc 24.99712371826172\n",
      "iteration 471 loss 3.059121608734131, acc 24.99712371826172\n",
      "iteration 472 loss 3.0589346885681152, acc 25.024518966674805\n",
      "iteration 473 loss 3.0587480068206787, acc 25.044790267944336\n",
      "iteration 474 loss 3.058561325073242, acc 25.044790267944336\n",
      "iteration 475 loss 3.058375358581543, acc 25.044790267944336\n",
      "iteration 476 loss 3.0581886768341064, acc 25.044790267944336\n",
      "iteration 477 loss 3.058002471923828, acc 25.04917335510254\n",
      "iteration 478 loss 3.057816743850708, acc 25.08040428161621\n",
      "iteration 479 loss 3.057630777359009, acc 25.08040428161621\n",
      "iteration 480 loss 3.0574450492858887, acc 25.08040428161621\n",
      "iteration 481 loss 3.0572593212127686, acc 25.08040428161621\n",
      "iteration 482 loss 3.0570738315582275, acc 25.08040428161621\n",
      "iteration 483 loss 3.0568883419036865, acc 25.08040428161621\n",
      "iteration 484 loss 3.0567033290863037, acc 25.08040428161621\n",
      "iteration 485 loss 3.056518077850342, acc 25.08040428161621\n",
      "iteration 486 loss 3.056333303451538, acc 25.09191131591797\n",
      "iteration 487 loss 3.0561485290527344, acc 25.108346939086914\n",
      "iteration 488 loss 3.0559635162353516, acc 25.1121826171875\n",
      "iteration 489 loss 3.055778980255127, acc 25.1121826171875\n",
      "iteration 490 loss 3.0555946826934814, acc 25.1121826171875\n",
      "iteration 491 loss 3.055410385131836, acc 25.1121826171875\n",
      "iteration 492 loss 3.0552263259887695, acc 25.1121826171875\n",
      "iteration 493 loss 3.055042028427124, acc 25.1121826171875\n",
      "iteration 494 loss 3.054858446121216, acc 25.1121826171875\n",
      "iteration 495 loss 3.0546743869781494, acc 25.1121826171875\n",
      "iteration 496 loss 3.054490804672241, acc 25.1121826171875\n",
      "iteration 497 loss 3.054307460784912, acc 25.1121826171875\n",
      "iteration 498 loss 3.054124116897583, acc 25.118207931518555\n",
      "iteration 499 loss 3.053940773010254, acc 25.118207931518555\n",
      "iteration 500 loss 3.053757667541504, acc 25.118207931518555\n",
      "iteration 501 loss 3.053575038909912, acc 25.118207931518555\n",
      "iteration 502 loss 3.053392171859741, acc 25.118207931518555\n",
      "iteration 503 loss 3.0532093048095703, acc 25.118207931518555\n",
      "iteration 504 loss 3.0530264377593994, acc 25.118207931518555\n",
      "iteration 505 loss 3.052844285964966, acc 25.118207931518555\n",
      "iteration 506 loss 3.052661895751953, acc 25.118207931518555\n",
      "iteration 507 loss 3.0524795055389404, acc 25.118207931518555\n",
      "iteration 508 loss 3.052297592163086, acc 25.118207931518555\n",
      "iteration 509 loss 3.0521161556243896, acc 25.147247314453125\n",
      "iteration 510 loss 3.051933765411377, acc 25.147247314453125\n",
      "iteration 511 loss 3.0517520904541016, acc 25.147247314453125\n",
      "iteration 512 loss 3.051570177078247, acc 25.147247314453125\n",
      "iteration 513 loss 3.051389217376709, acc 25.147247314453125\n",
      "iteration 514 loss 3.051208257675171, acc 25.147247314453125\n",
      "iteration 515 loss 3.0510265827178955, acc 25.147247314453125\n",
      "iteration 516 loss 3.0508458614349365, acc 25.147247314453125\n",
      "iteration 517 loss 3.0506649017333984, acc 25.147247314453125\n",
      "iteration 518 loss 3.0504841804504395, acc 25.15656089782715\n",
      "iteration 519 loss 3.0503034591674805, acc 25.15656089782715\n",
      "iteration 520 loss 3.0501232147216797, acc 25.15656089782715\n",
      "iteration 521 loss 3.0499427318573, acc 25.15656089782715\n",
      "iteration 522 loss 3.049762487411499, acc 25.15656089782715\n",
      "iteration 523 loss 3.049582004547119, acc 25.170257568359375\n",
      "iteration 524 loss 3.0494022369384766, acc 25.170257568359375\n",
      "iteration 525 loss 3.049221992492676, acc 25.170806884765625\n",
      "iteration 526 loss 3.049042224884033, acc 25.170806884765625\n",
      "iteration 527 loss 3.048862934112549, acc 25.170806884765625\n",
      "iteration 528 loss 3.0486834049224854, acc 25.170806884765625\n",
      "iteration 529 loss 3.048503875732422, acc 25.170806884765625\n",
      "iteration 530 loss 3.0483248233795166, acc 25.170806884765625\n",
      "iteration 531 loss 3.0481455326080322, acc 25.170806884765625\n",
      "iteration 532 loss 3.047966957092285, acc 25.181215286254883\n",
      "iteration 533 loss 3.047787666320801, acc 25.181215286254883\n",
      "iteration 534 loss 3.0476090908050537, acc 25.181215286254883\n",
      "iteration 535 loss 3.0474305152893066, acc 25.181215286254883\n",
      "iteration 536 loss 3.0472517013549805, acc 25.181215286254883\n",
      "iteration 537 loss 3.0470736026763916, acc 25.181215286254883\n",
      "iteration 538 loss 3.0468952655792236, acc 25.181215286254883\n",
      "iteration 539 loss 3.046717405319214, acc 25.181215286254883\n",
      "iteration 540 loss 3.046539306640625, acc 25.186147689819336\n",
      "iteration 541 loss 3.046361207962036, acc 25.186147689819336\n",
      "iteration 542 loss 3.0461838245391846, acc 25.186147689819336\n",
      "iteration 543 loss 3.0460057258605957, acc 25.186147689819336\n",
      "iteration 544 loss 3.045828342437744, acc 25.186147689819336\n",
      "iteration 545 loss 3.0456507205963135, acc 25.186147689819336\n",
      "iteration 546 loss 3.045473575592041, acc 25.193817138671875\n",
      "iteration 547 loss 3.0452964305877686, acc 25.193817138671875\n",
      "iteration 548 loss 3.0451197624206543, acc 25.193817138671875\n",
      "iteration 549 loss 3.044942617416382, acc 25.193817138671875\n",
      "iteration 550 loss 3.0447659492492676, acc 25.193817138671875\n",
      "iteration 551 loss 3.0445892810821533, acc 25.193817138671875\n",
      "iteration 552 loss 3.044412612915039, acc 25.193817138671875\n",
      "iteration 553 loss 3.044236421585083, acc 25.193817138671875\n",
      "iteration 554 loss 3.044059991836548, acc 25.193817138671875\n",
      "iteration 555 loss 3.043884038925171, acc 25.193817138671875\n",
      "iteration 556 loss 3.0437076091766357, acc 25.209157943725586\n",
      "iteration 557 loss 3.043531894683838, acc 25.209157943725586\n",
      "iteration 558 loss 3.04335618019104, acc 25.209157943725586\n",
      "iteration 559 loss 3.043180227279663, acc 25.22723960876465\n",
      "iteration 560 loss 3.0430047512054443, acc 25.22723960876465\n",
      "iteration 561 loss 3.0428292751312256, acc 25.23710060119629\n",
      "iteration 562 loss 3.042653799057007, acc 25.239839553833008\n",
      "iteration 563 loss 3.042478561401367, acc 25.239839553833008\n",
      "iteration 564 loss 3.0423038005828857, acc 25.239839553833008\n",
      "iteration 565 loss 3.042128801345825, acc 25.239839553833008\n",
      "iteration 566 loss 3.0419538021087646, acc 25.239839553833008\n",
      "iteration 567 loss 3.041779041290283, acc 25.239839553833008\n",
      "iteration 568 loss 3.04160475730896, acc 25.239839553833008\n",
      "iteration 569 loss 3.0414297580718994, acc 25.239839553833008\n",
      "iteration 570 loss 3.0412557125091553, acc 25.239839553833008\n",
      "iteration 571 loss 3.041081428527832, acc 25.239839553833008\n",
      "iteration 572 loss 3.040907382965088, acc 25.239839553833008\n",
      "iteration 573 loss 3.0407330989837646, acc 25.262853622436523\n",
      "iteration 574 loss 3.0405595302581787, acc 25.262853622436523\n",
      "iteration 575 loss 3.0403857231140137, acc 25.262853622436523\n",
      "iteration 576 loss 3.0402119159698486, acc 25.262853622436523\n",
      "iteration 577 loss 3.0400383472442627, acc 25.273263931274414\n",
      "iteration 578 loss 3.0398647785186768, acc 25.273263931274414\n",
      "iteration 579 loss 3.039691686630249, acc 25.2781925201416\n",
      "iteration 580 loss 3.0395185947418213, acc 25.2781925201416\n",
      "iteration 581 loss 3.0393455028533936, acc 25.2781925201416\n",
      "iteration 582 loss 3.039172410964966, acc 25.2781925201416\n",
      "iteration 583 loss 3.038999557495117, acc 25.2781925201416\n",
      "iteration 584 loss 3.0388269424438477, acc 25.27928924560547\n",
      "iteration 585 loss 3.0386545658111572, acc 25.27928924560547\n",
      "iteration 586 loss 3.0384814739227295, acc 25.27928924560547\n",
      "iteration 587 loss 3.038309335708618, acc 25.284767150878906\n",
      "iteration 588 loss 3.038137197494507, acc 25.290246963500977\n",
      "iteration 589 loss 3.0379650592803955, acc 25.290246963500977\n",
      "iteration 590 loss 3.0377931594848633, acc 25.290246963500977\n",
      "iteration 591 loss 3.0376205444335938, acc 25.294628143310547\n",
      "iteration 592 loss 3.037449598312378, acc 25.294628143310547\n",
      "iteration 593 loss 3.0372776985168457, acc 25.294628143310547\n",
      "iteration 594 loss 3.0371057987213135, acc 25.294628143310547\n",
      "iteration 595 loss 3.0369348526000977, acc 25.294628143310547\n",
      "iteration 596 loss 3.0367636680603027, acc 25.294628143310547\n",
      "iteration 597 loss 3.0365920066833496, acc 25.314903259277344\n",
      "iteration 598 loss 3.036421298980713, acc 25.31983184814453\n",
      "iteration 599 loss 3.036250114440918, acc 25.31983184814453\n",
      "iteration 600 loss 3.0360794067382812, acc 25.31983184814453\n",
      "iteration 601 loss 3.0359084606170654, acc 25.31983184814453\n",
      "iteration 602 loss 3.0357377529144287, acc 25.31983184814453\n",
      "iteration 603 loss 3.035567045211792, acc 25.31983184814453\n",
      "iteration 604 loss 3.0353970527648926, acc 25.324216842651367\n",
      "iteration 605 loss 3.035226583480835, acc 25.324216842651367\n",
      "iteration 606 loss 3.0350563526153564, acc 25.348323822021484\n",
      "iteration 607 loss 3.034886360168457, acc 25.348323822021484\n",
      "iteration 608 loss 3.0347161293029785, acc 25.348323822021484\n",
      "iteration 609 loss 3.0345466136932373, acc 25.348323822021484\n",
      "iteration 610 loss 3.034377098083496, acc 25.348323822021484\n",
      "iteration 611 loss 3.0342068672180176, acc 25.348323822021484\n",
      "iteration 612 loss 3.0340380668640137, acc 25.348323822021484\n",
      "iteration 613 loss 3.0338685512542725, acc 25.348323822021484\n",
      "iteration 614 loss 3.0336992740631104, acc 25.348323822021484\n",
      "iteration 615 loss 3.033529758453369, acc 25.348323822021484\n",
      "iteration 616 loss 3.0333609580993652, acc 25.348323822021484\n",
      "iteration 617 loss 3.0331921577453613, acc 25.348323822021484\n",
      "iteration 618 loss 3.033022880554199, acc 25.38338851928711\n",
      "iteration 619 loss 3.0328540802001953, acc 25.38338851928711\n",
      "iteration 620 loss 3.0326857566833496, acc 25.38338851928711\n",
      "iteration 621 loss 3.032517433166504, acc 25.38338851928711\n",
      "iteration 622 loss 3.032348871231079, acc 25.39653778076172\n",
      "iteration 623 loss 3.0321807861328125, acc 25.39653778076172\n",
      "iteration 624 loss 3.032012462615967, acc 25.39653778076172\n",
      "iteration 625 loss 3.031844139099121, acc 25.39653778076172\n",
      "iteration 626 loss 3.0316765308380127, acc 25.39653778076172\n",
      "iteration 627 loss 3.031508684158325, acc 25.39653778076172\n",
      "iteration 628 loss 3.0313408374786377, acc 25.39653778076172\n",
      "iteration 629 loss 3.0311732292175293, acc 25.39653778076172\n",
      "iteration 630 loss 3.031005859375, acc 25.39653778076172\n",
      "iteration 631 loss 3.03083872795105, acc 25.408592224121094\n",
      "iteration 632 loss 3.0306711196899414, acc 25.408592224121094\n",
      "iteration 633 loss 3.0305044651031494, acc 25.408592224121094\n",
      "iteration 634 loss 3.03033709526062, acc 25.408592224121094\n",
      "iteration 635 loss 3.03016996383667, acc 25.408592224121094\n",
      "iteration 636 loss 3.030003070831299, acc 25.408592224121094\n",
      "iteration 637 loss 3.029836654663086, acc 25.408592224121094\n",
      "iteration 638 loss 3.029670000076294, acc 25.408592224121094\n",
      "iteration 639 loss 3.029503345489502, acc 25.408592224121094\n",
      "iteration 640 loss 3.029337167739868, acc 25.410783767700195\n",
      "iteration 641 loss 3.0291707515716553, acc 25.416263580322266\n",
      "iteration 642 loss 3.0290043354034424, acc 25.434890747070312\n",
      "iteration 643 loss 3.028838872909546, acc 25.434890747070312\n",
      "iteration 644 loss 3.028672456741333, acc 25.434890747070312\n",
      "iteration 645 loss 3.0285067558288574, acc 25.434890747070312\n",
      "iteration 646 loss 3.028341054916382, acc 25.434890747070312\n",
      "iteration 647 loss 3.0281755924224854, acc 25.434890747070312\n",
      "iteration 648 loss 3.0280098915100098, acc 25.434890747070312\n",
      "iteration 649 loss 3.0278446674346924, acc 25.434890747070312\n",
      "iteration 650 loss 3.027679204940796, acc 25.434890747070312\n",
      "iteration 651 loss 3.0275137424468994, acc 25.434890747070312\n",
      "iteration 652 loss 3.027348756790161, acc 25.434890747070312\n",
      "iteration 653 loss 3.027184009552002, acc 25.434890747070312\n",
      "iteration 654 loss 3.0270190238952637, acc 25.434890747070312\n",
      "iteration 655 loss 3.0268542766571045, acc 25.442562103271484\n",
      "iteration 656 loss 3.0266895294189453, acc 25.442562103271484\n",
      "iteration 657 loss 3.0265252590179443, acc 25.442562103271484\n",
      "iteration 658 loss 3.0263609886169434, acc 25.442562103271484\n",
      "iteration 659 loss 3.026196241378784, acc 25.442562103271484\n",
      "iteration 660 loss 3.0260322093963623, acc 25.442562103271484\n",
      "iteration 661 loss 3.0258681774139404, acc 25.442562103271484\n",
      "iteration 662 loss 3.0257041454315186, acc 25.44365882873535\n",
      "iteration 663 loss 3.025540351867676, acc 25.44365882873535\n",
      "iteration 664 loss 3.025376319885254, acc 25.44365882873535\n",
      "iteration 665 loss 3.025212526321411, acc 25.44365882873535\n",
      "iteration 666 loss 3.0250492095947266, acc 25.44365882873535\n",
      "iteration 667 loss 3.024886131286621, acc 25.44365882873535\n",
      "iteration 668 loss 3.024722099304199, acc 25.4644775390625\n",
      "iteration 669 loss 3.0245587825775146, acc 25.4644775390625\n",
      "iteration 670 loss 3.0243959426879883, acc 25.4644775390625\n",
      "iteration 671 loss 3.0242326259613037, acc 25.465572357177734\n",
      "iteration 672 loss 3.0240697860717773, acc 25.465572357177734\n",
      "iteration 673 loss 3.02390718460083, acc 25.465572357177734\n",
      "iteration 674 loss 3.0237443447113037, acc 25.465572357177734\n",
      "iteration 675 loss 3.0235817432403564, acc 25.465572357177734\n",
      "iteration 676 loss 3.023419141769409, acc 25.465572357177734\n",
      "iteration 677 loss 3.02325701713562, acc 25.465572357177734\n",
      "iteration 678 loss 3.023094654083252, acc 25.465572357177734\n",
      "iteration 679 loss 3.022932291030884, acc 25.465572357177734\n",
      "iteration 680 loss 3.0227701663970947, acc 25.465572357177734\n",
      "iteration 681 loss 3.022608518600464, acc 25.465572357177734\n",
      "iteration 682 loss 3.0224461555480957, acc 25.465572357177734\n",
      "iteration 683 loss 3.022284984588623, acc 25.465572357177734\n",
      "iteration 684 loss 3.022122859954834, acc 25.465572357177734\n",
      "iteration 685 loss 3.021961212158203, acc 25.465572357177734\n",
      "iteration 686 loss 3.0217998027801514, acc 25.465572357177734\n",
      "iteration 687 loss 3.0216386318206787, acc 25.465572357177734\n",
      "iteration 688 loss 3.021477699279785, acc 25.465572357177734\n",
      "iteration 689 loss 3.0213162899017334, acc 25.465572357177734\n",
      "iteration 690 loss 3.02115535736084, acc 25.465572357177734\n",
      "iteration 691 loss 3.020993947982788, acc 25.465572357177734\n",
      "iteration 692 loss 3.0208334922790527, acc 25.465572357177734\n",
      "iteration 693 loss 3.02067232131958, acc 25.465572357177734\n",
      "iteration 694 loss 3.0205118656158447, acc 25.465572357177734\n",
      "iteration 695 loss 3.0203514099121094, acc 25.465572357177734\n",
      "iteration 696 loss 3.020191192626953, acc 25.465572357177734\n",
      "iteration 697 loss 3.0200304985046387, acc 25.465572357177734\n",
      "iteration 698 loss 3.0198700428009033, acc 25.465572357177734\n",
      "iteration 699 loss 3.0197103023529053, acc 25.465572357177734\n",
      "iteration 700 loss 3.019550323486328, acc 25.465572357177734\n",
      "iteration 701 loss 3.019390344619751, acc 25.465572357177734\n",
      "iteration 702 loss 3.019230365753174, acc 25.465572357177734\n",
      "iteration 703 loss 3.019070863723755, acc 25.47488784790039\n",
      "iteration 704 loss 3.018911361694336, acc 25.47488784790039\n",
      "iteration 705 loss 3.018751859664917, acc 25.47488784790039\n",
      "iteration 706 loss 3.018592596054077, acc 25.47488784790039\n",
      "iteration 707 loss 3.018433094024658, acc 25.480365753173828\n",
      "iteration 708 loss 3.0182738304138184, acc 25.480365753173828\n",
      "iteration 709 loss 3.0181148052215576, acc 25.480365753173828\n",
      "iteration 710 loss 3.017956018447876, acc 25.480365753173828\n",
      "iteration 711 loss 3.017796754837036, acc 25.480365753173828\n",
      "iteration 712 loss 3.0176382064819336, acc 25.480365753173828\n",
      "iteration 713 loss 3.017479658126831, acc 25.480365753173828\n",
      "iteration 714 loss 3.0173211097717285, acc 25.480365753173828\n",
      "iteration 715 loss 3.017162322998047, acc 25.480365753173828\n",
      "iteration 716 loss 3.0170037746429443, acc 25.480365753173828\n",
      "iteration 717 loss 3.016845703125, acc 25.480365753173828\n",
      "iteration 718 loss 3.0166876316070557, acc 25.480365753173828\n",
      "iteration 719 loss 3.0165295600891113, acc 25.480365753173828\n",
      "iteration 720 loss 3.016371488571167, acc 25.480365753173828\n",
      "iteration 721 loss 3.0162136554718018, acc 25.480365753173828\n",
      "iteration 722 loss 3.0160560607910156, acc 25.480365753173828\n",
      "iteration 723 loss 3.0158982276916504, acc 25.480365753173828\n",
      "iteration 724 loss 3.0157406330108643, acc 25.480365753173828\n",
      "iteration 725 loss 3.015583038330078, acc 25.480365753173828\n",
      "iteration 726 loss 3.015425682067871, acc 25.480365753173828\n",
      "iteration 727 loss 3.0152688026428223, acc 25.480365753173828\n",
      "iteration 728 loss 3.0151114463806152, acc 25.480365753173828\n",
      "iteration 729 loss 3.014953851699829, acc 25.492420196533203\n",
      "iteration 730 loss 3.0147972106933594, acc 25.492420196533203\n",
      "iteration 731 loss 3.0146405696868896, acc 25.492420196533203\n",
      "iteration 732 loss 3.014483690261841, acc 25.492420196533203\n",
      "iteration 733 loss 3.01432728767395, acc 25.492420196533203\n",
      "iteration 734 loss 3.0141701698303223, acc 25.492420196533203\n",
      "iteration 735 loss 3.0140140056610107, acc 25.492420196533203\n",
      "iteration 736 loss 3.013857364654541, acc 25.492420196533203\n",
      "iteration 737 loss 3.0137014389038086, acc 25.492420196533203\n",
      "iteration 738 loss 3.013545274734497, acc 25.492420196533203\n",
      "iteration 739 loss 3.0133891105651855, acc 25.492420196533203\n",
      "iteration 740 loss 3.0132334232330322, acc 25.492420196533203\n",
      "iteration 741 loss 3.0130772590637207, acc 25.492420196533203\n",
      "iteration 742 loss 3.0129215717315674, acc 25.492420196533203\n",
      "iteration 743 loss 3.0127663612365723, acc 25.492420196533203\n",
      "iteration 744 loss 3.0126101970672607, acc 25.492420196533203\n",
      "iteration 745 loss 3.0124549865722656, acc 25.492420196533203\n",
      "iteration 746 loss 3.0122995376586914, acc 25.492420196533203\n",
      "iteration 747 loss 3.012144088745117, acc 25.492420196533203\n",
      "iteration 748 loss 3.011989116668701, acc 25.492420196533203\n",
      "iteration 749 loss 3.011833667755127, acc 25.492420196533203\n",
      "iteration 750 loss 3.01167893409729, acc 25.492420196533203\n",
      "iteration 751 loss 3.011524200439453, acc 25.492420196533203\n",
      "iteration 752 loss 3.011369228363037, acc 25.492420196533203\n",
      "iteration 753 loss 3.0112147331237793, acc 25.492420196533203\n",
      "iteration 754 loss 3.0110599994659424, acc 25.492420196533203\n",
      "iteration 755 loss 3.0109052658081055, acc 25.492420196533203\n",
      "iteration 756 loss 3.010751247406006, acc 25.492420196533203\n",
      "iteration 757 loss 3.010596513748169, acc 25.492420196533203\n",
      "iteration 758 loss 3.0104424953460693, acc 25.495708465576172\n",
      "iteration 759 loss 3.010288953781128, acc 25.495708465576172\n",
      "iteration 760 loss 3.01013445854187, acc 25.495708465576172\n",
      "iteration 761 loss 3.0099804401397705, acc 25.495708465576172\n",
      "iteration 762 loss 3.009826898574829, acc 25.499544143676758\n",
      "iteration 763 loss 3.0096728801727295, acc 25.499544143676758\n",
      "iteration 764 loss 3.009519338607788, acc 25.504472732543945\n",
      "iteration 765 loss 3.0093657970428467, acc 25.504472732543945\n",
      "iteration 766 loss 3.0092127323150635, acc 25.509952545166016\n",
      "iteration 767 loss 3.009059190750122, acc 25.509952545166016\n",
      "iteration 768 loss 3.008906364440918, acc 25.509952545166016\n",
      "iteration 769 loss 3.0087528228759766, acc 25.509952545166016\n",
      "iteration 770 loss 3.0085997581481934, acc 25.509952545166016\n",
      "iteration 771 loss 3.0084469318389893, acc 25.509952545166016\n",
      "iteration 772 loss 3.008293867111206, acc 25.509952545166016\n",
      "iteration 773 loss 3.008141040802002, acc 25.509952545166016\n",
      "iteration 774 loss 3.007988691329956, acc 25.509952545166016\n",
      "iteration 775 loss 3.00783634185791, acc 25.509952545166016\n",
      "iteration 776 loss 3.007683515548706, acc 25.509952545166016\n",
      "iteration 777 loss 3.00753116607666, acc 25.509952545166016\n",
      "iteration 778 loss 3.0073790550231934, acc 25.509952545166016\n",
      "iteration 779 loss 3.0072267055511475, acc 25.526390075683594\n",
      "iteration 780 loss 3.0070745944976807, acc 25.52693748474121\n",
      "iteration 781 loss 3.006922483444214, acc 25.52693748474121\n",
      "iteration 782 loss 3.006770372390747, acc 25.5373477935791\n",
      "iteration 783 loss 3.0066192150115967, acc 25.5373477935791\n",
      "iteration 784 loss 3.006467342376709, acc 25.5373477935791\n",
      "iteration 785 loss 3.0063159465789795, acc 25.54227638244629\n",
      "iteration 786 loss 3.00616455078125, acc 25.547758102416992\n",
      "iteration 787 loss 3.0060126781463623, acc 25.547758102416992\n",
      "iteration 788 loss 3.005861282348633, acc 25.547758102416992\n",
      "iteration 789 loss 3.0057106018066406, acc 25.547758102416992\n",
      "iteration 790 loss 3.005559206008911, acc 25.547758102416992\n",
      "iteration 791 loss 3.0054078102111816, acc 25.547758102416992\n",
      "iteration 792 loss 3.0052571296691895, acc 25.547758102416992\n",
      "iteration 793 loss 3.0051064491271973, acc 25.547758102416992\n",
      "iteration 794 loss 3.004955768585205, acc 25.547758102416992\n",
      "iteration 795 loss 3.004805088043213, acc 25.547758102416992\n",
      "iteration 796 loss 3.0046546459198, acc 25.547758102416992\n",
      "iteration 797 loss 3.0045039653778076, acc 25.547758102416992\n",
      "iteration 798 loss 3.0043537616729736, acc 25.547758102416992\n",
      "iteration 799 loss 3.0042035579681396, acc 25.551044464111328\n",
      "iteration 800 loss 3.0040528774261475, acc 25.551044464111328\n",
      "iteration 801 loss 3.0039026737213135, acc 25.556522369384766\n",
      "iteration 802 loss 3.003753185272217, acc 25.556522369384766\n",
      "iteration 803 loss 3.0036027431488037, acc 25.556522369384766\n",
      "iteration 804 loss 3.003453254699707, acc 25.564193725585938\n",
      "iteration 805 loss 3.0033037662506104, acc 25.564193725585938\n",
      "iteration 806 loss 3.0031542778015137, acc 25.565837860107422\n",
      "iteration 807 loss 3.003004550933838, acc 25.565837860107422\n",
      "iteration 808 loss 3.002855062484741, acc 25.565837860107422\n",
      "iteration 809 loss 3.0027058124542236, acc 25.565837860107422\n",
      "iteration 810 loss 3.002556562423706, acc 25.565837860107422\n",
      "iteration 811 loss 3.0024075508117676, acc 25.565837860107422\n",
      "iteration 812 loss 3.002258062362671, acc 25.565837860107422\n",
      "iteration 813 loss 3.0021092891693115, acc 25.565837860107422\n",
      "iteration 814 loss 3.001960515975952, acc 25.565837860107422\n",
      "iteration 815 loss 3.0018115043640137, acc 25.576793670654297\n",
      "iteration 816 loss 3.001662492752075, acc 25.576793670654297\n",
      "iteration 817 loss 3.001514196395874, acc 25.576793670654297\n",
      "iteration 818 loss 3.001365900039673, acc 25.576793670654297\n",
      "iteration 819 loss 3.0012173652648926, acc 25.576793670654297\n",
      "iteration 820 loss 3.0010693073272705, acc 25.576793670654297\n",
      "iteration 821 loss 3.000920534133911, acc 25.576793670654297\n",
      "iteration 822 loss 3.000772714614868, acc 25.576793670654297\n",
      "iteration 823 loss 3.000624895095825, acc 25.576793670654297\n",
      "iteration 824 loss 3.000476837158203, acc 25.576793670654297\n",
      "iteration 825 loss 3.0003292560577393, acc 25.576793670654297\n",
      "iteration 826 loss 3.0001814365386963, acc 25.57953643798828\n",
      "iteration 827 loss 3.0000336170196533, acc 25.57953643798828\n",
      "iteration 828 loss 2.9998860359191895, acc 25.57953643798828\n",
      "iteration 829 loss 2.9997386932373047, acc 25.57953643798828\n",
      "iteration 830 loss 2.999591112136841, acc 25.57953643798828\n",
      "iteration 831 loss 2.999443292617798, acc 25.57953643798828\n",
      "iteration 832 loss 2.9992966651916504, acc 25.5800838470459\n",
      "iteration 833 loss 2.9991490840911865, acc 25.5800838470459\n",
      "iteration 834 loss 2.99900221824646, acc 25.5800838470459\n",
      "iteration 835 loss 2.9988555908203125, acc 25.5800838470459\n",
      "iteration 836 loss 2.9987082481384277, acc 25.5800838470459\n",
      "iteration 837 loss 2.9985616207122803, acc 25.5800838470459\n",
      "iteration 838 loss 2.998414993286133, acc 25.5800838470459\n",
      "iteration 839 loss 2.9982681274414062, acc 25.5800838470459\n",
      "iteration 840 loss 2.998121976852417, acc 25.5800838470459\n",
      "iteration 841 loss 2.9979755878448486, acc 25.59487533569336\n",
      "iteration 842 loss 2.9978291988372803, acc 25.60090446472168\n",
      "iteration 843 loss 2.997683048248291, acc 25.60090446472168\n",
      "iteration 844 loss 2.9975368976593018, acc 25.620079040527344\n",
      "iteration 845 loss 2.9973907470703125, acc 25.620079040527344\n",
      "iteration 846 loss 2.9972450733184814, acc 25.62117576599121\n",
      "iteration 847 loss 2.9970991611480713, acc 25.621723175048828\n",
      "iteration 848 loss 2.996953248977661, acc 25.621723175048828\n",
      "iteration 849 loss 2.99680757522583, acc 25.621723175048828\n",
      "iteration 850 loss 2.9966623783111572, acc 25.621723175048828\n",
      "iteration 851 loss 2.996516227722168, acc 25.621723175048828\n",
      "iteration 852 loss 2.996371269226074, acc 25.621723175048828\n",
      "iteration 853 loss 2.9962258338928223, acc 25.621723175048828\n",
      "iteration 854 loss 2.9960806369781494, acc 25.627201080322266\n",
      "iteration 855 loss 2.9959356784820557, acc 25.633228302001953\n",
      "iteration 856 loss 2.995790481567383, acc 25.633228302001953\n",
      "iteration 857 loss 2.99564528465271, acc 25.633228302001953\n",
      "iteration 858 loss 2.9955005645751953, acc 25.633228302001953\n",
      "iteration 859 loss 2.9953558444976807, acc 25.647472381591797\n",
      "iteration 860 loss 2.995210886001587, acc 25.647472381591797\n",
      "iteration 861 loss 2.9950666427612305, acc 25.647472381591797\n",
      "iteration 862 loss 2.994922161102295, acc 25.64966583251953\n",
      "iteration 863 loss 2.994777202606201, acc 25.64966583251953\n",
      "iteration 864 loss 2.9946329593658447, acc 25.65733528137207\n",
      "iteration 865 loss 2.9944889545440674, acc 25.65733528137207\n",
      "iteration 866 loss 2.994344711303711, acc 25.65733528137207\n",
      "iteration 867 loss 2.9942007064819336, acc 25.65733528137207\n",
      "iteration 868 loss 2.9940567016601562, acc 25.65733528137207\n",
      "iteration 869 loss 2.993912696838379, acc 25.663909912109375\n",
      "iteration 870 loss 2.9937691688537598, acc 25.663909912109375\n",
      "iteration 871 loss 2.9936254024505615, acc 25.663909912109375\n",
      "iteration 872 loss 2.9934816360473633, acc 25.663909912109375\n",
      "iteration 873 loss 2.993337869644165, acc 25.663909912109375\n",
      "iteration 874 loss 2.993194818496704, acc 25.663909912109375\n",
      "iteration 875 loss 2.993051290512085, acc 25.663909912109375\n",
      "iteration 876 loss 2.992908000946045, acc 25.682538986206055\n",
      "iteration 877 loss 2.992764949798584, acc 25.682538986206055\n",
      "iteration 878 loss 2.992622137069702, acc 25.682538986206055\n",
      "iteration 879 loss 2.992478609085083, acc 25.693496704101562\n",
      "iteration 880 loss 2.992335796356201, acc 25.709932327270508\n",
      "iteration 881 loss 2.9921929836273193, acc 25.709932327270508\n",
      "iteration 882 loss 2.9920504093170166, acc 25.709932327270508\n",
      "iteration 883 loss 2.9919073581695557, acc 25.709932327270508\n",
      "iteration 884 loss 2.991764783859253, acc 25.709932327270508\n",
      "iteration 885 loss 2.9916226863861084, acc 25.716506958007812\n",
      "iteration 886 loss 2.9914798736572266, acc 25.716506958007812\n",
      "iteration 887 loss 2.991338014602661, acc 25.716506958007812\n",
      "iteration 888 loss 2.9911954402923584, acc 25.716506958007812\n",
      "iteration 889 loss 2.991053342819214, acc 25.716506958007812\n",
      "iteration 890 loss 2.9909112453460693, acc 25.716506958007812\n",
      "iteration 891 loss 2.990769147872925, acc 25.716506958007812\n",
      "iteration 892 loss 2.9906272888183594, acc 25.716506958007812\n",
      "iteration 893 loss 2.990485906600952, acc 25.716506958007812\n",
      "iteration 894 loss 2.9903435707092285, acc 25.716506958007812\n",
      "iteration 895 loss 2.9902021884918213, acc 25.716506958007812\n",
      "iteration 896 loss 2.990060806274414, acc 25.716506958007812\n",
      "iteration 897 loss 2.9899191856384277, acc 25.7203426361084\n",
      "iteration 898 loss 2.9897775650024414, acc 25.7203426361084\n",
      "iteration 899 loss 2.9896366596221924, acc 25.7203426361084\n",
      "iteration 900 loss 2.989495277404785, acc 25.7203426361084\n",
      "iteration 901 loss 2.989354372024536, acc 25.7203426361084\n",
      "iteration 902 loss 2.989213228225708, acc 25.7203426361084\n",
      "iteration 903 loss 2.989072322845459, acc 25.7203426361084\n",
      "iteration 904 loss 2.988931655883789, acc 25.7203426361084\n",
      "iteration 905 loss 2.98879075050354, acc 25.7203426361084\n",
      "iteration 906 loss 2.988649845123291, acc 25.730205535888672\n",
      "iteration 907 loss 2.9885096549987793, acc 25.730205535888672\n",
      "iteration 908 loss 2.9883689880371094, acc 25.730205535888672\n",
      "iteration 909 loss 2.9882283210754395, acc 25.734588623046875\n",
      "iteration 910 loss 2.9880881309509277, acc 25.737327575683594\n",
      "iteration 911 loss 2.987948179244995, acc 25.737327575683594\n",
      "iteration 912 loss 2.9878077507019043, acc 25.737327575683594\n",
      "iteration 913 loss 2.9876677989959717, acc 25.737327575683594\n",
      "iteration 914 loss 2.987527847290039, acc 25.737327575683594\n",
      "iteration 915 loss 2.9873876571655273, acc 25.737327575683594\n",
      "iteration 916 loss 2.987247943878174, acc 25.737327575683594\n",
      "iteration 917 loss 2.9871082305908203, acc 25.737327575683594\n",
      "iteration 918 loss 2.986968755722046, acc 25.737327575683594\n",
      "iteration 919 loss 2.9868288040161133, acc 25.737327575683594\n",
      "iteration 920 loss 2.986689567565918, acc 25.737327575683594\n",
      "iteration 921 loss 2.9865500926971436, acc 25.750478744506836\n",
      "iteration 922 loss 2.98641037940979, acc 25.753767013549805\n",
      "iteration 923 loss 2.986271381378174, acc 25.753767013549805\n",
      "iteration 924 loss 2.9861326217651367, acc 25.753767013549805\n",
      "iteration 925 loss 2.985992908477783, acc 25.753767013549805\n",
      "iteration 926 loss 2.985853910446167, acc 25.755409240722656\n",
      "iteration 927 loss 2.985715866088867, acc 25.755409240722656\n",
      "iteration 928 loss 2.985576868057251, acc 25.755409240722656\n",
      "iteration 929 loss 2.9854376316070557, acc 25.755409240722656\n",
      "iteration 930 loss 2.9852993488311768, acc 25.755409240722656\n",
      "iteration 931 loss 2.9851605892181396, acc 25.755409240722656\n",
      "iteration 932 loss 2.9850218296051025, acc 25.755409240722656\n",
      "iteration 933 loss 2.9848837852478027, acc 25.755409240722656\n",
      "iteration 934 loss 2.9847450256347656, acc 25.755409240722656\n",
      "iteration 935 loss 2.984607458114624, acc 25.755409240722656\n",
      "iteration 936 loss 2.984468936920166, acc 25.758148193359375\n",
      "iteration 937 loss 2.9843311309814453, acc 25.758148193359375\n",
      "iteration 938 loss 2.9841933250427246, acc 25.758148193359375\n",
      "iteration 939 loss 2.984055280685425, acc 25.758148193359375\n",
      "iteration 940 loss 2.983916997909546, acc 25.758148193359375\n",
      "iteration 941 loss 2.9837794303894043, acc 25.758148193359375\n",
      "iteration 942 loss 2.9836418628692627, acc 25.776777267456055\n",
      "iteration 943 loss 2.983504295349121, acc 25.776777267456055\n",
      "iteration 944 loss 2.9833672046661377, acc 25.776777267456055\n",
      "iteration 945 loss 2.983229875564575, acc 25.776777267456055\n",
      "iteration 946 loss 2.9830920696258545, acc 25.776777267456055\n",
      "iteration 947 loss 2.982954978942871, acc 25.776777267456055\n",
      "iteration 948 loss 2.9828176498413086, acc 25.776777267456055\n",
      "iteration 949 loss 2.982680559158325, acc 25.776777267456055\n",
      "iteration 950 loss 2.982543706893921, acc 25.776777267456055\n",
      "iteration 951 loss 2.9824070930480957, acc 25.776777267456055\n",
      "iteration 952 loss 2.9822702407836914, acc 25.776777267456055\n",
      "iteration 953 loss 2.982133388519287, acc 25.788284301757812\n",
      "iteration 954 loss 2.981996774673462, acc 25.788284301757812\n",
      "iteration 955 loss 2.981860399246216, acc 25.788284301757812\n",
      "iteration 956 loss 2.9817237854003906, acc 25.788284301757812\n",
      "iteration 957 loss 2.9815866947174072, acc 25.788284301757812\n",
      "iteration 958 loss 2.9814510345458984, acc 25.78883171081543\n",
      "iteration 959 loss 2.9813144207000732, acc 25.78883171081543\n",
      "iteration 960 loss 2.9811785221099854, acc 25.805267333984375\n",
      "iteration 961 loss 2.9810423851013184, acc 25.815677642822266\n",
      "iteration 962 loss 2.9809064865112305, acc 25.815677642822266\n",
      "iteration 963 loss 2.9807708263397217, acc 25.815677642822266\n",
      "iteration 964 loss 2.9806346893310547, acc 25.815677642822266\n",
      "iteration 965 loss 2.980498790740967, acc 25.815677642822266\n",
      "iteration 966 loss 2.980363368988037, acc 25.815677642822266\n",
      "iteration 967 loss 2.9802279472351074, acc 25.815677642822266\n",
      "iteration 968 loss 2.9800915718078613, acc 25.815677642822266\n",
      "iteration 969 loss 2.979956865310669, acc 25.815677642822266\n",
      "iteration 970 loss 2.97982120513916, acc 25.815677642822266\n",
      "iteration 971 loss 2.9796860218048096, acc 25.815677642822266\n",
      "iteration 972 loss 2.979551076889038, acc 25.815677642822266\n",
      "iteration 973 loss 2.9794156551361084, acc 25.815677642822266\n",
      "iteration 974 loss 2.979280710220337, acc 25.815677642822266\n",
      "iteration 975 loss 2.9791462421417236, acc 25.815677642822266\n",
      "iteration 976 loss 2.979010820388794, acc 25.815677642822266\n",
      "iteration 977 loss 2.9788761138916016, acc 25.815677642822266\n",
      "iteration 978 loss 2.978741407394409, acc 25.815677642822266\n",
      "iteration 979 loss 2.978607177734375, acc 25.815677642822266\n",
      "iteration 980 loss 2.9784724712371826, acc 25.815677642822266\n",
      "iteration 981 loss 2.9783377647399902, acc 25.822799682617188\n",
      "iteration 982 loss 2.978203535079956, acc 25.822799682617188\n",
      "iteration 983 loss 2.978069305419922, acc 25.822799682617188\n",
      "iteration 984 loss 2.9779350757598877, acc 25.822799682617188\n",
      "iteration 985 loss 2.9778008460998535, acc 25.822799682617188\n",
      "iteration 986 loss 2.9776668548583984, acc 25.822799682617188\n",
      "iteration 987 loss 2.9775326251983643, acc 25.822799682617188\n",
      "iteration 988 loss 2.9773988723754883, acc 25.829374313354492\n",
      "iteration 989 loss 2.977264881134033, acc 25.829374313354492\n",
      "iteration 990 loss 2.9771313667297363, acc 25.829374313354492\n",
      "iteration 991 loss 2.9769978523254395, acc 25.829374313354492\n",
      "iteration 992 loss 2.9768638610839844, acc 25.829374313354492\n",
      "iteration 993 loss 2.9767303466796875, acc 25.829374313354492\n",
      "iteration 994 loss 2.9765970706939697, acc 25.829374313354492\n",
      "iteration 995 loss 2.976463556289673, acc 25.829374313354492\n",
      "iteration 996 loss 2.976330280303955, acc 25.829374313354492\n",
      "iteration 997 loss 2.9761977195739746, acc 25.829374313354492\n",
      "iteration 998 loss 2.976064443588257, acc 25.844717025756836\n",
      "iteration 999 loss 2.975931167602539, acc 25.844717025756836\n"
     ]
    }
   ],
   "source": [
    "iterations = 1000\n",
    "lr = 0.5\n",
    "reg  = 0.01\n",
    "\n",
    "for k in range(iterations):\n",
    "\n",
    "    W.grad = None\n",
    "    logits = W[X_train[:, 0], X_train[:, 1]]\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y_train) + reg*torch.mean(W ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    W.data -= lr * W.grad\n",
    "    pred = logits.argmax(dim = 1)\n",
    "\n",
    "    acc = (pred == Y_train).float().mean().data\n",
    "    print(f\"iteration {k} loss {loss.data}, acc {acc * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1600a1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.975931167602539, acc 21.819934844970703\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    p = W[X_test[:, 0], X_test[:, 1]]\n",
    "    pred = p.argmax(dim = 1)\n",
    "    loss = F.cross_entropy(logits, Y_train) + reg*torch.mean(W ** 2)\n",
    "    acc = (pred == Y_test).float().mean().data\n",
    "    print(f\"loss {loss.data}, acc {acc * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eed91a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dexzm.\n",
      "aoglkurkicqzktyhwmvmzimjttainrlkfukzkatda.\n",
      "rfcxvpubjtbhrmgotzx.\n",
      "iczixqctvujkwptedogkkjemkmmsidguenkbvgynywftbspmhwcivgbvtahlvsu.\n",
      "asdxxblnwglhpyiw.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for k in range(5):\n",
    "\n",
    "    out = []\n",
    "    idx0 = 0\n",
    "    idx1 = 0\n",
    "    idx2 = None\n",
    "\n",
    "    while True:\n",
    "        logits = W[idx0, idx1]\n",
    "        p = logits.exp() / logits.exp().sum()\n",
    "        idx2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[idx2])\n",
    "        if idx2 == 0:\n",
    "            break\n",
    "        idx0 = idx1\n",
    "        idx1 = idx2\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "16e45b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.rand(nchars * 2, nchars) * 0.1\n",
    "b = torch.zeros(nchars)\n",
    "\n",
    "W.requires_grad = True\n",
    "b.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dc6e87f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 3.2963011264801025, acc 3.504331111907959\n",
      "loss 3.2496845722198486, acc 19.305599212646484\n",
      "loss 3.20658540725708, acc 20.822717666625977\n",
      "loss 3.167017698287964, acc 21.366777420043945\n",
      "loss 3.130943536758423, acc 21.479642868041992\n",
      "loss 3.098236322402954, acc 21.883987426757812\n",
      "loss 3.068695545196533, acc 21.921245574951172\n",
      "loss 3.0420548915863037, acc 21.936038970947266\n",
      "loss 3.018009662628174, acc 21.96124267578125\n",
      "loss 2.996251106262207, acc 21.96124267578125\n",
      "loss 2.976480007171631, acc 22.066438674926758\n",
      "loss 2.9584341049194336, acc 22.028085708618164\n",
      "loss 2.941887855529785, acc 22.11246109008789\n",
      "loss 2.926650285720825, acc 22.079586029052734\n",
      "loss 2.9125683307647705, acc 22.04178237915039\n",
      "loss 2.89951229095459, acc 22.03466033935547\n",
      "loss 2.887378454208374, acc 22.03411102294922\n",
      "loss 2.876075267791748, acc 22.027536392211914\n",
      "loss 2.8655264377593994, acc 22.01055335998535\n",
      "loss 2.8556630611419678, acc 22.006717681884766\n",
      "loss 2.8464250564575195, acc 22.00562286376953\n",
      "loss 2.837759017944336, acc 22.004526138305664\n",
      "loss 2.829615592956543, acc 22.003976821899414\n",
      "loss 2.8219499588012695, acc 22.001237869262695\n",
      "loss 2.814723253250122, acc 22.001237869262695\n",
      "loss 2.8078980445861816, acc 22.000690460205078\n",
      "loss 2.8014419078826904, acc 22.000690460205078\n",
      "loss 2.7953250408172607, acc 22.000690460205078\n",
      "loss 2.7895195484161377, acc 22.001237869262695\n",
      "loss 2.784001588821411, acc 21.995758056640625\n",
      "loss 2.7787492275238037, acc 21.995758056640625\n",
      "loss 2.773740768432617, acc 22.004526138305664\n",
      "loss 2.7689599990844727, acc 22.007265090942383\n",
      "loss 2.7643890380859375, acc 22.008909225463867\n",
      "loss 2.760014057159424, acc 22.011648178100586\n",
      "loss 2.755821704864502, acc 22.019319534301758\n",
      "loss 2.751798391342163, acc 22.017675399780273\n",
      "loss 2.7479350566864014, acc 22.023700714111328\n",
      "loss 2.744220018386841, acc 22.027536392211914\n",
      "loss 2.740644693374634, acc 22.042329788208008\n",
      "loss 2.737200975418091, acc 22.043973922729492\n",
      "loss 2.733879804611206, acc 22.043973922729492\n",
      "loss 2.73067569732666, acc 22.046165466308594\n",
      "loss 2.7275819778442383, acc 22.048357009887695\n",
      "loss 2.7245914936065674, acc 22.066984176635742\n",
      "loss 2.721698760986328, acc 22.066984176635742\n",
      "loss 2.718899726867676, acc 22.064794540405273\n",
      "loss 2.716188430786133, acc 22.061506271362305\n",
      "loss 2.7135612964630127, acc 22.075204849243164\n",
      "loss 2.71101450920105, acc 22.077394485473633\n",
      "loss 2.708543062210083, acc 22.11246109008789\n",
      "loss 2.706143617630005, acc 22.109722137451172\n",
      "loss 2.703813314437866, acc 22.113008499145508\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 23\u001b[0m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, Y_train) \u001b[38;5;241m+\u001b[39m reg\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(W \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# counts = logits.exp()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# counts_norm = counts * counts.sum(dim = 1, keepdim=True)**-1\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# logs = -counts_norm[torch.arange(x_train.shape[0]), y_train].log()\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# loss = logs.mean() + reg*torch.mean(W ** 2)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m W\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m W\u001b[38;5;241m.\u001b[39mgrad\n\u001b[1;32m     26\u001b[0m b\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m b\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/miniconda3/envs/i2dl/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/i2dl/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "lr = 0.5\n",
    "reg  = torch.tensor(0.01)\n",
    "reg.requires_grad = True\n",
    "\n",
    "for k in range(iterations):\n",
    "\n",
    "    W.grad = None\n",
    "    b.grad = None\n",
    "    reg.grad = None\n",
    "    \n",
    "    logits = (W[X_train].sum(1)) + b\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y_train) + reg*torch.mean(W ** 2)\n",
    "\n",
    "    # counts = logits.exp()\n",
    "\n",
    "    # counts_norm = counts * counts.sum(dim = 1, keepdim=True)**-1\n",
    "\n",
    "    # logs = -counts_norm[torch.arange(x_train.shape[0]), y_train].log()\n",
    "    # loss = logs.mean() + reg*torch.mean(W ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    W.data -= lr * W.grad\n",
    "    b.data -= lr * b.grad\n",
    "    reg.data -= reg * reg.grad\n",
    "\n",
    "    pred = logits.argmax(dim = 1)\n",
    "\n",
    "    acc = (pred == Y_train).float().mean().data\n",
    "    print(f\"loss {loss.data}, acc {acc * 100}\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    p = (W[x_test].sum(1)) + b\n",
    "    pred = p.argmax(dim = 1)\n",
    "    acc = (pred == y_test).float().mean().data\n",
    "    print(f\"acc on val {acc * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c6fb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dexze.\n",
      "aoallurailazityhn.\n",
      "rllimjtnainrlkaan.\n",
      "ka.\n",
      "aa.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for k in range(5):\n",
    "\n",
    "    out = []\n",
    "    idx0 = 0\n",
    "    idx1 = 0\n",
    "    idx2 = None\n",
    "\n",
    "    while True:\n",
    "        logits = (W[idx0] + W[idx1]) + b\n",
    "        p = logits.exp() / logits.exp().sum()\n",
    "        idx2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[idx2])\n",
    "        if idx2 == 0:\n",
    "            break\n",
    "        idx0 = idx1\n",
    "        idx1 = idx2\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
